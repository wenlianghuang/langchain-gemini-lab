import os
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
# âœ¨ ä¿®æ”¹é» 1: ç§»é™¤ PDF å’Œ Chroma ç›¸é—œçš„ importï¼Œæ”¹ç”¨ Tavily
#from langchain_tavily import TavilySearchAPIRetriever
from langchain_community.retrievers import TavilySearchAPIRetriever

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser

from datetime import datetime
# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def main():
    # æª¢æŸ¥ API Key
    if not os.getenv("TAVILY_API_KEY"):
        print("âŒ éŒ¯èª¤ï¼šæœªåµæ¸¬åˆ° TAVILY_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆã€‚")
        return

    print("ğŸš€ åˆå§‹åŒ–å…·å‚™ã€Œè¨˜æ†¶åŠŸèƒ½ã€çš„ Web Search ç³»çµ±...")

    # --- âœ¨ ä¿®æ”¹é» 2: æº–å‚™ Retriever (å¾ VectorStore æ›æˆ Web Search) ---
    # k=3 ä»£è¡¨æ¯æ¬¡æœå°‹å›å‚³ 3 ç¯‡æœ€ç›¸é—œçš„ç¶²é å…§å®¹
    retriever = TavilySearchAPIRetriever(k=3)

    # --- 2. æº–å‚™ LLM (ç¶­æŒä¸è®Š) ---
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)

    # --- 3. å»ºç«‹ã€Œå•é¡Œé‡çµ„ã€éˆ (ç¶­æŒä¸è®Š) ---
    # é€™æ®µé‚è¼¯å°æ–¼ Web Search æ›´é‡è¦ï¼Œå› ç‚ºç¶²è·¯æœå°‹å°é—œéµå­—å¾ˆæ•æ„Ÿ
    contextualize_q_system_prompt = """
    çµ¦å®šä¸€æ®µèŠå¤©æ­·å²è¨˜éŒ„å’Œä½¿ç”¨è€…æœ€æ–°çš„å•é¡Œï¼Œ
    è«‹å°‡è©²å•é¡Œé‡æ–°è¡¨è¿°ç‚ºä¸€å€‹ç¨ç«‹çš„å•é¡Œï¼Œä½¿å…¶åœ¨æ²’æœ‰èŠå¤©æ­·å²è¨˜éŒ„çš„æƒ…æ³ä¸‹ä¹Ÿèƒ½è¢«ç†è§£ã€‚
    ç›´æ¥å›å‚³æ”¹å¯«å¾Œçš„å•é¡Œå³å¯ã€‚
    """
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    def format_docs(docs):
        """æ ¼å¼åŒ–æœå°‹åˆ°çš„ç¶²é å…§å®¹"""
        # Tavily å›å‚³çš„ doc.page_content å·²ç¶“æ˜¯æ‘˜è¦éçš„ç´”æ–‡å­—
        return "\n\n".join(
            f"[ä¾†æº: {doc.metadata.get('source', 'æœªçŸ¥')}]\n{doc.page_content}" 
            for doc in docs
        )
    
    def get_standalone_question(input_dict):
        if input_dict.get("chat_history"):
            standalone_question_chain = contextualize_q_prompt | llm | StrOutputParser()
            return standalone_question_chain.invoke(input_dict)
        return input_dict["input"]
    
    def retrieve_documents(input_dict):
        question = get_standalone_question(input_dict)
        print(f"\nğŸ” æ­£åœ¨æœå°‹ç¶²è·¯ä¸Šé—œæ–¼: '{question}' çš„è³‡æ–™...") # åŠ å€‹ log è®“æ‚¨çœ‹åˆ°å®ƒåœ¨æŸ¥ä»€éº¼
        docs = retriever.invoke(question)
        return format_docs(docs)
    
    # --- 4. å»ºç«‹ã€Œå•ç­”ã€éˆ (å¾®èª¿ Prompt) ---
    today_date = datetime.now().strftime("%Y-%m-%d")
    # âœ¨ ä¿®æ”¹é» 3: æç¤ºè©ç¨å¾®èª¿æ•´ï¼Œè®“ AI çŸ¥é“å®ƒçš„è³‡è¨Šä¾†è‡ªç¶²è·¯
    qa_system_prompt = f"""
    ä½ æ˜¯ä¸€å€‹å³æ™‚ç¶²è·¯è³‡è¨ŠåŠ©æ‰‹ã€‚ç¾åœ¨çš„æ™‚é–“æ˜¯ï¼š{today_date}ã€‚
    è«‹æ ¹æ“šä»¥ä¸‹çš„ã€Œç¶²è·¯æœå°‹çµæœã€ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    
    é‡è¦è¦å‰‡ï¼š
    1. ç•¶ä½¿ç”¨è€…è©¢å•ã€Œæœ€æ–°ã€æˆ–ã€Œä»Šå¤©ã€çš„è³‡è¨Šï¼ˆå¦‚è‚¡åƒ¹ã€æ–°èï¼‰æ™‚ï¼Œè«‹å‹™å¿…å„ªå…ˆåƒè€ƒèˆ‡ {today_date} æœ€æ¥è¿‘çš„æœå°‹çµæœã€‚
    2. å¦‚æœæœå°‹çµæœä¸­çš„æ—¥æœŸæ˜¯èˆŠçš„ï¼ˆä¾‹å¦‚å¥½å¹¾å¤©å‰ï¼‰ï¼Œè«‹æ˜ç¢ºå‘Šè¨´ä½¿ç”¨è€…è©²è³‡è¨Šçš„æ—¥æœŸï¼Œä¸è¦å‡è£å®ƒæ˜¯æœ€æ–°çš„ã€‚
    3. å¦‚æœæ‰¾ä¸åˆ°ç¢ºåˆ‡çš„ä»Šæ—¥æ•¸æ“šï¼Œè«‹å›ç­”ã€Œæ‰¾ä¸åˆ°ä»Šæ—¥æ•¸æ“šï¼Œä½†æœ€è¿‘ä¸€ç­†æ•¸æ“šæ˜¯...ã€ã€‚
    {{context}}
    """
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # --- 5. çµ„åˆ RAG Chain (æ¶æ§‹å®Œå…¨ä¸è®Šï¼) ---
    rag_chain = (
        {
            "context": RunnableLambda(retrieve_documents),
            "input": lambda x: x["input"],
            "chat_history": lambda x: x.get("chat_history", []),
        }
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    # --- 6. é–‹å§‹å°è©± ---
    print("\nâœ… ç³»çµ±å°±ç·’ï¼è©¦è‘—å•æˆ‘æœ€è¿‘çš„æ–°è (ä¾‹å¦‚ï¼š'æ˜¨å¤©é‚£æ–¯é”å…‹æŒ‡æ•¸å¦‚ä½•ï¼Ÿ')\n")
    
    chat_history = []

    while True:
        user_input = input("ä½ ï¼š")
        if user_input.lower() in ["exit", "quit", "bye"]:
            break
        
        if not user_input.strip():
            continue

        print("ğŸ¤– (ä¸Šç¶²ä¸­)...", end="", flush=True)
        
        try:
            response = rag_chain.invoke({
                "input": user_input,
                "chat_history": chat_history
            })
            
            print(f"\rAIï¼š{response}\n")
            
            chat_history.append(HumanMessage(content=user_input))
            chat_history.append(AIMessage(content=response))
            if len(chat_history) > 10: 
                chat_history = chat_history[-10:]
                
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
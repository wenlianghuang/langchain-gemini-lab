import os
import uvicorn
import asyncio
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# --- FastAPI & LangServe Imports ---
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from langserve import add_routes

# --- LangChain & LangGraph Imports ---
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool, StructuredTool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools.tavily_search import TavilySearchResults

from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver

# --- MCP Client Imports ---
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# Pydantic ç”¨æ–¼å®šç¾©è¼¸å…¥ä»‹é¢
from pydantic import BaseModel
from typing import List, Union

load_dotenv()

# ==========================================
# 0. è¨­å®š LLM
# ==========================================
def get_llm():
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
    return ChatGroq(
        model="llama-3.3-70b-versatile", # è«‹ç¢ºèª Groq æ˜¯å¦æ”¯æ´æ­¤æ¨¡å‹åç¨±ï¼Œæˆ–æ”¹ç‚º "llama3-70b-8192" ç­‰
        temperature=0,
        max_retries=2,
    )

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ– (Local RAG)
# ==========================================
print("ğŸš€ [Server] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
pdf_path = "./data/Tree_of_Thoughts.pdf"
retriever = None

# ç‚ºäº†é¿å…æ¯æ¬¡å­˜æª”éƒ½é‡æ–°è·‘ embeddingï¼Œå»ºè­°æª¢æŸ¥æ˜¯å¦å­˜åœ¨
if os.path.exists(pdf_path):
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        # æ³¨æ„ï¼šæ­£å¼ç’°å¢ƒå»ºè­°ä½¿ç”¨ persist_directory ä¾†å„²å­˜ Chroma
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        print("âœ… PDF è¼‰å…¥å®Œæˆã€‚")
    except Exception as e:
        print(f"âš ï¸ PDF è™•ç†éŒ¯èª¤ (å¯èƒ½æ˜¯ API Key å•é¡Œ): {e}")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# ==========================================
# 2. å®šç¾©å·¥å…· (Local + MCP)
# ==========================================

# --- Local Tool: RAG ---
@tool
def lookup_pdf_knowledge(query: str) -> str:
    """æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚"""
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ”§ [Tool: RAG] Server æ­£åœ¨æª¢ç´¢ PDF: {query}")
    try:
        llm_rag = get_llm()
        prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm_rag
            | StrOutputParser()
        )
        return chain.invoke(query)
    except Exception as e:
        return f"RAG æª¢ç´¢å¤±æ•—: {e}"

# --- Local Tool: Web Search ---
@tool
def search_web(query: str) -> str:
    """æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚"""
    print(f"   ğŸ”§ [Tool: Web] Server æ­£åœ¨æœå°‹: {query}")
    try:
        search_tool = TavilySearchResults(k=3)
        results = search_tool.invoke(query)
        if isinstance(results, list):
            formatted_results = []
            for item in results:
                if isinstance(item, dict):
                    title = item.get("title", "ç„¡æ¨™é¡Œ")
                    url = item.get("url", "")
                    content = item.get("content", "")
                    formatted_results.append(f"æ¨™é¡Œ: {title}\nç¶²å€: {url}\nå…§å®¹: {content}")
                else:
                    formatted_results.append(str(item))
            return "\n\n".join(formatted_results)
        return str(results)
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

# --- MCP Tool Client Wrapper ---
# é€™æ˜¯ä¸€å€‹ wrapper functionï¼Œç•¶ LLM æ±ºå®šå‘¼å« "get_stock_price" æ™‚ï¼Œ
# é€™å€‹ function æœƒå‹•æ…‹å•Ÿå‹• MCP Server ä¸¦è½‰ç™¼è«‹æ±‚ã€‚
async def query_mcp_stock_server(ticker: str) -> str:
    """é€é MCP Server æŸ¥è©¢è‚¡ç¥¨åƒ¹æ ¼ã€‚"""
    print(f"   ğŸ“¡ [MCP Client] é€£æ¥ Stock MCP Server æŸ¥è©¢: {ticker}")
    
    # è¨­å®š MCP Server çš„å•Ÿå‹•åƒæ•¸ (å‡è¨­ä½¿ç”¨ uv run)
    server_params = StdioServerParameters(
        command="uv",
        args=["run", "stock_mcp_server.py"], # ç¢ºä¿æª”åæ­£ç¢º
        env=os.environ.copy() # å‚³éç’°å¢ƒè®Šæ•¸ (API Keys)
    )

    try:
        # å»ºç«‹ Stdio é€£ç·š
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                
                # å‘¼å«é ç«¯å·¥å…·
                result = await session.call_tool("get_stock_price", arguments={"ticker": ticker})
                
                # è§£æçµæœ (MCP å›å‚³çš„æ˜¯ TextContent ç‰©ä»¶åˆ—è¡¨)
                if result.content and hasattr(result.content[0], 'text'):
                    return result.content[0].text
                return str(result)
    except Exception as e:
        return f"MCP é€£ç·šæˆ–åŸ·è¡ŒéŒ¯èª¤: {e}"

# å°‡ Wrapper è½‰æ›ç‚º LangChain Tool
mcp_stock_tool = StructuredTool.from_function(
    coroutine=query_mcp_stock_server,
    name="get_stock_price",
    description="æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ (ä¾‹å¦‚ 2330.TW, NVDA)ã€‚é€™æ˜¯ä¸€å€‹å¤–éƒ¨ MCP å·¥å…·ã€‚",
)

# æ•´åˆæ‰€æœ‰å·¥å…·
tools_list = [lookup_pdf_knowledge, search_web, mcp_stock_tool]

# ==========================================
# 3. å®šç¾©è¼¸å…¥ä»‹é¢
# ==========================================
class AgentInput(BaseModel):
    messages: List[Union[HumanMessage, AIMessage, SystemMessage]]

# ==========================================
# 4. å»ºæ§‹ LangGraph
# ==========================================
def create_agent_graph():
    llm = get_llm()
    # ç¶å®šå·¥å…·
    llm_with_tools = llm.bind_tools(tools_list)

    def agent_node(state: MessagesState):
        messages = state["messages"]
        
        # ç³»çµ±æç¤ºè©
        has_system_msg = any(isinstance(msg, SystemMessage) for msg in messages)
        if not has_system_msg:
            system_msg = SystemMessage(
                content="ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·ä¾†å›ç­”å•é¡Œã€‚\n\n"
                "å¯ç”¨å·¥å…·ï¼š\n"
                "1. get_stock_price(ticker: str) - [MCP] æŸ¥è©¢è‚¡ç¥¨åƒ¹æ ¼ (å¦‚ 2330.TW, NVDA)ã€‚\n"
                "2. lookup_pdf_knowledge(query: str) - [Local] æŸ¥è©¢PDFçŸ¥è­˜åº«ã€‚\n"
                "3. search_web(query: str) - [Local] æœå°‹ç¶²è·¯è³‡è¨Šã€‚\n"
            )
            messages = [system_msg] + messages
        
        # å‘¼å« LLM
        response = llm_with_tools.invoke(messages)
        return {"messages": [response]}

    # å»ºæ§‹ Graph
    builder = StateGraph(MessagesState)
    builder.add_node("agent", agent_node)
    builder.add_node("tools", ToolNode(tools_list))

    builder.add_edge(START, "agent")
    builder.add_conditional_edges("agent", tools_condition)
    builder.add_edge("tools", "agent")

    memory = MemorySaver()
    graph = builder.compile(checkpointer=memory)
    
    # ç¶å®šå‹åˆ¥èˆ‡ Config
    graph = graph.with_types(input_type=AgentInput)
    graph = graph.with_config(configurable={"thread_id": "web-user-demo"})
    
    return graph

# ==========================================
# 5. å»ºç«‹ FastAPI æ‡‰ç”¨
# ==========================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    # å•Ÿå‹•æ™‚æª¢æŸ¥ MCP æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists("stock_mcp_server.py"):
        print("âŒ è­¦å‘Šï¼šæ‰¾ä¸åˆ° stock_mcp_server.pyï¼Œè‚¡ç¥¨åŠŸèƒ½å°‡ç„¡æ³•é‹ä½œï¼")
    else:
        print("âœ… æ£€æµ‹åˆ° stock_mcp_server.py")
    yield
    print("ğŸ‘‹ Server é—œé–‰ä¸­...")

app = FastAPI(
    title="LangGraph Agent (with MCP)",
    version="1.1",
    description="LangGraph Agent connecting to Local Tools and MCP Servers",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å»ºç«‹ Graph
graph = create_agent_graph()

# è¨­å®šè·¯ç”±
add_routes(
    app,
    graph,
    path="/agent",
    playground_type="default", 
)

if __name__ == "__main__":
    print("\nğŸš€ Server å•Ÿå‹•ä¸­...")
    print("ğŸ‘‰ Playground: http://localhost:8000/agent/playground/")
    uvicorn.run(app, host="0.0.0.0", port=8000)
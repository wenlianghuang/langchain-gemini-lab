import os
import yfinance as yf
from dotenv import load_dotenv


# âœ… æ–°å¢ Groq
from langchain_groq import ChatGroq

# Embedding æˆ‘å€‘æš«æ™‚ç¶­æŒ Googleï¼Œå› ç‚º Embedding çš„é¡åº¦è¨ˆç®—é€šå¸¸åˆ†é–‹ä¸”è¼ƒä¾¿å®œ
# å¦‚æœé€£ Embedding éƒ½çˆ†äº†ï¼Œå¯ä»¥æ”¹ç”¨ HuggingFaceEmbeddings (æœ¬åœ°ç«¯)
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools.tavily_search import TavilySearchResults

# --- LangGraph Imports ---
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, Annotated
from langchain_core.messages import AIMessage

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ==========================================
# 0. è¨­å®š LLM (æ›´æ›å¼•æ“æ ¸å¿ƒ)
# ==========================================
def get_llm():
    """
    çµ±ä¸€ç®¡ç† LLM æ¨¡å‹ã€‚
    é€™è£¡ä½¿ç”¨ Groq çš„ Llama 3.3 70Bï¼Œå®ƒæ˜¯ç›®å‰é–‹æºç•Œæœ€å¼·çš„æ¨¡å‹ä¹‹ä¸€ï¼Œ
    éå¸¸æ“…é•· Tool Calling å’Œè¤‡é›œé‚è¼¯ã€‚
    """
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
    
    # å˜—è©¦ä½¿ç”¨æ”¯æ´å·¥å…·èª¿ç”¨çš„æ¨¡å‹
    # æ³¨æ„ï¼šllama-3.1-70b-versatile å·²è¢«åœç”¨ï¼Œæ”¹ç”¨ llama-3.3-70b-versatile
    # å¯é¸æ¨¡å‹ï¼šllama-3.3-70b-versatile, llama-3.1-8b-instant, mixtral-8x7b-32768
    model_name = os.getenv("GROQ_MODEL", "llama-3.3-70b-versatile")
    
    print(f"   ğŸ“Œ ä½¿ç”¨ Groq æ¨¡å‹: {model_name}")
    
    return ChatGroq(
        model=model_name,
        temperature=0.1,  # ç¨å¾®æé«˜æº«åº¦å¯èƒ½æœ‰åŠ©æ–¼å·¥å…·èª¿ç”¨
        max_retries=2,
    )

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ–ï¼šå…¨åŸŸè³‡æº (PDF VectorStore)
# ==========================================
print("ğŸš€ [System] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
pdf_path = "./data/Tree_of_Thoughts.pdf"

retriever = None
if os.path.exists(pdf_path):
    # å¦‚æœ Google Embedding ä¹Ÿçˆ†é¡åº¦ï¼Œè«‹æ”¹ç”¨: from langchain_community.embeddings import HuggingFaceEmbeddings
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… PDF è¼‰å…¥å®Œæˆ (Embedding ä½¿ç”¨ Googleï¼Œæ¨è«–ä½¿ç”¨ Groq)ã€‚")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# ==========================================
# 2. å®šç¾©å·¥å…· (Tools)
# ==========================================

@tool
def get_stock_price(ticker: str) -> str:
    """æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ã€‚åƒæ•¸ ticker æ˜¯è‚¡ç¥¨ä»£ç¢¼ï¼Œä¾‹å¦‚ '2330.TW' (å°ç©é›») æˆ– 'NVDA' (NVIDIA)ã€‚"""
    print(f"   ğŸ”§ [Tool: Stock] æŸ¥è©¢: {ticker}")
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="1d")
        if hist.empty: return f"æ‰¾ä¸åˆ° {ticker}"
        price = hist['Close'].iloc[-1]
        curr = stock.info.get('currency', '?')
        return f"{ticker} ç¾åƒ¹: {price:.2f} {curr}"
    except Exception as e:
        return f"è‚¡å¸‚æŸ¥è©¢éŒ¯èª¤: {e}"

@tool
def lookup_pdf_knowledge(query: str) -> str:
    """
    æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚
    ç•¶å•åŠè«–æ–‡ç´°ç¯€ã€ä½œè€…æˆ–æ ¸å¿ƒæ¦‚å¿µæ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ”§ [Tool: RAG] æª¢ç´¢ PDF: {query}")
    
    # æ³¨æ„ï¼šé€™è£¡çš„å°åŠ©æ‰‹ä¹Ÿè¦æ›æˆ Groqï¼
    llm_rag = get_llm()
    
    prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_rag
        | StrOutputParser()
    )
    return chain.invoke(query)

@tool
def search_web(query: str) -> str:
    """
    æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èã€å¤©æ°£æˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚
    ç„¡æ³•æŸ¥è‚¡åƒ¹æˆ– PDF æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    print(f"   ğŸ”§ [Tool: Web] ä¸Šç¶²æœå°‹: {query}")
    try:
        tool = TavilySearchResults(k=3)
        return tool.invoke(query)
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

# å·¥å…·åˆ—è¡¨
tools_list = [get_stock_price, lookup_pdf_knowledge, search_web]

# ==========================================
# 3. å»ºæ§‹ LangGraph (å«åæ€æ©Ÿåˆ¶)
# ==========================================

# A. æ“´å±• State ä»¥è¿½è¹¤è¿­ä»£æ¬¡æ•¸
class ReflectionState(MessagesState):
    """æ“´å±• MessagesState ä»¥è¿½è¹¤åæ€è¿­ä»£æ¬¡æ•¸"""
    iteration: int = 0  # è¿½è¹¤è¿­ä»£æ¬¡æ•¸ï¼ˆä¸ä½¿ç”¨ operator.addï¼Œç›´æ¥è¨­ç½®å€¼ï¼‰

# B. åˆå§‹åŒ–ä¸»å¤§è…¦ (Groq) ä¸¦ç¶å®šå·¥å…·
llm = get_llm()
llm_with_tools = llm.bind_tools(tools_list)

# C. å®šç¾©ç¯€é» (Nodes)

def agent_node(state: ReflectionState):
    """æ€è€ƒç¯€é»ï¼šç”Ÿæˆå›æ‡‰æˆ–æ±ºå®šå‘¼å«å·¥å…·"""
    messages = state["messages"]
    
    # æ·»åŠ ç°¡åŒ–çš„ç³»çµ±æç¤ºï¼Œä¸é‡è¤‡æè¿°å·¥å…·ï¼ˆå·¥å…·å®šç¾©å·²ç”± bind_tools æä¾›ï¼‰
    # é¿å…èˆ‡ bind_tools çš„å·¥å…·å®šç¾©è¡çªï¼Œå°è‡´ tool_use_failed éŒ¯èª¤
    system_prompt = SystemMessage(
        content="ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤ ä½¿ç”¨å·¥å…·ä¾†å›ç­”å•é¡Œã€‚\n"
        "ç•¶ç”¨æˆ¶è©¢å•éœ€è¦å¯¦æ™‚æ•¸æ“šçš„å•é¡Œæ™‚ï¼Œè«‹ä½¿ç”¨ç›¸æ‡‰çš„å·¥å…·ç²å–è³‡è¨Šã€‚"
    )
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç³»çµ±è¨Šæ¯ï¼Œé¿å…é‡è¤‡
    has_system = any(isinstance(msg, SystemMessage) for msg in messages)
    if not has_system:
        messages = [system_prompt] + messages
    
    try:
        response = llm_with_tools.invoke(messages)
        # ä¸æ›´æ–° iterationï¼Œä¿æŒç•¶å‰å€¼
        return {"messages": [response]}
    except Exception as e:
        # å¦‚æœå·¥å…·èª¿ç”¨æ ¼å¼éŒ¯èª¤ï¼Œå˜—è©¦ä¸ä½¿ç”¨å·¥å…·ç›´æ¥å›ç­”
        error_msg = str(e)
        error_type = type(e).__name__
        
        print(f"   âš ï¸ [Agent] ç™¼ç”ŸéŒ¯èª¤ ({error_type}): {error_msg[:200]}...")
        
        if "tool_use_failed" in error_msg or "Failed to call a function" in error_msg or "BadRequestError" in error_type:
            print(f"   ğŸ”„ [Agent] å·¥å…·èª¿ç”¨æ ¼å¼éŒ¯èª¤ï¼Œå˜—è©¦ä¸ä½¿ç”¨å·¥å…·ç›´æ¥å›ç­”...")
            # ç§»é™¤ç³»çµ±æç¤ºä¸­çš„å·¥å…·ç›¸é—œå…§å®¹ï¼Œæ”¹ç”¨ç°¡å–®æç¤º
            simple_messages = []
            for msg in messages:
                if isinstance(msg, SystemMessage):
                    # ç°¡åŒ–ç³»çµ±æç¤ºï¼Œä¸æåŠå·¥å…·
                    simple_msg = SystemMessage(
                        content="ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šç”¨æˆ¶çš„å•é¡Œæä¾›æœ‰ç”¨çš„å›ç­”ã€‚"
                    )
                    simple_messages.append(simple_msg)
                else:
                    simple_messages.append(msg)
            
            # ä½¿ç”¨ä¸ç¶å®šå·¥å…·çš„ LLM ä¾†ç”Ÿæˆå›æ‡‰
            try:
                response = llm.invoke(simple_messages)
                # æ·»åŠ ä¸€å€‹èªªæ˜ï¼Œå‘ŠçŸ¥ç”¨æˆ¶å·¥å…·èª¿ç”¨å¤±æ•—
                if response.content:
                    response.content = f"[è¨»ï¼šå·¥å…·èª¿ç”¨æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œä»¥ä¸‹æ˜¯åŸºæ–¼ç¾æœ‰çŸ¥è­˜çš„å›ç­”]\n\n{response.content}"
                return {"messages": [response]}
            except Exception as e2:
                # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œè¿”å›éŒ¯èª¤è¨Šæ¯
                error_response = AIMessage(
                    content=f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚é‡åˆ°æŠ€è¡“å•é¡Œã€‚\n\néŒ¯èª¤è©³æƒ…ï¼š{error_type}\n\nå»ºè­°ï¼šè«‹å˜—è©¦é‡æ–°è¡¨è¿°æ‚¨çš„å•é¡Œï¼Œæˆ–ç¨å¾Œå†è©¦ã€‚"
                )
                return {"messages": [error_response]}
        else:
            # å…¶ä»–éŒ¯èª¤ï¼Œç›´æ¥æ‹‹å‡º
            raise

def reflect_node(state: ReflectionState):
    """åæ€ç¯€é»ï¼šè©•ä¼°ç•¶å‰å›æ‡‰çš„å“è³ªï¼Œæ±ºå®šæ˜¯å¦éœ€è¦æ”¹é€²"""
    messages = state["messages"]
    iteration = state.get("iteration", 0)
    max_iterations = 5  # æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œé¿å…ç„¡é™å¾ªç’°
    
    # æª¢æŸ¥æ˜¯å¦è¶…éæœ€å¤§è¿­ä»£æ¬¡æ•¸
    if iteration >= max_iterations:
        print(f"   ğŸ”„ [Reflect] å·²é”æœ€å¤§è¿­ä»£æ¬¡æ•¸ ({max_iterations})ï¼ŒçµæŸåæ€å¾ªç’°ã€‚")
        return {"messages": []}  # ä¸æ·»åŠ æ–°è¨Šæ¯ï¼Œè®“æµç¨‹çµæŸ
    
    # å¢åŠ è¿­ä»£è¨ˆæ•¸
    current_iteration = iteration + 1
    
    # æ‰¾åˆ°æœ€å¾Œä¸€å€‹ AI å›æ‡‰ï¼ˆæ²’æœ‰ tool_calls çš„ï¼‰
    last_ai_message = None
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and not msg.tool_calls:
            last_ai_message = msg
            break
    
    if not last_ai_message or not last_ai_message.content:
        # å¦‚æœæ²’æœ‰æœ€çµ‚å›æ‡‰ï¼Œç¹¼çºŒæµç¨‹
        return {"messages": []}
    
    # æ§‹å»ºåæ€æç¤º
    reflection_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹åš´æ ¼çš„å“è³ªè©•ä¼°è€…ã€‚è«‹è©•ä¼°ä»¥ä¸‹ AI å›æ‡‰çš„å“è³ªã€‚"),
        ("human", """è«‹ä»”ç´°è©•ä¼°ä»¥ä¸‹ AI å›æ‡‰æ˜¯å¦å®Œæ•´ã€æº–ç¢ºåœ°å›ç­”äº†ç”¨æˆ¶çš„å•é¡Œã€‚

ç”¨æˆ¶å•é¡Œï¼š{user_question}

AI å›æ‡‰ï¼š{ai_response}

è«‹å›ç­”ï¼š
1. é€™å€‹å›æ‡‰æ˜¯å¦å®Œæ•´å›ç­”äº†ç”¨æˆ¶çš„å•é¡Œï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
2. å›æ‡‰ä¸­æ˜¯å¦æœ‰æ˜é¡¯çš„éŒ¯èª¤æˆ–éºæ¼ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
3. æ˜¯å¦éœ€è¦æ›´å¤šè³‡è¨Šæ‰èƒ½çµ¦å‡ºæ›´å¥½çš„å›ç­”ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰

è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
COMPLETE: [æ˜¯/å¦]
ACCURATE: [æ˜¯/å¦]
NEEDS_MORE: [æ˜¯/å¦]
REASON: [ç°¡çŸ­èªªæ˜åŸå› ]

å¦‚æœ COMPLETE=å¦ æˆ– ACCURATE=å¦ æˆ– NEEDS_MORE=æ˜¯ï¼Œå‰‡æ‡‰è©²ç¹¼çºŒæ”¹é€²å›ç­”ã€‚""")
    ])
    
    # æ‰¾åˆ°åŸå§‹ç”¨æˆ¶å•é¡Œ
    user_question = ""
    for msg in messages:
        if isinstance(msg, HumanMessage):
            user_question = msg.content
            break
    
    # åŸ·è¡Œåæ€è©•ä¼°
    try:
        reflection_chain = reflection_prompt | llm | StrOutputParser()
        reflection_result = reflection_chain.invoke({
            "user_question": user_question,
            "ai_response": last_ai_message.content
        })
    except Exception as e:
        error_str = str(e)
        error_type = type(e).__name__
        print(f"   âš ï¸ [Reflect] åæ€è©•ä¼°æ™‚ç™¼ç”ŸéŒ¯èª¤ ({error_type}): {error_str[:200]}...")
        # å¦‚æœåæ€å¤±æ•—ï¼Œç›´æ¥èªç‚ºå›æ‡‰å“è³ªè‰¯å¥½ï¼Œé¿å…ç„¡é™å¾ªç’°
        print(f"   âœ… [Reflect] åæ€è©•ä¼°å¤±æ•—ï¼Œå‡è¨­å›æ‡‰å“è³ªè‰¯å¥½ï¼ŒçµæŸåæ€å¾ªç’°ã€‚")
        return {"messages": []}  # ä¸æ·»åŠ æ–°è¨Šæ¯ï¼Œè®“æµç¨‹çµæŸ
    
    print(f"   ğŸ”„ [Reflect] ç¬¬ {current_iteration} æ¬¡åæ€è©•ä¼°ï¼š")
    print(f"      {reflection_result[:200]}...")  # åªé¡¯ç¤ºå‰200å­—
    
    # è§£æåæ€çµæœ
    needs_improvement = (
        "COMPLETE: å¦" in reflection_result or
        "ACCURATE: å¦" in reflection_result or
        "NEEDS_MORE: æ˜¯" in reflection_result
    )
    
    if needs_improvement and current_iteration < max_iterations:
        print(f"   ğŸ”„ [Reflect] æ±ºå®šï¼šéœ€è¦æ”¹é€²ï¼Œç¹¼çºŒæ€è€ƒæˆ–å‘¼å«å·¥å…·ã€‚")
        # æ·»åŠ åæ€è¨Šæ¯ï¼Œå¼•å° agent æ”¹é€²
        reflection_msg = HumanMessage(
            content=f"è«‹æ ¹æ“šä»¥ä¸‹åæ€æ”¹é€²ä½ çš„å›ç­”ï¼š\n{reflection_result}\n\nè«‹é‡æ–°æ€è€ƒä¸¦æä¾›æ›´å®Œæ•´ã€æº–ç¢ºçš„å›ç­”ã€‚å¦‚æœéœ€è¦çš„è©±ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·ç²å–æ›´å¤šè³‡è¨Šã€‚"
        )
        # æ›´æ–°è¿­ä»£è¨ˆæ•¸
        return {"messages": [reflection_msg], "iteration": current_iteration}
    else:
        if current_iteration >= max_iterations:
            print(f"   âš ï¸ [Reflect] å·²é”æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œåœæ­¢æ”¹é€²ã€‚")
        else:
            print(f"   âœ… [Reflect] æ±ºå®šï¼šå›æ‡‰å“è³ªè‰¯å¥½ï¼Œå¯ä»¥çµæŸã€‚")
        return {"messages": []}  # ä¸æ·»åŠ æ–°è¨Šæ¯ï¼Œè®“æµç¨‹çµæŸ

def should_continue(state: ReflectionState) -> str:
    """æ¢ä»¶åˆ¤æ–·ï¼šæ±ºå®šæ˜¯ç¹¼çºŒæ”¹é€²é‚„æ˜¯çµæŸ"""
    messages = state["messages"]
    
    # å¦‚æœæœ€å¾Œä¸€æ¢è¨Šæ¯æ˜¯ AI å›æ‡‰ä¸”æ²’æœ‰ tool_callsï¼Œé€²å…¥åæ€
    if messages:
        last_msg = messages[-1]
        if isinstance(last_msg, AIMessage) and not last_msg.tool_calls:
            return "reflect"  # é€²å…¥åæ€ç¯€é»
    
    # å¦‚æœæœ‰ tool_callsï¼ŒåŸ·è¡Œå·¥å…·
    if messages:
        last_msg = messages[-1]
        if isinstance(last_msg, AIMessage) and last_msg.tool_calls:
            return "tools"
    
    # å¦å‰‡çµæŸ
    return "end"

def should_continue_after_reflect(state: ReflectionState) -> str:
    """åæ€å¾Œçš„æ¢ä»¶åˆ¤æ–·ï¼šæ±ºå®šæ˜¯å›åˆ° agent é‚„æ˜¯çµæŸ"""
    messages = state["messages"]
    iteration = state.get("iteration", 0)
    max_iterations = 5  # æœ€å¤§è¿­ä»£æ¬¡æ•¸
    
    # å¦‚æœè¶…éæœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼ŒçµæŸ
    if iteration >= max_iterations:
        return "end"
    
    # å¦‚æœæœ‰æ–°çš„åæ€è¨Šæ¯ï¼Œå›åˆ° agent
    if messages and isinstance(messages[-1], HumanMessage):
        return "agent"
    
    # å¦å‰‡çµæŸ
    return "end"

# D. å»ºç«‹åœ–è¡¨ (Graph Construction)
builder = StateGraph(ReflectionState)

# æ·»åŠ ç¯€é»
builder.add_node("agent", agent_node)
builder.add_node("tools", ToolNode(tools_list))
builder.add_node("reflect", reflect_node)

# å®šç¾©æµç¨‹
builder.add_edge(START, "agent")

# Agent å¾Œï¼šæª¢æŸ¥æ˜¯å¦éœ€è¦å‘¼å«å·¥å…·æˆ–é€²å…¥åæ€
builder.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "reflect": "reflect",
        "end": END
    }
)

# å·¥å…·åŸ·è¡Œå¾Œï¼Œå›åˆ° agent
builder.add_edge("tools", "agent")

# åæ€å¾Œï¼šæ±ºå®šæ˜¯å›åˆ° agent æ”¹é€²é‚„æ˜¯çµæŸ
builder.add_conditional_edges(
    "reflect",
    should_continue_after_reflect,
    {
        "agent": "agent",
        "end": END
    }
)

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# ==========================================
# 4. åŸ·è¡Œä¸»ç¨‹å¼
# ==========================================
def main():
    print("\nğŸ¤– LangGraph Super Agent with Reflection (Engine: Groq Llama 3.3) ä¸Šç·šï¼")
    print("ğŸ‘‰ é€Ÿåº¦æœƒæ¯” Gemini å¿«å¾ˆå¤šï¼Œè«‹ç›¡æƒ…æ¸¬è©¦ã€‚")
    print("ğŸ”„ æ–°å¢åæ€æ©Ÿåˆ¶ï¼šAI æœƒè‡ªæˆ‘è©•ä¼°ä¸¦æ”¹é€²å›ç­”å“è³ªï¼")
    print("ğŸ’¡ è©¦è©¦ï¼š'å°ç©é›»è‚¡åƒ¹å¤šå°‘ï¼Ÿå¦å¤–ç¶²è·¯ä¸Šæœ‰ä»€éº¼é—œæ–¼å®ƒçš„æ–°èï¼Ÿ'\n")

    config = {"configurable": {"thread_id": "demo-groq-reflect-001"}}

    while True:
        try:
            user_input = input("User: ").strip()
            if user_input.lower() in ["quit", "exit"]:
                break
            if not user_input: continue

            print("   (Groq æ€è€ƒä¸­...)")
            
            last_printed_msg_id = None
            last_node = None
            
            # ä½¿ç”¨ stream_mode="updates" ä¾†è¿½è¹¤ç¯€é»è½‰æ›å’Œè¨Šæ¯
            is_first_agent = True  # æ¨™è¨˜æ˜¯å¦ç‚ºç¬¬ä¸€æ¬¡é€²å…¥ agent
            for event in graph.stream(
                {"messages": [HumanMessage(content=user_input)], "iteration": 0}, 
                config, 
                stream_mode="updates"
            ):
                # é¡¯ç¤ºç¯€é»è½‰æ›
                for node_name, node_state in event.items():
                    # é¡¯ç¤ºç¯€é»é€²å…¥æç¤º
                    if node_name != last_node:
                        if node_name == "reflect":
                            print(f"   ğŸ”„ [é€²å…¥åæ€ç¯€é»]")
                        elif node_name == "agent":
                            if not is_first_agent:  # åªåœ¨éé¦–æ¬¡é€²å…¥æ™‚é¡¯ç¤º
                                print(f"   ğŸ¤” [é‡æ–°æ€è€ƒä¸­...]")
                            is_first_agent = False
                        elif node_name == "tools":
                            print(f"   ğŸ”§ [é€²å…¥å·¥å…·ç¯€é»]")
                        last_node = node_name
                    
                    # è™•ç†è¨Šæ¯
                    if "messages" in node_state:
                        current_messages = node_state["messages"]
                        if not current_messages: continue
                        
                        last_msg = current_messages[-1]
                        # é¿å…é‡è¤‡æ‰“å°ç›¸åŒè¨Šæ¯
                        if hasattr(last_msg, 'id') and last_msg.id == last_printed_msg_id: 
                            continue
                        if hasattr(last_msg, 'id'):
                            last_printed_msg_id = last_msg.id

                        if isinstance(last_msg, AIMessage) and hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
                            tool_names = [tc.get("name", "unknown") for tc in last_msg.tool_calls]
                            print(f"   â¡ï¸ [Agent] æ±ºå®šå‘¼å«: {tool_names}")
                        
                        elif hasattr(last_msg, 'type') and last_msg.type == "tool":
                            tool_name = getattr(last_msg, 'name', 'unknown')
                            print(f"   â¡ï¸ [Tool] {tool_name} å®Œæˆã€‚")
                        
                        elif isinstance(last_msg, AIMessage) and (not hasattr(last_msg, 'tool_calls') or not last_msg.tool_calls) and hasattr(last_msg, 'content') and last_msg.content:
                            print(f"\nAI: {last_msg.content}\n")
                    
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()
import os
import yfinance as yf
from dotenv import load_dotenv

# --- LangChain Imports ---
# âŒ ç§»é™¤ ChatGoogleGenerativeAI
# from langchain_google_genai import ChatGoogleGenerativeAI

# âœ… æ–°å¢ Groq
from langchain_groq import ChatGroq

# Embedding æˆ‘å€‘æš«æ™‚ç¶­æŒ Googleï¼Œå› ç‚º Embedding çš„é¡åº¦è¨ˆç®—é€šå¸¸åˆ†é–‹ä¸”è¼ƒä¾¿å®œ
# å¦‚æœé€£ Embedding éƒ½çˆ†äº†ï¼Œå¯ä»¥æ”¹ç”¨ HuggingFaceEmbeddings (æœ¬åœ°ç«¯)
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools.tavily_search import TavilySearchResults

# --- LangGraph Imports ---
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ==========================================
# 0. è¨­å®š LLM (æ›´æ›å¼•æ“æ ¸å¿ƒ)
# ==========================================
def get_llm():
    """
    çµ±ä¸€ç®¡ç† LLM æ¨¡å‹ã€‚
    é€™è£¡ä½¿ç”¨ Groq çš„ Llama 3.3 70Bï¼Œå®ƒæ˜¯ç›®å‰é–‹æºç•Œæœ€å¼·çš„æ¨¡å‹ä¹‹ä¸€ï¼Œ
    éå¸¸æ“…é•· Tool Calling å’Œè¤‡é›œé‚è¼¯ã€‚
    """
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
        
    return ChatGroq(
        model="llama-3.3-70b-versatile", # Groq ç›®å‰æœ€å¼·çš„æ¨¡å‹
        temperature=0,
        max_retries=2,
    )

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ–ï¼šå…¨åŸŸè³‡æº (PDF VectorStore)
# ==========================================
print("ğŸš€ [System] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
pdf_path = "./data/Tree_of_Thoughts.pdf"

retriever = None
if os.path.exists(pdf_path):
    # å¦‚æœ Google Embedding ä¹Ÿçˆ†é¡åº¦ï¼Œè«‹æ”¹ç”¨: from langchain_community.embeddings import HuggingFaceEmbeddings
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… PDF è¼‰å…¥å®Œæˆ (Embedding ä½¿ç”¨ Googleï¼Œæ¨è«–ä½¿ç”¨ Groq)ã€‚")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# ==========================================
# 2. å®šç¾©å·¥å…· (Tools)
# ==========================================

@tool
def get_stock_price(ticker: str) -> str:
    """
    æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ã€‚
    è¼¸å…¥åƒæ•¸ ticker å¿…é ˆæ˜¯è‚¡ç¥¨ä»£ç¢¼ (å¦‚ 2330.TW, NVDA, GOOG)ã€‚
    """
    print(f"   ğŸ”§ [Tool: Stock] æŸ¥è©¢: {ticker}")
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="1d")
        if hist.empty: return f"æ‰¾ä¸åˆ° {ticker}"
        price = hist['Close'].iloc[-1]
        curr = stock.info.get('currency', '?')
        return f"{ticker} ç¾åƒ¹: {price:.2f} {curr}"
    except Exception as e:
        return f"è‚¡å¸‚æŸ¥è©¢éŒ¯èª¤: {e}"

@tool
def lookup_pdf_knowledge(query: str) -> str:
    """
    æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚
    ç•¶å•åŠè«–æ–‡ç´°ç¯€ã€ä½œè€…æˆ–æ ¸å¿ƒæ¦‚å¿µæ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ”§ [Tool: RAG] æª¢ç´¢ PDF: {query}")
    
    # æ³¨æ„ï¼šé€™è£¡çš„å°åŠ©æ‰‹ä¹Ÿè¦æ›æˆ Groqï¼
    llm_rag = get_llm()
    
    prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_rag
        | StrOutputParser()
    )
    return chain.invoke(query)

@tool
def search_web(query: str) -> str:
    """
    æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èã€å¤©æ°£æˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚
    ç„¡æ³•æŸ¥è‚¡åƒ¹æˆ– PDF æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    print(f"   ğŸ”§ [Tool: Web] ä¸Šç¶²æœå°‹: {query}")
    try:
        tool = TavilySearchResults(k=3)
        return tool.invoke(query)
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

# å·¥å…·åˆ—è¡¨
tools_list = [get_stock_price, lookup_pdf_knowledge, search_web]

# ==========================================
# 3. å»ºæ§‹ LangGraph
# ==========================================

# A. åˆå§‹åŒ–ä¸»å¤§è…¦ (Groq) ä¸¦ç¶å®šå·¥å…·
llm = get_llm()
llm_with_tools = llm.bind_tools(tools_list)

# B. å®šç¾©ç¯€é» (Nodes)
def agent_node(state: MessagesState):
    """æ€è€ƒç¯€é»"""
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}

# C. å»ºç«‹åœ–è¡¨ (Graph Construction)
builder = StateGraph(MessagesState)

builder.add_node("agent", agent_node)
builder.add_node("tools", ToolNode(tools_list))

builder.add_edge(START, "agent")
builder.add_conditional_edges("agent", tools_condition)
builder.add_edge("tools", "agent")

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# ==========================================
# 4. åŸ·è¡Œä¸»ç¨‹å¼
# ==========================================
def main():
    print("\nğŸ¤– LangGraph Super Agent (Engine: Groq Llama 3.3) ä¸Šç·šï¼")
    print("ğŸ‘‰ é€Ÿåº¦æœƒæ¯” Gemini å¿«å¾ˆå¤šï¼Œè«‹ç›¡æƒ…æ¸¬è©¦ã€‚")
    print("ğŸ’¡ è©¦è©¦ï¼š'å°ç©é›»è‚¡åƒ¹å¤šå°‘ï¼Ÿå¦å¤–ç¶²è·¯ä¸Šæœ‰ä»€éº¼é—œæ–¼å®ƒçš„æ–°èï¼Ÿ'\n")

    config = {"configurable": {"thread_id": "demo-groq-001"}}

    while True:
        try:
            user_input = input("User: ").strip()
            if user_input.lower() in ["quit", "exit"]:
                break
            if not user_input: continue

            print("   (Groq æ€è€ƒä¸­...)")
            
            last_printed_msg_id = None
            
            # ä½¿ç”¨ stream_mode="values"
            for event in graph.stream({"messages": [HumanMessage(content=user_input)]}, config, stream_mode="values"):
                current_messages = event["messages"]
                if not current_messages: continue
                
                last_msg = current_messages[-1]
                if last_msg.id == last_printed_msg_id: continue
                last_printed_msg_id = last_msg.id

                if last_msg.type == "ai" and last_msg.tool_calls:
                    tool_names = [tc["name"] for tc in last_msg.tool_calls]
                    print(f"   â¡ï¸ [Agent] æ±ºå®šå‘¼å«: {tool_names}")
                
                elif last_msg.type == "tool":
                    print(f"   â¡ï¸ [Tool] {last_msg.name} å®Œæˆã€‚")

                elif last_msg.type == "ai" and not last_msg.tool_calls:
                    print(f"\nAI: {last_msg.content}\n")
                    
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
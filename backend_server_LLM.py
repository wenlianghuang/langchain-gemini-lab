import os
import uvicorn
import yfinance as yf
from dotenv import load_dotenv

# --- FastAPI & LangServe Imports ---
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from langserve import add_routes

# --- LangChain & LangGraph Imports ---
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools.tavily_search import TavilySearchResults

from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver

# Pydantic ç”¨æ–¼å®šç¾©è¼¸å…¥ä»‹é¢
from pydantic import BaseModel
from typing import List, Union

load_dotenv()

# ==========================================
# 0. è¨­å®š LLM
# ==========================================
def get_llm():
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
    return ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0,
        max_retries=2,
    )

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ– (ç¶­æŒä¸è®Š)
# ==========================================
print("ğŸš€ [Server] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
pdf_path = "./data/Tree_of_Thoughts.pdf"
retriever = None
if os.path.exists(pdf_path):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… PDF è¼‰å…¥å®Œæˆã€‚")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# ==========================================
# 2. å®šç¾©å·¥å…· (ç¶­æŒä¸è®Š)
# ==========================================
@tool
def get_stock_price(ticker: str) -> str:
    """æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ (ä¾‹å¦‚ 2330.TW, NVDA)ã€‚"""
    print(f"   ğŸ”§ [Tool: Stock] Server æ­£åœ¨æŸ¥è©¢: {ticker}")
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="1d")
        if hist.empty: return f"æ‰¾ä¸åˆ° {ticker}"
        price = hist['Close'].iloc[-1]
        curr = stock.info.get('currency', '?')
        return f"{ticker} ç¾åƒ¹: {price:.2f} {curr}"
    except Exception as e:
        return f"è‚¡å¸‚æŸ¥è©¢éŒ¯èª¤: {e}"

@tool
def lookup_pdf_knowledge(query: str) -> str:
    """æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚"""
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ”§ [Tool: RAG] Server æ­£åœ¨æª¢ç´¢ PDF: {query}")
    llm_rag = get_llm()
    prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_rag
        | StrOutputParser()
    )
    return chain.invoke(query)

@tool
def search_web(query: str) -> str:
    """æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚"""
    print(f"   ğŸ”§ [Tool: Web] Server æ­£åœ¨æœå°‹: {query}")
    try:
        search_tool = TavilySearchResults(k=3)
        results = search_tool.invoke(query)
        # TavilySearchResults è¿”å›åˆ—è¡¨ï¼Œéœ€è¦è½‰æ›ç‚ºå­—ç¬¦ä¸²
        if isinstance(results, list):
            formatted_results = []
            for item in results:
                if isinstance(item, dict):
                    title = item.get("title", "ç„¡æ¨™é¡Œ")
                    url = item.get("url", "")
                    content = item.get("content", "")
                    formatted_results.append(f"æ¨™é¡Œ: {title}\nç¶²å€: {url}\nå…§å®¹: {content}")
                else:
                    formatted_results.append(str(item))
            return "\n\n".join(formatted_results)
        return str(results)
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

tools_list = [get_stock_price, lookup_pdf_knowledge, search_web]

# ==========================================
# 3. å®šç¾©è¼¸å…¥ä»‹é¢ (è®“ Playground è®Šæ¼‚äº®)
# ==========================================
class AgentInput(BaseModel):
    # é€™æœƒè®“ Playground é¡¯ç¤ºä¸€å€‹å‹å–„çš„ "Messages" åˆ—è¡¨è¼¸å…¥æ¡†
    # æ³¨æ„ï¼šToolMessage æ˜¯ç³»çµ±å…§éƒ¨ä½¿ç”¨çš„ï¼Œä¸æ‡‰è©²å‡ºç¾åœ¨ç”¨æˆ¶è¼¸å…¥ä¸­
    messages: List[Union[HumanMessage, AIMessage, SystemMessage]]

# ==========================================
# 4. å»ºæ§‹ LangGraph
# ==========================================
def create_agent_graph():
    llm = get_llm()
    llm_with_tools = llm.bind_tools(tools_list)

    def agent_node(state: MessagesState):
        messages = state["messages"]
        
        # èª¿è©¦ï¼šæ‰“å°ç•¶å‰æ¶ˆæ¯ç‹€æ…‹
        print(f"\nğŸ” [Agent Node] æ”¶åˆ° {len(messages)} æ¢æ¶ˆæ¯")
        for i, msg in enumerate(messages):
            msg_type = type(msg).__name__
            has_tool_calls = hasattr(msg, 'tool_calls') and msg.tool_calls
            has_content = hasattr(msg, 'content') and msg.content
            print(f"   æ¶ˆæ¯ {i}: {msg_type}, æœ‰ tool_calls: {bool(has_tool_calls)}, æœ‰ content: {bool(has_content)}")
        
        # ç¢ºä¿ç¬¬ä¸€æ¢è¨Šæ¯æ˜¯ç³»çµ±æç¤ºï¼Œå¼•å°æ¨¡å‹æ­£ç¢ºä½¿ç”¨å·¥å…·
        # æ³¨æ„ï¼šéœ€è¦æª¢æŸ¥æ˜¯å¦å·²æœ‰ç³»çµ±è¨Šæ¯ï¼Œé¿å…é‡è¤‡æ·»åŠ 
        has_system_msg = any(isinstance(msg, SystemMessage) for msg in messages)
        if not has_system_msg:
            system_msg = SystemMessage(
                content="ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·ä¾†å›ç­”å•é¡Œã€‚\n\n"
                "å¯ç”¨å·¥å…·ï¼š\n"
                "1. get_stock_price(ticker: str) - æŸ¥è©¢è‚¡ç¥¨åƒ¹æ ¼ã€‚"
                "   è‚¡ç¥¨ä»£ç¢¼æ ¼å¼ï¼šå°ç©é›»ä½¿ç”¨ '2330.TW'ï¼ŒNVIDIA ä½¿ç”¨ 'NVDA'ï¼Œ"
                "   å…¶ä»–å°ç£è‚¡ç¥¨æ ¼å¼ç‚º 'è‚¡ç¥¨ä»£ç¢¼.TW'ï¼Œç¾åœ‹è‚¡ç¥¨ç›´æ¥ä½¿ç”¨è‚¡ç¥¨ä»£ç¢¼ã€‚\n"
                "2. lookup_pdf_knowledge(query: str) - æŸ¥è©¢PDFçŸ¥è­˜åº«ã€‚\n"
                "3. search_web(query: str) - æœå°‹ç¶²è·¯è³‡è¨Šã€‚\n\n"
                "è«‹æ ¹æ“šç”¨æˆ¶å•é¡Œé¸æ“‡åˆé©çš„å·¥å…·ï¼Œä¸¦ç¢ºä¿åƒæ•¸æ ¼å¼æ­£ç¢ºã€‚"
            )
            messages = [system_msg] + messages
        
        try:
            response = llm_with_tools.invoke(messages)
            
            # èª¿è©¦ï¼šæ‰“å°éŸ¿æ‡‰ä¿¡æ¯
            has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
            has_content = hasattr(response, 'content') and response.content
            print(f"âœ… [Agent Node] ç”ŸæˆéŸ¿æ‡‰: æœ‰ tool_calls: {bool(has_tool_calls)}, æœ‰ content: {bool(has_content)}")
            if has_content:
                content_preview = str(response.content)[:100] if response.content else ""
                print(f"   å…§å®¹é è¦½: {content_preview}...")
            
            return {"messages": [response]}
        except Exception as e:
            error_str = str(e)
            print(f"âŒ [Agent Node] éŒ¯èª¤: {error_str}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºå·¥å…·å‘¼å«æ ¼å¼éŒ¯èª¤
            if "Failed to call a function" in error_str or "tool_use_failed" in error_str:
                # å˜—è©¦ä¸ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥å›ç­”
                try:
                    llm_without_tools = get_llm()
                    fallback_response = llm_without_tools.invoke(messages)
                    return {"messages": [fallback_response]}
                except Exception as fallback_error:
                    # å¦‚æœé‚„æ˜¯å¤±æ•—ï¼Œè¿”å›éŒ¯èª¤è¨Šæ¯
                    from langchain_core.messages import AIMessage
                    error_msg = AIMessage(
                        content=f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹ç¨å¾Œå†è©¦æˆ–é‡æ–°è¡¨è¿°æ‚¨çš„å•é¡Œã€‚\néŒ¯èª¤è©³æƒ…ï¼š{str(fallback_error)[:200]}"
                    )
                    return {"messages": [error_msg]}
            else:
                # å…¶ä»–éŒ¯èª¤ï¼Œç›´æ¥è¿”å›éŒ¯èª¤è¨Šæ¯
                from langchain_core.messages import AIMessage
                error_msg = AIMessage(
                    content=f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error_str[:200]}"
                )
                return {"messages": [error_msg]}

    # åŒ…è£… ToolNode ä»¥æ·»åŠ è°ƒè¯•æ—¥å¿—
    def tools_node_with_logging(state: MessagesState):
        print(f"\nğŸ”§ [Tools Node] å¼€å§‹æ‰§è¡Œå·¥å…·...")
        result = ToolNode(tools_list).invoke(state)
        print(f"âœ… [Tools Node] å·¥å…·æ‰§è¡Œå®Œæˆï¼Œè¿”å› {len(result.get('messages', []))} æ¡æ¶ˆæ¯")
        return result
    
    builder = StateGraph(MessagesState)
    builder.add_node("agent", agent_node)
    builder.add_node("tools", tools_node_with_logging)

    builder.add_edge(START, "agent")
    builder.add_conditional_edges("agent", tools_condition)
    builder.add_edge("tools", "agent")

    memory = MemorySaver()
    graph = builder.compile(checkpointer=memory)
    
    # âœ¨ é—œéµ 1ï¼šç¶å®š Input Schemaï¼Œé€™æœƒè®“ Screenshot 2 çš„ä»‹é¢å‡ºç¾æ¼‚äº®çš„è¼¸å…¥æ¡†
    graph = graph.with_types(input_type=AgentInput)
    
    # âœ¨ é—œéµ 2ï¼šç›´æ¥ç¶å®šè¨˜æ†¶ IDï¼
    # é€™æ¨£ä½ åœ¨ç¶²é ä¸Šå°±ã€Œä¸ç”¨ã€æ‰¾ Configurable äº†ï¼Œç³»çµ±æœƒè‡ªå‹•ä½¿ç”¨é€™å€‹ ID
    graph_with_config = graph.with_config(configurable={"thread_id": "web-user-demo"})
    
    return graph_with_config

# ==========================================
# 5. å»ºç«‹ FastAPI æ‡‰ç”¨
# ==========================================
app = FastAPI(
    title="LangGraph Super Agent",
    version="1.0",
    description="LangGraph Agent API",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

graph = create_agent_graph()

# âœ¨ é—œéµ 3ï¼šä½¿ç”¨ "default" Playground
# é€™æ˜¯æœ€ç©©å®šçš„æ¨¡å¼ï¼Œä¸æœƒå‡ºç¾ Screenshot 1 çš„éŒ¯èª¤
add_routes(
    app,
    graph,
    path="/agent",
    playground_type="default", 
)

# æ·»åŠ è°ƒè¯•ç«¯ç‚¹ï¼Œç”¨äºæµ‹è¯•æµå¼å“åº”
@app.get("/debug/stream-test")
async def debug_stream_test():
    """æµ‹è¯•æµå¼å“åº”æ ¼å¼"""
    from fastapi.responses import StreamingResponse
    import json
    
    async def generate():
        # æ¨¡æ‹Ÿ LangServe çš„æµå¼å“åº”æ ¼å¼
        test_data = {
            "event": "data",
            "data": {
                "output": {
                    "messages": [
                        {
                            "type": "ai",
                            "content": "è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ¶ˆæ¯",
                            "id": "test-1"
                        }
                    ]
                }
            }
        }
        yield f"data: {json.dumps(test_data)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")

if __name__ == "__main__":
    print("\nğŸš€ Server å•Ÿå‹•ä¸­...")
    print("ğŸ‘‰ LangServe Playground: http://localhost:8000/agent/playground/")
    print("ğŸ‘‰ å‰ç«¯æ‡‰ç”¨: http://localhost:3000")
    print("ğŸ‘‰ API ç«¯é»: http://localhost:8000/agent")
    print("ğŸ‘‰ æµå¼ç«¯é»: http://localhost:8000/agent/stream")
    print("ğŸ‘‰ èª¿è©¦ç«¯é»: http://localhost:8000/debug/stream-test")
    uvicorn.run(app, host="0.0.0.0", port=8000)
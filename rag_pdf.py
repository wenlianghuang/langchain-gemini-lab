import os
from dotenv import load_dotenv

# 1. è¼‰å…¥å¿…è¦çš„ LangChain å…ƒä»¶
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# æ–°å¢ï¼šè¼‰å…¥ PDF Loader
from langchain_community.document_loaders import PyPDFLoader

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def main():
    pdf_path = "./data/Tree_of_Thoughts.pdf"
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(pdf_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {pdf_path}")
        print("è«‹åœ¨å°ˆæ¡ˆç›®éŒ„ä¸‹å»ºç«‹ 'data' è³‡æ–™å¤¾ï¼Œä¸¦æ”¾å…¥ä¸€å€‹åç‚º 'Tree_of_Thought.pdf' çš„æª”æ¡ˆã€‚")
        return

    print(f"ğŸ“„ æ­£åœ¨è®€å–ä¸¦è™•ç† PDFï¼š{pdf_path} ...")

    # --- 1. Load (è¼‰å…¥) ---
    # ä½¿ç”¨ PyPDFLoader è®€å–æª”æ¡ˆï¼Œå®ƒæœƒæŠŠæ¯ä¸€é è®Šæˆä¸€å€‹ Document ç‰©ä»¶
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    print(f"   -> æˆåŠŸè¼‰å…¥ï¼Œå…± {len(docs)} é ã€‚")

    # --- 2. Split (åˆ‡å‰²) ---
    # PDF é€šå¸¸å…§å®¹è¼ƒå¤šï¼Œæˆ‘å€‘éœ€è¦åˆ‡å¾—æ›´ç´°ç·»
    # chunk_size=1000: æ¯ 1000 å€‹å­—å…ƒåˆ‡ä¸€å¡Š
    # chunk_overlap=200: å‰å¾Œä¿ç•™ 200 å­—é‡ç–Šï¼Œé¿å…åˆ‡åˆ°ä¸€åŠèªæ„ä¸­æ–·
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    print(f"   -> åˆ‡å‰²å®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(splits)} å€‹æ–‡å­—å¡Š (Chunks)ã€‚")

    # --- 3. Embed & Store (å‘é‡åŒ–èˆ‡å„²å­˜) ---
    print("ğŸ§  æ­£åœ¨å°‡æ–‡å­—è½‰ç‚ºå‘é‡ä¸¦å­˜å…¥è³‡æ–™åº« (é€™å¯èƒ½éœ€è¦å¹¾ç§’é˜)...")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    # å»ºç«‹å‘é‡è³‡æ–™åº«
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(
        search_type="similarity", # ä½¿ç”¨ç›¸ä¼¼åº¦æœå°‹
        search_kwargs={"k": 3}    # æ¯æ¬¡åªæ‰¾æœ€ç›¸é—œçš„ã€Œ3å€‹ã€ç‰‡æ®µçµ¦ AI åƒè€ƒ
    )

    # --- 4. Define Chain (å®šç¾©æµç¨‹) ---
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)

    template = """æ‚¨æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡ä»¶åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ã€åƒè€ƒæ–‡ä»¶ã€‘ç‰‡æ®µä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    
    æ³¨æ„ï¼š
    1. è«‹åªæ ¹æ“šæä¾›çš„å…§å®¹å›ç­”ï¼Œä¸è¦ä½¿ç”¨æ‚¨åŸæœ¬çš„å¤–éƒ¨çŸ¥è­˜ã€‚
    2. å¦‚æœæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè«‹è€å¯¦èªªã€Œæ–‡ä»¶ä¸­æœªæåŠç›¸é—œè³‡è¨Šã€ã€‚
    3. å›ç­”è«‹ä¿æŒç°¡æ½”æœ‰åŠ›ã€‚

    ã€åƒè€ƒæ–‡ä»¶ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("\nâœ… ç³»çµ±å°±ç·’ï¼æ‚¨å¯ä»¥é–‹å§‹è©¢å•é—œæ–¼é€™ä»½ PDF çš„å•é¡Œäº† (è¼¸å…¥ 'exit' é›¢é–‹)ï¼š\n")

    # --- 5. äº’å‹•è¿´åœˆ ---
    while True:
        user_input = input("è«‹è¼¸å…¥å•é¡Œ: ")
        if user_input.lower() in ["exit", "quit", "bye"]:
            print("ğŸ‘‹ å†è¦‹ï¼")
            break
        
        if not user_input.strip():
            continue

        print("ğŸ¤– æ€è€ƒä¸­...", end="", flush=True)
        response = rag_chain.invoke(user_input)
        print(f"\rå›ç­”ï¼š{response}\n")
        print("-" * 30)

if __name__ == "__main__":
    main()
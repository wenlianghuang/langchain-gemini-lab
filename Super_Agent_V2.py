import os
import yfinance as yf
from dotenv import load_dotenv

# LangChain Imports
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# âœ¨ æ–°å¢ï¼šå¼•å…¥ Tavily æœå°‹å·¥å…·
from langchain_community.tools.tavily_search import TavilySearchResults

load_dotenv()

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ–ï¼šé å…ˆè¼‰å…¥ PDF
# ==========================================
print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ä¸‰åˆä¸€ Super Agent (PDF + Stock + Web)...")

pdf_path = "./data/Tree_of_Thoughts.pdf"
if not os.path.exists(pdf_path):
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° PDF: {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡å—é™ã€‚")
    retriever = None
else:
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… PDF è¼‰å…¥å®Œæˆ")

# ==========================================
# 2. å®šç¾©å·¥å…· (Tools)
# ==========================================

# --- å·¥å…· A: æŸ¥è‚¡åƒ¹ ---
@tool
def get_stock_price(ticker: str) -> str:
    """
    æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ã€‚
    è¼¸å…¥åƒæ•¸ ticker å¿…é ˆæ˜¯è‚¡ç¥¨ä»£ç¢¼ã€‚
    å¦‚æœæ˜¯å°è‚¡ï¼Œè«‹åœ¨ä»£ç¢¼å¾ŒåŠ ä¸Š .TW (ä¾‹å¦‚ 2330.TW)ã€‚
    å¦‚æœæ˜¯ç¾è‚¡ï¼Œç›´æ¥è¼¸å…¥ä»£ç¢¼ (ä¾‹å¦‚ AAPL, TSLA, GOOG)ã€‚
    """
    print(f"\nğŸ”§ [Tool: Stock] æŸ¥è©¢è‚¡åƒ¹: {ticker} ...")
    try:
        stock = yf.Ticker(ticker)
        history = stock.history(period="1d")
        if history.empty:
            return f"æ‰¾ä¸åˆ°è‚¡ç¥¨ä»£ç¢¼ {ticker} çš„è³‡æ–™ã€‚"
        current_price = history['Close'].iloc[-1]
        currency = stock.info.get('currency', 'Unknown')
        return f"{ticker} ç›®å‰åƒ¹æ ¼ç‚º {current_price:.2f} {currency}"
    except Exception as e:
        return f"æŸ¥è©¢å¤±æ•—: {e}"

# --- å·¥å…· B: æŸ¥ PDF (RAG) ---
@tool
def lookup_pdf_knowledge(query: str) -> str:
    """
    æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚
    åªæœ‰ç•¶ä½¿ç”¨è€…å•åˆ°é—œæ–¼ ToTã€æ€ç¶­æ¨¹ã€Prompt Engineering æˆ–è«–æ–‡ç´°ç¯€æ™‚ï¼Œæ‰ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    if retriever is None:
        return "PDF è³‡æ–™åº«æœªè¼‰å…¥ï¼Œç„¡æ³•æŸ¥è©¢ã€‚"
        
    print(f"\nğŸ”§ [Tool: RAG] æŸ¥è©¢å…§éƒ¨æ–‡ä»¶: {query} ...")
    llm_for_rag = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
    template = "è«‹æ ¹æ“šä»¥ä¸‹æ–‡ä»¶ç‰‡æ®µå›ç­”å•é¡Œï¼š\n{context}\nå•é¡Œï¼š{question}"
    prompt = ChatPromptTemplate.from_template(template)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_for_rag
        | StrOutputParser()
    )
    return rag_chain.invoke(query)

# --- âœ¨ å·¥å…· C: æŸ¥ç¶²è·¯ (Web Search) ---
@tool
def search_web(query: str) -> str:
    """
    æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°è³‡è¨Šã€æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚
    ç•¶ä½¿ç”¨è€…çš„å•é¡Œç„¡æ³•é€éå…§éƒ¨æ–‡ä»¶ (PDF) æˆ– è‚¡ç¥¨å·¥å…· å›ç­”æ™‚ï¼Œ
    æˆ–è€…ä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ã€Œæœå°‹ç¶²è·¯ã€ã€ã€Œæ–°èã€æ™‚ï¼Œè«‹ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    print(f"\nğŸ”§ [Tool: Web] æ­£åœ¨ä¸Šç¶²æœå°‹: {query} ...")
    try:
        # k=3 ä»£è¡¨å›å‚³ 3 ç­†çµæœ
        search = TavilySearchResults(k=3)
        # TavilySearchResults æœ¬èº«å°±æ˜¯ä¸€å€‹ Toolï¼Œæˆ‘å€‘å¯ä»¥ç›´æ¥å‘¼å« invoke
        results = search.invoke(query)
        
        # ç°¡å–®æ•´ç†å›å‚³æ ¼å¼
        response_text = ""
        for res in results:
            response_text += f"- ä¾†æº: {res['url']}\n  å…§å®¹: {res['content']}\n"
        return response_text
    except Exception as e:
        return f"ç¶²è·¯æœå°‹å¤±æ•—: {e}"

# ==========================================
# 3. ä¸»ç¨‹å¼ Loop
# ==========================================
def main():
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
    
    # âœ¨ é—œéµï¼šæŠŠä¸‰å€‹å·¥å…·éƒ½åŠ é€²å»ï¼
    tools = [get_stock_price, lookup_pdf_knowledge, search_web]
    llm_with_tools = llm.bind_tools(tools)
    
    messages = []
    
    print("\nğŸ¤– å…¨èƒ½ Agent ä¸Šç·šï¼(æ”¯æ´ï¼šè‚¡åƒ¹ã€PDFã€ç¶²è·¯æœå°‹)")
    print("ğŸ‘‰ è©¦è©¦çœ‹ï¼š'å°ç©é›»ä»Šå¤©è‚¡åƒ¹å¤šå°‘ï¼Ÿæœ€è¿‘æœ‰ä»€éº¼é—œæ–¼å®ƒçš„æ–°èï¼Ÿ'")
    
    while True:
        try:
            user_input = input("\nUser: ").strip()
            if user_input.lower() in ["exit", "quit"]:
                break
            if not user_input:
                continue

            messages.append(HumanMessage(content=user_input))

            # éšæ®µ 1: æ€è€ƒèˆ‡æ±ºç­–
            ai_decision = llm_with_tools.invoke(messages)
            messages.append(ai_decision)

            if ai_decision.tool_calls:
                print(f"\nğŸ¤– AI æ±ºå®šä½¿ç”¨ {len(ai_decision.tool_calls)} å€‹å·¥å…·...")
                
                for tool_call in ai_decision.tool_calls:
                    # å»ºç«‹å·¥å…·å°ç…§è¡¨
                    tool_map = {
                        "get_stock_price": get_stock_price,
                        "lookup_pdf_knowledge": lookup_pdf_knowledge,
                        "search_web": search_web
                    }
                    
                    selected_tool = tool_map.get(tool_call["name"])
                    if selected_tool:
                        tool_output = selected_tool.invoke(tool_call["args"])
                        messages.append(ToolMessage(tool_output, tool_call_id=tool_call["id"]))
                
                # éšæ®µ 2: æ•´åˆå›ç­” (Streaming)
                print("ğŸ’¡ AI: ", end="", flush=True)
                full_response = ""
                for chunk in llm_with_tools.stream(messages):
                    content = chunk.content
                    # é˜²ç¦¦æ€§æª¢æŸ¥
                    text = content if isinstance(content, str) else str(content)
                    if text:
                        print(text, end="", flush=True)
                        full_response += text
                messages.append(AIMessage(content=full_response))
                
            else:
                # æ²’ç”¨å·¥å…·
                print(f"AI: {ai_decision.content}")

        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
import os
from dotenv import load_dotenv
import torch

# è¨­å®š HuggingFace æ¨¡åž‹ç·©å­˜ç›®éŒ„åˆ°å¤–æŽ¥ SSD
EXTERNAL_SSD_PATH = "/Volumes/T7_SSD"
HF_CACHE_DIR = os.path.join(EXTERNAL_SSD_PATH, "huggingface_cache")

# æª¢æŸ¥å¤–æŽ¥ SSD æ˜¯å¦å­˜åœ¨
if os.path.exists(EXTERNAL_SSD_PATH):
    # å‰µå»ºç·©å­˜ç›®éŒ„ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰
    os.makedirs(HF_CACHE_DIR, exist_ok=True)
    # è¨­ç½® HuggingFace ç’°å¢ƒè®Šæ•¸ï¼ˆå¿…é ˆåœ¨å°Žå…¥ HuggingFace ç›¸é—œåº«ä¹‹å‰è¨­ç½®ï¼‰
    os.environ["HF_HOME"] = HF_CACHE_DIR
    os.environ["TRANSFORMERS_CACHE"] = os.path.join(HF_CACHE_DIR, "transformers")
    os.environ["HF_HUB_CACHE"] = os.path.join(HF_CACHE_DIR, "hub")
    print(f"ðŸ’¾ æ¨¡åž‹ç·©å­˜ç›®éŒ„ï¼š{HF_CACHE_DIR}")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°å¤–æŽ¥ SSD {EXTERNAL_SSD_PATH}ï¼Œå°‡ä½¿ç”¨é è¨­ç·©å­˜ç›®éŒ„")

# è¼‰å…¥ LangChain å…ƒä»¶ï¼ˆä½¿ç”¨æœ€æ–°çš„ LCEL APIï¼‰
# âœ… ä½¿ç”¨ Groq æ›¿ä»£ Google Generative AIï¼Œé¿å…é¡åº¦å•é¡Œ
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def get_device():
    """è‡ªå‹•æª¢æ¸¬å¯ç”¨çš„è¨­å‚™ï¼ˆå„ªå…ˆä½¿ç”¨ Apple Silicon GPUï¼‰"""
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def main():
    pdf_path = "./data/Tree_of_Thoughts.pdf"
    if not os.path.exists(pdf_path):
        print("âŒ æ‰¾ä¸åˆ° PDF æª”æ¡ˆï¼Œè«‹ç¢ºèª data/Tree_of_Thoughts.pdf å­˜åœ¨ã€‚")
        return

    print("ðŸš€ åˆå§‹åŒ–å…·å‚™ã€Œè¨˜æ†¶åŠŸèƒ½ã€çš„ RAG ç³»çµ±ï¼ˆä½¿ç”¨ Jina Embeddings v3 å¤šèªžè¨€ç‰ˆ + Groq LLM + LCEL APIï¼‰...")

    # --- 1. æº–å‚™è³‡æ–™ (ä½¿ç”¨ Jina Embeddings) ---
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    # ä½¿ç”¨ Jina Embeddings (é–‹æºï¼Œç„¡é¡åº¦é™åˆ¶)
    device = get_device()
    device_name = "Apple Silicon GPU (MPS)" if device == "mps" else ("NVIDIA GPU (CUDA)" if device == "cuda" else "CPU")
    print(f"ðŸ“¦ æ­£åœ¨è¼‰å…¥ Jina Embeddings æ¨¡åž‹ï¼ˆé¦–æ¬¡ä½¿ç”¨æœƒä¸‹è¼‰æ¨¡åž‹ï¼Œè«‹ç¨å€™ï¼‰...")
    print(f"ðŸ”§ ä½¿ç”¨è¨­å‚™ï¼š{device_name}")
    
    # è¨­å®šç·©å­˜ç›®éŒ„
    cache_folder = None
    if os.path.exists(EXTERNAL_SSD_PATH):
        cache_folder = os.path.join(HF_CACHE_DIR, "transformers")
        os.makedirs(cache_folder, exist_ok=True)
        print(f"ðŸ’¾ æ¨¡åž‹å°‡ä¸‹è¼‰åˆ°ï¼š{cache_folder}")
    
    # æº–å‚™ model_kwargsï¼ŒåŒ…å« trust_remote_code å’Œ device
    model_kwargs = {
        "device": device,  # è‡ªå‹•ä½¿ç”¨ MPS (Apple GPU) æˆ– CPU
        "trust_remote_code": True  # Jina æ¨¡åž‹éœ€è¦ä¿¡ä»»é ç«¯ä»£ç¢¼ä¾†è¼‰å…¥è‡ªå®šç¾©æ¨¡çµ„
    }
    
    # å»ºç«‹ embeddingsï¼Œä½¿ç”¨ Jina v3 å¤šèªžè¨€ç‰ˆæœ¬ï¼ˆæ”¯æ´ä¸­æ–‡ï¼Œæ€§èƒ½æ›´å¥½ï¼‰
    embeddings_kwargs = {
        "model_name": "jinaai/jina-embeddings-v3",  # v3 å¤šèªžè¨€ç‰ˆæœ¬ï¼ˆåŒ…å«ä¸­æ–‡ï¼‰ï¼Œæ€§èƒ½æ›´å¥½
        "model_kwargs": model_kwargs,
        "encode_kwargs": {
            "normalize_embeddings": True,  # å»ºè­° normalize
            "batch_size": 4,  # v3 æ¨¡åž‹è¼ƒå¤§ï¼Œä½¿ç”¨è¼ƒå°çš„æ‰¹æ¬¡å¤§å°ä»¥é¿å…è¨˜æ†¶é«”æº¢å‡º
        },
        "show_progress": True  # é¡¯ç¤ºé€²åº¦æ¢ï¼ˆä½œç‚º HuggingFaceEmbeddings çš„ç›´æŽ¥åƒæ•¸ï¼‰
    }
    
    # å¦‚æžœæœ‰ç·©å­˜ç›®éŒ„ï¼Œæ·»åŠ åˆ° embeddings åƒæ•¸
    if cache_folder:
        embeddings_kwargs["cache_folder"] = cache_folder
    
    # å˜—è©¦è¼‰å…¥æ¨¡åž‹ï¼Œå¦‚æžœå¤±æ•—å‰‡æ¸…ç†ç·©å­˜ä¸¦é‡è©¦
    import shutil
    try:
        embeddings = HuggingFaceEmbeddings(**embeddings_kwargs)
        print("âœ… Jina Embeddings è¼‰å…¥å®Œæˆ")
    except (FileNotFoundError, OSError, Exception) as e:
        error_msg = str(e)
        if "No such file or directory" in error_msg or "cache" in error_msg.lower() or "transformers_modules" in error_msg:
            print("âš ï¸ æª¢æ¸¬åˆ°æ¨¡åž‹ç·©å­˜ä¸å®Œæ•´æˆ–æå£žï¼Œæ­£åœ¨æ¸…ç†ä¸¦é‡æ–°ä¸‹è¼‰...")
            # æ¸…ç†å¯èƒ½æœ‰å•é¡Œçš„ç·©å­˜ç›®éŒ„ï¼ˆåŒ…æ‹¬ jina å’Œç›¸é—œä¾è³´ï¼‰
            cache_paths_to_clean = [
                os.path.join(HF_CACHE_DIR, "modules", "transformers_modules", "jinaai"),
                os.path.join(HF_CACHE_DIR, "modules", "transformers_modules", "jinaai", "jina_hyphen_embeddings_hyphen_v3"),
            ]
            
            for cache_path in cache_paths_to_clean:
                if os.path.exists(cache_path):
                    try:
                        shutil.rmtree(cache_path)
                        print(f"   âœ“ å·²æ¸…ç†ç·©å­˜ï¼š{os.path.basename(cache_path)}")
                    except Exception as cleanup_error:
                        print(f"   âš  æ¸…ç†ç·©å­˜æ™‚å‡ºç¾éŒ¯èª¤ï¼ˆå¯å¿½ç•¥ï¼‰ï¼š{cleanup_error}")
            
            # é‡æ–°å˜—è©¦è¼‰å…¥
            print("   æ­£åœ¨é‡æ–°ä¸‹è¼‰æ¨¡åž‹ï¼ˆé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰...")
            embeddings = HuggingFaceEmbeddings(**embeddings_kwargs)
            print("âœ… Jina Embeddings è¼‰å…¥å®Œæˆï¼ˆå·²é‡æ–°ä¸‹è¼‰ï¼‰")
        else:
            # å…¶ä»–éŒ¯èª¤ç›´æŽ¥æ‹‹å‡º
            print(f"âŒ è¼‰å…¥æ¨¡åž‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error_msg}")
            raise
    
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # --- 2. æº–å‚™ LLM (ä½¿ç”¨ Groq é¿å…é¡åº¦å•é¡Œ) ---
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
    
    llm = ChatGroq(
        model="llama-3.3-70b-versatile",  # Groq ç›®å‰æœ€å¼·çš„æ¨¡åž‹
        temperature=0,
        max_retries=2,
    )

    # --- 3. å»ºç«‹ã€Œå•é¡Œé‡çµ„ã€éˆ (History Aware Retriever) - ä½¿ç”¨ LCEL ---
    # é€™å€‹ Prompt çš„ç›®çš„æ˜¯ï¼šå¦‚æžœä½¿ç”¨è€…å•äº†ä»£åè©žï¼Œåƒè€ƒæ­·å²ç´€éŒ„æŠŠå®ƒæ”¹å¯«æˆå®Œæ•´å•é¡Œ
    contextualize_q_system_prompt = """
    çµ¦å®šä¸€æ®µèŠå¤©æ­·å²è¨˜éŒ„å’Œä½¿ç”¨è€…æœ€æ–°çš„å•é¡Œï¼ˆè©²å•é¡Œå¯èƒ½å¼•ç”¨äº†æ­·å²è¨˜éŒ„ä¸­çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œ
    è«‹å°‡è©²å•é¡Œé‡æ–°è¡¨è¿°ç‚ºä¸€å€‹ç¨ç«‹çš„å•é¡Œï¼Œä½¿å…¶åœ¨æ²’æœ‰èŠå¤©æ­·å²è¨˜éŒ„çš„æƒ…æ³ä¸‹ä¹Ÿèƒ½è¢«ç†è§£ã€‚
    ç›´æŽ¥å›žå‚³æ”¹å¯«å¾Œçš„å•é¡Œå³å¯ï¼Œä¸è¦å›žç­”å•é¡Œï¼Œä¹Ÿä¸è¦è§£é‡‹ã€‚
    """
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # ä½¿ç”¨ LCEL å¯¦ç¾ï¼šå…ˆæ”¹å¯«å•é¡Œï¼Œå†ç”¨æ”¹å¯«å¾Œçš„å•é¡Œæª¢ç´¢
    def format_docs(docs):
        """å°‡æª¢ç´¢åˆ°çš„æ–‡æª”æ ¼å¼åŒ–ç‚ºå­—ä¸²"""
        return "\n\n".join(doc.page_content for doc in docs)
    
    # History-aware retriever: å¦‚æžœæœ‰æ­·å²è¨˜éŒ„ï¼Œå…ˆæ”¹å¯«å•é¡Œå†æª¢ç´¢ï¼›å¦å‰‡ç›´æŽ¥æª¢ç´¢
    def get_standalone_question(input_dict):
        """æ ¹æ“šæ­·å²è¨˜éŒ„æ”¹å¯«å•é¡Œï¼Œä½¿å…¶æˆç‚ºç¨ç«‹å•é¡Œ"""
        # å¦‚æžœæœ‰æ­·å²è¨˜éŒ„ï¼Œç”¨ LLM æ”¹å¯«å•é¡Œ
        if input_dict.get("chat_history"):
            standalone_question_chain = contextualize_q_prompt | llm | StrOutputParser()
            return standalone_question_chain.invoke(input_dict)
        # æ²’æœ‰æ­·å²è¨˜éŒ„ï¼Œç›´æŽ¥è¿”å›žåŽŸå§‹å•é¡Œ
        return input_dict["input"]
    
    # çµ„åˆï¼šæ”¹å¯«å•é¡Œ -> æª¢ç´¢æ–‡æª” -> æ ¼å¼åŒ–
    def retrieve_documents(input_dict):
        """æª¢ç´¢æ–‡æª”ä¸¦æ ¼å¼åŒ–"""
        question = get_standalone_question(input_dict)
        docs = retriever.invoke(question)
        return format_docs(docs)
    
    # --- 4. å»ºç«‹ã€Œå•ç­”ã€éˆ (Answer Chain) - ä½¿ç”¨ LCEL ---
    qa_system_prompt = """
    ä½ æ˜¯ä¸€å€‹å•ç­”åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ä¸Šä¸‹æ–‡ç‰‡æ®µä¾†å›žç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æžœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±èªªä¸çŸ¥é“ï¼Œä¸è¦è©¦åœ–ç·¨é€ ç­”æ¡ˆã€‚
    å›žç­”è«‹ä¿æŒç°¡æ½”ã€‚
    
    {context}
    """
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # ä½¿ç”¨ LCEL çµ„åˆå®Œæ•´çš„ RAG éˆ
    rag_chain = (
        {
            "context": RunnableLambda(retrieve_documents),
            "input": lambda x: x["input"],
            "chat_history": lambda x: x.get("chat_history", []),
        }
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    # --- 5. é–‹å§‹å°è©± (ç®¡ç†è¨˜æ†¶) ---
    print("\nâœ… ç³»çµ±å°±ç·’ï¼æˆ‘æ˜¯æœ‰è¨˜æ†¶çš„ PDF åŠ©æ‰‹ï¼ˆä½¿ç”¨ Jina Embeddings v3 å¤šèªžè¨€ç‰ˆ + Groq Llama 3.3ï¼‰ã€‚(è¼¸å…¥ 'exit' é›¢é–‹)\n")
    
    # æˆ‘å€‘ç”¨ä¸€å€‹ List ä¾†æ‰‹å‹•ç®¡ç†å°è©±æ­·å²
    chat_history = []

    while True:
        user_input = input("ä½ ï¼š")
        if user_input.lower() in ["exit", "quit", "bye"]:
            break
        
        if not user_input.strip():
            continue

        print("ðŸ¤– (Groq æ€è€ƒä¸­)...", end="", flush=True)
        
        # å‘¼å« Chainï¼Œä¸¦å‚³å…¥ç›®å‰çš„ chat_history
        response = rag_chain.invoke({
            "input": user_input,
            "chat_history": chat_history
        })
        
        print(f"\rAIï¼š{response}\n")
        
        # æ›´æ–°æ­·å²ç´€éŒ„
        # 1. åŠ å…¥ä½¿ç”¨è€…çš„è©±
        chat_history.append(HumanMessage(content=user_input))
        # 2. åŠ å…¥ AI çš„å›žç­”
        chat_history.append(AIMessage(content=response))

        # (é¸ç”¨) ä¿æŒæ­·å²ç´€éŒ„ä¸è¦å¤ªé•·ï¼Œä»¥å…å¡žçˆ† Context Window
        if len(chat_history) > 10: 
            chat_history = chat_history[-10:]

if __name__ == "__main__":
    main()


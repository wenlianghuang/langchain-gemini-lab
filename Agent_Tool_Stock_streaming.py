import os 
import json
import yfinance as yf
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage,ToolMessage

load_dotenv()

# --- 1. å®šç¾©å·¥å…· (The "Hands") ---
# ä½¿ç”¨ @tool è£é£¾å™¨ï¼ŒLangChain æœƒè‡ªå‹•è§£æå‡½å¼åç¨±ã€åƒæ•¸å‹åˆ¥å’Œ docstring è®Šæˆ JSON Schema çµ¦ LLM çœ‹

@tool 
def get_stock_price(ticker: str) -> str:
    """
    æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ã€‚
    è¼¸å…¥åƒæ•¸ ticker å¿…é ˆæ˜¯è‚¡ç¥¨ä»£ç¢¼ã€‚
    å¦‚æœæ˜¯å°è‚¡ï¼Œè«‹åœ¨ä»£ç¢¼å¾ŒåŠ ä¸Š .TW (ä¾‹å¦‚ 2330.TW)ã€‚
    å¦‚æœæ˜¯ç¾è‚¡ï¼Œç›´æ¥è¼¸å…¥ä»£ç¢¼ (ä¾‹å¦‚ AAPL, TSLA, GOOG)ã€‚
    """
    print(f"\nğŸ”§ [Tool Called] æ­£åœ¨æŸ¥è©¢: {ticker} ...") # Debug ç”¨ï¼Œè®“ä½ çœ‹è¦‹ AI çœŸçš„åœ¨åšäº‹
    try:
        stock = yf.Ticker(ticker)
        # å–å¾—æœ€æ–°æ”¶ç›¤åƒ¹æˆ–ç•¶å‰åƒ¹æ ¼
        history = stock.history(period="1d")
        if history.empty:
            return f"æ‰¾ä¸åˆ°è‚¡ç¥¨ä»£ç¢¼ {ticker} çš„è³‡æ–™ã€‚"
            
        current_price = history['Close'].iloc[-1]
        currency = stock.info.get('currency', 'Unknown')
        return f"{ticker} ç›®å‰åƒ¹æ ¼ç‚º {current_price:.2f} {currency}"
    except Exception as e:
        return f"æŸ¥è©¢å¤±æ•—: {e}"

def main():
    # --- 2. ç¶å®šå·¥å…· ---
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
    llm_with_tools = llm.bind_tools([get_stock_price])

    print("ğŸ“ˆ AI è‚¡ç¥¨åŠ©ç† (å…·å‚™ Tool Calling + Streaming)...")
    print("ğŸ’¡ æç¤ºï¼šè¼¸å…¥ 'exit' çµæŸ\n")
    
    messages = []
    
    while True:
        try:
            query = input("è«‹è¼¸å…¥æ‚¨æƒ³æŸ¥è©¢çš„è‚¡ç¥¨: ").strip()
            if query.lower() in ['exit', 'quit']:
                break
            if not query:
                continue
            
            print(f"\nUser: {query}")
            messages.append(HumanMessage(content=query))
            
            # --- éšæ®µ 1: æ±ºç­– (ä¾èˆŠä½¿ç”¨ invoke) ---
            # ç‚ºä»€éº¼é€™è£¡ä¸ç”¨ streamï¼Ÿå› ç‚ºå¦‚æœ AI æ±ºå®š Call Toolï¼Œ
            # å®ƒåå‡ºä¾†çš„æ˜¯çµæ§‹åŒ– JSONï¼Œé€å­—é¡¯ç¤ºçµ¦ä½¿ç”¨è€…çœ‹æ²’æ„ç¾©ä¸”æœƒäº‚ç¢¼ã€‚
            # æˆ‘å€‘ç­‰å®ƒå®Œæ•´æ±ºå®šå¥½ã€Œæˆ‘è¦å‘¼å«ä»€éº¼ã€å†å¾€ä¸‹èµ°ã€‚
            ai_msg_decision = llm_with_tools.invoke(messages)
            
            # åˆ¤æ–·æ˜¯å¦è¦å‘¼å«å·¥å…·
            if ai_msg_decision.tool_calls:
                print(f"\nğŸ¤– AI æ±ºå®šå‘¼å«å·¥å…·: {len(ai_msg_decision.tool_calls)} å€‹")
                messages.append(ai_msg_decision)
                
                # --- åŸ·è¡Œå·¥å…· ---
                for tool_call in ai_msg_decision.tool_calls:
                    selected_tool = {"get_stock_price": get_stock_price}[tool_call["name"]]
                    print(f"ğŸ”§ åŸ·è¡Œå·¥å…·: {tool_call['name']} ({tool_call['args']})")
                    tool_output = selected_tool.invoke(tool_call["args"])
                    messages.append(ToolMessage(tool_output, tool_call_id=tool_call["id"]))
                
                # --- éšæ®µ 2: æœ€çµ‚å›ç­” (æ”¹ç”¨ Stream!) ---
                print("\nAI: ", end="", flush=True) # æº–å‚™é–‹å§‹æ‰“å­—
                
                full_content = ""
                # ä½¿ç”¨ .stream() å–ä»£ .invoke()
                for chunk in llm_with_tools.stream(messages):
                    # #region agent log
                    #with open('/Users/matthuang/Desktop/langchain-gemini-lab/.cursor/debug.log', 'a') as f:
                    #    f.write(json.dumps({"sessionId":"debug-session","runId":"run1","hypothesisId":"A","location":"Agent_Tool_Stock_streaming.py:81","message":"chunk.content type check","data":{"type":str(type(chunk.content)),"is_list":isinstance(chunk.content,list),"value":str(chunk.content)[:100] if chunk.content else None},"timestamp":int(__import__('time').time()*1000)})+'\n')
                    # #endregion
                    # åªè¦ chunk è£¡é¢æœ‰æ–‡å­—å…§å®¹ï¼Œå°±å°å‡ºä¾†
                    if chunk.content:
                        # #region agent log
                        #with open('/Users/matthuang/Desktop/langchain-gemini-lab/.cursor/debug.log', 'a') as f:
                        #    f.write(json.dumps({"sessionId":"debug-session","runId":"run1","hypothesisId":"A","location":"Agent_Tool_Stock_streaming.py:83","message":"before concatenation","data":{"chunk_content_type":str(type(chunk.content)),"full_content_type":str(type(full_content))},"timestamp":int(__import__('time').time()*1000)})+'\n')
                        # #endregion
                        # è™•ç† content å¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…æ³
                        content_str = chunk.content if isinstance(chunk.content, str) else ''.join(chunk.content) if isinstance(chunk.content, list) else str(chunk.content)
                        print(content_str, end="", flush=True)
                        full_content += content_str
                
                print("\n") # æ›è¡Œ
                
                # é‡è¦ï¼å¿…é ˆæŠŠå®Œæ•´çš„å…§å®¹å­˜å›è¨˜æ†¶ï¼Œä¸ç„¶ä¸‹ä¸€è¼ª AI æœƒå¤±æ†¶
                messages.append(AIMessage(content=full_content))

            else:
                # å¦‚æœ AI æ²’ç”¨å·¥å…·ï¼Œç›´æ¥é–’èŠï¼Œä¹Ÿæ”¯æ´ Stream
                print("\nAI: ", end="", flush=True)
                full_content = ""
                # #region agent log
                with open('/Users/matthuang/Desktop/langchain-gemini-lab/.cursor/debug.log', 'a') as f:
                    f.write(json.dumps({"sessionId":"debug-session","runId":"run1","hypothesisId":"B","location":"Agent_Tool_Stock_streaming.py:100","message":"ai_msg_decision.content type check","data":{"type":str(type(ai_msg_decision.content)),"is_list":isinstance(ai_msg_decision.content,list),"value":str(ai_msg_decision.content)[:100] if ai_msg_decision.content else None},"timestamp":int(__import__('time').time()*1000)})+'\n')
                # #endregion
                # é€™è£¡ä¹Ÿè¦æ”¹ç”¨ streamï¼Œå› ç‚º ai_msg_decision å·²ç¶“æ˜¯å®Œæˆå“äº†ï¼Œ
                # æˆ‘å€‘å¾—é‡æ–°ç”¨ stream è·‘ä¸€æ¬¡ï¼Œæˆ–è€…ç°¡å–®é»ï¼š
                # ç‚ºäº†é¿å…æµªè²»éŒ¢é‡è·‘ï¼Œå¦‚æœç¬¬ä¸€éšæ®µç™¼ç¾ä¸æ˜¯ tool callï¼Œ
                # æˆ‘å€‘å¯ä»¥ç›´æ¥é¡¯ç¤º ai_msg_decision.content (é€™æ˜¯åŒæ­¥çš„)
                # ä½†ç‚ºäº†çµ±ä¸€é«”é©—ï¼Œé€šå¸¸å»ºè­°ç¬¬ä¸€éšæ®µä¹Ÿç”¨ stream (æ¯”è¼ƒé€²éš)ï¼Œ
                # é€™è£¡ç‚ºäº†å¥½æ‡‚ï¼Œè‹¥æ²’ç”¨å·¥å…·ï¼Œæˆ‘å€‘ç›´æ¥å°å‡ºå‰›å‰› invoke çš„çµæœå³å¯ã€‚
                # è™•ç† content å¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…æ³
                content_str = ai_msg_decision.content if isinstance(ai_msg_decision.content, str) else ''.join(ai_msg_decision.content) if isinstance(ai_msg_decision.content, list) else str(ai_msg_decision.content)
                print(content_str + "\n")
                messages.append(ai_msg_decision)
                
        except KeyboardInterrupt:
            print("\nç¨‹å¼å·²ä¸­æ–·")
            break
        except Exception as e:
            print(f"\nâŒ éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
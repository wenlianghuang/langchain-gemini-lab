import os
import time
import yfinance as yf
from dotenv import load_dotenv

# --- LangChain Imports ---
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_google_genai.chat_models import ChatGoogleGenerativeAIError
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools.tavily_search import TavilySearchResults

# --- âœ¨ LangGraph Imports (æ ¸å¿ƒä¸»è§’) ---
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ–ï¼šå…¨åŸŸè³‡æº (PDF VectorStore)
# ==========================================
print("ğŸš€ [System] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
pdf_path = "./data/Tree_of_Thoughts.pdf"

retriever = None
if os.path.exists(pdf_path):
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    # ä½¿ç”¨ Chroma å»ºç«‹è¨˜æ†¶é«”å…§çš„å‘é‡è³‡æ–™åº«
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… PDF è¼‰å…¥å®Œæˆï¼ŒRAG åŠŸèƒ½å°±ç·’ã€‚")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# ==========================================
# 2. å®šç¾©å·¥å…· (Tools)
# ==========================================

@tool
def get_stock_price(ticker: str) -> str:
    """
    æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ã€‚
    è¼¸å…¥åƒæ•¸ ticker å¿…é ˆæ˜¯è‚¡ç¥¨ä»£ç¢¼ (å¦‚ 2330.TW, NVDA, GOOG)ã€‚
    """
    print(f"   ğŸ”§ [Tool: Stock] æŸ¥è©¢: {ticker}")
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="1d")
        if hist.empty: return f"æ‰¾ä¸åˆ° {ticker}"
        price = hist['Close'].iloc[-1]
        curr = stock.info.get('currency', '?')
        return f"{ticker} ç¾åƒ¹: {price:.2f} {curr}"
    except Exception as e:
        return f"è‚¡å¸‚æŸ¥è©¢éŒ¯èª¤: {e}"

@tool
def lookup_pdf_knowledge(query: str) -> str:
    """
    æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚
    ç•¶å•åŠè«–æ–‡ç´°ç¯€ã€ä½œè€…æˆ–æ ¸å¿ƒæ¦‚å¿µæ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ”§ [Tool: RAG] æª¢ç´¢ PDF: {query}")
    
    try:
        # é€™è£¡åœ¨å·¥å…·å…§å»ºç«‹ä¸€å€‹å°å‹çš„ Retrieval Chain
        llm_rag = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
        prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm_rag
            | StrOutputParser()
        )
        
        # ä½¿ç”¨éŒ¯èª¤è™•ç†èˆ‡é‡è©¦
        def invoke_chain():
            return chain.invoke(query)
        
        return handle_api_error_with_retry(invoke_chain)
    except Exception as e:
        return f"PDF çŸ¥è­˜åº«æŸ¥è©¢éŒ¯èª¤: {str(e)[:200]}"

@tool
def search_web(query: str) -> str:
    """
    æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èã€å¤©æ°£æˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚
    ç„¡æ³•æŸ¥è‚¡åƒ¹æˆ– PDF æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    print(f"   ğŸ”§ [Tool: Web] ä¸Šç¶²æœå°‹: {query}")
    try:
        tool = TavilySearchResults(k=3)
        return tool.invoke(query)
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

# âœ¨ é—œéµï¼šå°‡æ‰€æœ‰å·¥å…·æ”¾å…¥åˆ—è¡¨
tools_list = [get_stock_price, lookup_pdf_knowledge, search_web]

# ==========================================
# 2.5. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦å·¥å…·å‡½æ•¸
# ==========================================

def handle_api_error_with_retry(func, max_retries=3, base_delay=2):
    """
    è™•ç† API éŒ¯èª¤ä¸¦è‡ªå‹•é‡è©¦ï¼Œç‰¹åˆ¥é‡å° 429 (é…é¡é™åˆ¶) éŒ¯èª¤ã€‚
    
    Args:
        func: è¦åŸ·è¡Œçš„å‡½æ•¸ï¼ˆç„¡åƒæ•¸ï¼‰
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        base_delay: åŸºç¤å»¶é²æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œæœƒä½¿ç”¨æŒ‡æ•¸é€€é¿
    """
    for attempt in range(max_retries):
        try:
            return func()
        except ChatGoogleGenerativeAIError as e:
            error_str = str(e)
            
            # æª¢æŸ¥æ˜¯å¦ç‚º 429 é…é¡é™åˆ¶éŒ¯èª¤
            if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                # å˜—è©¦å¾éŒ¯èª¤è¨Šæ¯ä¸­æå–å»ºè­°çš„ç­‰å¾…æ™‚é–“
                retry_delay = base_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                
                # å˜—è©¦å¾éŒ¯èª¤è¨Šæ¯ä¸­è§£æå»ºè­°çš„ç­‰å¾…æ™‚é–“
                if "retry in" in error_str.lower() or "retrydelay" in error_str.lower():
                    try:
                        # ç°¡å–®çš„è§£æé‚è¼¯ï¼Œå°‹æ‰¾æ•¸å­—
                        import re
                        delay_match = re.search(r'(\d+(?:\.\d+)?)\s*s', error_str, re.IGNORECASE)
                        if delay_match:
                            retry_delay = max(float(delay_match.group(1)), retry_delay)
                    except:
                        pass
                
                if attempt < max_retries - 1:
                    wait_time = min(retry_delay, 120)  # æœ€å¤šç­‰å¾… 120 ç§’
                    print(f"\n   âš ï¸ [API é…é¡é™åˆ¶] å·²é”åˆ°å…è²»ç‰ˆæ¯æ—¥è«‹æ±‚é™åˆ¶ (20æ¬¡/å¤©)")
                    print(f"   â³ ç­‰å¾… {wait_time:.1f} ç§’å¾Œè‡ªå‹•é‡è©¦... (å˜—è©¦ {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"\n   âŒ [API éŒ¯èª¤] å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œç„¡æ³•å®Œæˆè«‹æ±‚ã€‚")
                    print(f"   ğŸ’¡ å»ºè­°ï¼š")
                    print(f"      1. ç­‰å¾…ä¸€æ®µæ™‚é–“å¾Œå†è©¦ï¼ˆå…è²»ç‰ˆæ¯æ—¥é™åˆ¶ï¼š20æ¬¡è«‹æ±‚ï¼‰")
                    print(f"      2. æª¢æŸ¥ API é…é¡ï¼šhttps://ai.dev/usage?tab=rate-limit")
                    print(f"      3. è€ƒæ…®å‡ç´šåˆ°ä»˜è²»æ–¹æ¡ˆä»¥ç²å¾—æ›´é«˜é…é¡")
                    raise Exception(f"API é…é¡å·²ç”¨ç›¡ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚éŒ¯èª¤è©³æƒ…ï¼š{error_str[:200]}")
            else:
                # å…¶ä»–é¡å‹çš„éŒ¯èª¤ï¼Œç›´æ¥æ‹‹å‡º
                print(f"\n   âŒ [API éŒ¯èª¤] {error_str[:200]}")
                raise
    
    # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
    raise Exception("API è«‹æ±‚å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")

# ==========================================
# 3. å»ºæ§‹ LangGraph
# ==========================================

# A. åˆå§‹åŒ– LLM ä¸¦ç¶å®šæ‰€æœ‰å·¥å…·
llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
llm_with_tools = llm.bind_tools(tools_list)

# B. å®šç¾©ç¯€é» (Nodes)
def agent_node(state: MessagesState):
    """æ€è€ƒç¯€é»ï¼šæ¥æ”¶æ­·å²è¨Šæ¯ï¼Œç”¢å‡ºä¸‹ä¸€æ­¥æ±ºç­–"""
    messages = state["messages"]
    
    # ä½¿ç”¨éŒ¯èª¤è™•ç†èˆ‡é‡è©¦
    def invoke_llm():
        return llm_with_tools.invoke(messages)
    
    try:
        response = handle_api_error_with_retry(invoke_llm)
        # å›å‚³æ›´æ–° (LangGraph æœƒè‡ªå‹•å°‡æ–°è¨Šæ¯ append åˆ°æ¸…å–®ä¸­)
        return {"messages": [response]}
    except Exception as e:
        # å¦‚æœé‡è©¦å¾Œä»ç„¶å¤±æ•—ï¼Œè¿”å›éŒ¯èª¤è¨Šæ¯
        from langchain_core.messages import AIMessage
        error_msg = AIMessage(content=f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        return {"messages": [error_msg]}

# C. å»ºç«‹åœ–è¡¨ (Graph Construction)
builder = StateGraph(MessagesState)

# 1. åŠ å…¥ç¯€é»
builder.add_node("agent", agent_node)
builder.add_node("tools", ToolNode(tools_list)) # âœ¨ ToolNode è‡ªå‹•è™•ç†å¤šå·¥å…·ä¸¦è¡ŒåŸ·è¡Œ

# 2. å®šç¾©é‚Š (Edges)
builder.add_edge(START, "agent")

# 3. æ¢ä»¶é‚Š (Conditional Edge)
# tools_condition æœƒè‡ªå‹•æª¢æŸ¥ agent çš„è¼¸å‡ºï¼š
# - å¦‚æœæœ‰ tool_calls -> å‰å¾€ "tools" ç¯€é»
# - å¦‚æœæ²’æœ‰ -> å‰å¾€ END
builder.add_conditional_edges("agent", tools_condition)

# 4. å¾ªç’°é‚Š (Loop)
# å·¥å…·åŸ·è¡Œå®Œå¾Œï¼Œå¿…é ˆå›åˆ° agent è®“å®ƒæ ¹æ“šçµæœç”¢ç”Ÿå›ç­”
builder.add_edge("tools", "agent")

# D. ç·¨è­¯åœ–è¡¨ (Compile with Memory)
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# ==========================================
# 4. åŸ·è¡Œä¸»ç¨‹å¼
# ==========================================
def main():
    print("\nğŸ¤– LangGraph Super Agent ä¸Šç·šï¼(æ¶æ§‹ï¼šGraph ReAct)")
    print("ğŸ‘‰ æ”¯æ´ï¼šå¤šå·¥å…·ä¸¦è¡Œã€ç‹€æ…‹è¨˜æ†¶ã€è‡ªå‹•è·¯ç”±")
    print("ğŸ’¡ è©¦è©¦ï¼š'å°ç©é›»è‚¡åƒ¹å¤šå°‘ï¼Ÿå¦å¤–ç¶²è·¯ä¸Šæœ‰ä»€éº¼é—œæ–¼å®ƒçš„æ–°èï¼Ÿ'\n")

    # è¨­å®šé€™å ´å°è©±çš„ ID (ç”¨æ–¼è¨˜æ†¶æª¢ç´¢)
    config = {"configurable": {"thread_id": "demo-user-001"}}

    while True:
        user_input = input("User: ").strip()
        if user_input.lower() in ["quit", "exit"]:
            break
        if not user_input: continue

        # åŸ·è¡Œ Graph
        # stream_mode="values" æœƒå›å‚³æ¯å€‹æ­¥é©Ÿæ›´æ–°å¾Œçš„å®Œæ•´ state
        print("   (Graph æ€è€ƒèˆ‡èª¿åº¦ä¸­...)")
        
        # é€™è£¡æˆ‘å€‘åªé¡¯ç¤ºæœ€å¾Œç”¢ç”Ÿçš„è¨Šæ¯ï¼Œé¿å…æ´—ç‰ˆ
        last_printed_msg_id = None
        
        try:
            for event in graph.stream({"messages": [HumanMessage(content=user_input)]}, config, stream_mode="values"):
                current_messages = event["messages"]
                if not current_messages: continue
                
                last_msg = current_messages[-1]
                
                # é¿å…é‡è¤‡å°å‡ºåŒä¸€å‰‡è¨Šæ¯
                if last_msg.id == last_printed_msg_id:
                    continue
                last_printed_msg_id = last_msg.id

                # é¡¯ç¤º Agent çš„æ±ºç­–
                if last_msg.type == "ai" and last_msg.tool_calls:
                    tool_names = [tc["name"] for tc in last_msg.tool_calls]
                    print(f"   â¡ï¸ [Node: Agent] æ±ºå®šå‘¼å«: {tool_names}")
                
                # é¡¯ç¤º Tools çš„çµæœ
                elif last_msg.type == "tool":
                    # æ“·å–éƒ¨åˆ†å…§å®¹é¿å…å¤ªé•·
                    preview = str(last_msg.content)[:50] + "..."
                    print(f"   â¡ï¸ [Node: Tools] å·¥å…· {last_msg.name} å®Œæˆã€‚")

                # é¡¯ç¤ºæœ€çµ‚å›ç­”
                elif last_msg.type == "ai" and not last_msg.tool_calls:
                    print(f"\nAI: {last_msg.content}\n")
        
        except Exception as e:
            # æ•ç²ä¸¦é¡¯ç¤ºä»»ä½•æœªé æœŸçš„éŒ¯èª¤
            error_msg = str(e)
            if "é…é¡" in error_msg or "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                print(f"\n   âŒ [éŒ¯èª¤] API é…é¡å·²ç”¨ç›¡")
                print(f"   ğŸ’¡ è«‹ç¨å¾Œå†è©¦ï¼Œæˆ–æª¢æŸ¥æ‚¨çš„ API é…é¡ç‹€æ…‹")
            else:
                print(f"\n   âŒ [éŒ¯èª¤] è™•ç†è«‹æ±‚æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{error_msg[:300]}")
            print()

if __name__ == "__main__":
    main()
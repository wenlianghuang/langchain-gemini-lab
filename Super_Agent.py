import os
import yfinance as yf
from dotenv import load_dotenv

# LangChain Imports
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

load_dotenv()

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ–ï¼šé å…ˆè¼‰å…¥ PDF (åªåšä¸€æ¬¡)
# ==========================================
print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç³»çµ±èˆ‡å‘é‡è³‡æ–™åº« (é€™å¯èƒ½éœ€è¦å¹¾ç§’é˜)...")

pdf_path = "./data/Tree_of_Thoughts.pdf"
if not os.path.exists(pdf_path):
    raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° PDF: {pdf_path}")

# è¼‰å…¥èˆ‡åˆ‡å‰² PDF
loader = PyPDFLoader(pdf_path)
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

# å»ºç«‹ VectorStore (å­˜åœ¨è¨˜æ†¶é«”ä¸­)
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

print("âœ… PDF è¼‰å…¥å®Œæˆï¼ŒAgent æº–å‚™å°±ç·’ï¼\n")

# ==========================================
# 2. å®šç¾©å·¥å…· (Tools)
# ==========================================

# --- å·¥å…· A: æŸ¥è‚¡åƒ¹ ---
@tool
def get_stock_price(ticker: str) -> str:
    """
    æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ã€‚
    è¼¸å…¥åƒæ•¸ ticker å¿…é ˆæ˜¯è‚¡ç¥¨ä»£ç¢¼ã€‚
    å¦‚æœæ˜¯å°è‚¡ï¼Œè«‹åœ¨ä»£ç¢¼å¾ŒåŠ ä¸Š .TW (ä¾‹å¦‚ 2330.TW)ã€‚
    å¦‚æœæ˜¯ç¾è‚¡ï¼Œç›´æ¥è¼¸å…¥ä»£ç¢¼ (ä¾‹å¦‚ AAPL, TSLA, GOOG)ã€‚
    """
    print(f"\nğŸ”§ [Tool: Stock] æŸ¥è©¢è‚¡åƒ¹: {ticker} ...")
    try:
        stock = yf.Ticker(ticker)
        history = stock.history(period="1d")
        if history.empty:
            return f"æ‰¾ä¸åˆ°è‚¡ç¥¨ä»£ç¢¼ {ticker} çš„è³‡æ–™ã€‚"
        current_price = history['Close'].iloc[-1]
        currency = stock.info.get('currency', 'Unknown')
        return f"{ticker} ç›®å‰åƒ¹æ ¼ç‚º {current_price:.2f} {currency}"
    except Exception as e:
        return f"æŸ¥è©¢å¤±æ•—: {e}"

# --- å·¥å…· B: æŸ¥ PDF (RAG) ---
@tool
def lookup_pdf_knowledge(query: str) -> str:
    """
    æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚
    ç•¶ä½¿ç”¨è€…å•åˆ°é—œæ–¼ ToTã€æ€ç¶­æ¨¹ã€Prompt Engineering æˆ–è«–æ–‡ç´°ç¯€æ™‚ï¼Œå‹™å¿…ä½¿ç”¨æ­¤å·¥å…·ã€‚
    è¼¸å…¥åƒæ•¸ query æ‡‰è©²æ˜¯ä¸€å€‹å®Œæ•´çš„å•å¥ã€‚
    """
    print(f"\nğŸ”§ [Tool: RAG] æŸ¥è©¢å…§éƒ¨æ–‡ä»¶: {query} ...")
    
    # åœ¨é€™è£¡ï¼Œæˆ‘å€‘åœ¨å·¥å…·å…§éƒ¨è·‘ä¸€å€‹å°å‹çš„ RAG Chain
    # é€™æ¨£åšçš„å¥½è™•æ˜¯ï¼šä¸» Agent ä¸éœ€è¦çŸ¥é“ RAG çš„ç´°ç¯€ï¼Œå®ƒåªè¦ç­‰ç­”æ¡ˆå°±å¥½
    
    llm_for_rag = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
    
    template = """è«‹æ ¹æ“šä»¥ä¸‹çš„æ–‡ä»¶ç‰‡æ®µå›ç­”å•é¡Œï¼š
    {context}
    
    å•é¡Œï¼š{question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    # å®šç¾©å°å‹ Chain
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_for_rag
        | StrOutputParser()
    )
    
    try:
        result = rag_chain.invoke(query)
        return result
    except Exception as e:
        return f"RAG æª¢ç´¢å¤±æ•—: {e}"

# ==========================================
# 3. ä¸»ç¨‹å¼ Loop
# ==========================================
def main():
    # åˆå§‹åŒ–ä¸»å¤§è…¦
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)
    
    # ç¶å®šæ‰€æœ‰å·¥å…·ï¼ (é€™å°±æ˜¯ Super Agent çš„é—œéµ)
    tools = [get_stock_price, lookup_pdf_knowledge]
    llm_with_tools = llm.bind_tools(tools)
    
    messages = []
    
    print("ğŸ¤– Super Agent ä¸Šç·šï¼æˆ‘å¯ä»¥æŸ¥è‚¡åƒ¹ï¼Œä¹Ÿå¯ä»¥å›ç­” PDF å…§å®¹ã€‚")
    print("ğŸ’¡ è©¦è©¦çœ‹ï¼š'è«‹å• Tree of Thoughts çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€éº¼ï¼Ÿé€™è·Ÿå°ç©é›»(2330.TW)è‚¡åƒ¹æœ‰é—œå—ï¼Ÿ'(é›–ç„¶æ²’é—œï¼Œä½†å¯ä»¥æ¸¬è©¦å®ƒåŒæ™‚åšå…©ä»¶äº‹)")
    print("ğŸ‘‰ è¼¸å…¥ 'exit' é›¢é–‹\n")

    while True:
        try:
            user_input = input("User: ").strip()
            if user_input.lower() in ["exit", "quit"]:
                break
            if not user_input:
                continue

            messages.append(HumanMessage(content=user_input))

            # --- éšæ®µ 1: æ±ºç­– (Decision) ---
            # AI æ€è€ƒè¦ä¸è¦ç”¨å·¥å…·
            ai_decision = llm_with_tools.invoke(messages)
            messages.append(ai_decision)

            # åˆ¤æ–·æ˜¯å¦å‘¼å«å·¥å…·
            if ai_decision.tool_calls:
                print(f"\nğŸ¤– AI æ±ºå®šä½¿ç”¨ {len(ai_decision.tool_calls)} å€‹å·¥å…·...")
                
                for tool_call in ai_decision.tool_calls:
                    # æ ¹æ“šåç¨±æ‰¾åˆ°å°æ‡‰çš„å‡½å¼
                    selected_tool = {
                        "get_stock_price": get_stock_price,
                        "lookup_pdf_knowledge": lookup_pdf_knowledge
                    }[tool_call["name"]]
                    
                    # åŸ·è¡Œå·¥å…·
                    tool_output = selected_tool.invoke(tool_call["args"])
                    
                    # å°‡çµæœå­˜å›è¨Šæ¯åˆ—
                    messages.append(ToolMessage(tool_output, tool_call_id=tool_call["id"]))

                # --- éšæ®µ 2: æ•´åˆå›ç­” (Synthesis) ---
                print("ğŸ’¡ AI æ­£åœ¨æ•´åˆè³‡è¨Š...\nAI: ", end="", flush=True)
                
                full_response = ""
                # ä½¿ç”¨ä¸²æµé¡¯ç¤ºæœ€çµ‚ç­”æ¡ˆ
                for chunk in llm_with_tools.stream(messages):
                    content = chunk.content
                    if content:
                        # é˜²ç¦¦æ€§æª¢æŸ¥ (åŒä¸Šä¸€å ‚èª²)
                        text = content if isinstance(content, str) else str(content)
                        print(text, end="", flush=True)
                        full_response += text
                print("\n")
                messages.append(AIMessage(content=full_response))

            else:
                # æ²’ç”¨å·¥å…·ï¼Œç›´æ¥å›ç­” (é–’èŠ)
                print(f"\nAI: {ai_decision.content}\n")

        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
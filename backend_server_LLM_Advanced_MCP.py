import os
import uvicorn
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# --- FastAPI & LangServe Imports ---
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from langserve import add_routes

# --- LangChain & LangGraph Imports ---
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool, StructuredTool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools.tavily_search import TavilySearchResults

from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver

# --- MCP Client Imports ---
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# Pydantic ç”¨æ–¼å®šç¾©è¼¸å…¥ä»‹é¢
from pydantic import BaseModel
from typing import List, Union, Optional, Any

load_dotenv()

# ==========================================
# 0. è¨­å®šèˆ‡ç’°å¢ƒè®Šæ•¸
# ==========================================
WORKSPACE_DIR = os.path.abspath("./workspace")  # è¨­å®š AI çš„å·¥ä½œç›®éŒ„

if not os.path.exists(WORKSPACE_DIR):
    os.makedirs(WORKSPACE_DIR)
    print(f"ğŸ“ å·²å»ºç«‹å·¥ä½œç›®éŒ„: {WORKSPACE_DIR}")

def get_llm():
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
    return ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0,
        max_retries=2,
    )

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ– (Local RAG)
# ==========================================
print("ğŸš€ [Server] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
pdf_path = "./data/Tree_of_Thoughts.pdf"
retriever = None

if os.path.exists(pdf_path):
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        print("âœ… PDF è¼‰å…¥å®Œæˆã€‚")
    except Exception as e:
        print(f"âš ï¸ PDF è™•ç†éŒ¯èª¤ (å¯èƒ½æ˜¯ API Key å•é¡Œ): {e}")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# ==========================================
# 2. å®šç¾© MCP é€£ç·šç®¡ç† (æ ¸å¿ƒå‡ç´šéƒ¨åˆ†)
# ==========================================

async def run_mcp_tool(server_params: StdioServerParameters, tool_name: str, arguments: dict) -> str:
    """
    é€šç”¨å‡½å¼ï¼šå»ºç«‹èˆ‡ MCP Server çš„é€£ç·šï¼ŒåŸ·è¡Œå·¥å…·ï¼Œç„¶å¾Œé—œé–‰é€£ç·šã€‚
    """
    try:
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                
                # å‘¼å«é ç«¯å·¥å…·
                result = await session.call_tool(tool_name, arguments=arguments)
                
                # è§£æçµæœ
                if result.content and hasattr(result.content[0], 'text'):
                    return result.content[0].text
                return str(result)
    except Exception as e:
        return f"âŒ MCP åŸ·è¡ŒéŒ¯èª¤ ({tool_name}): {e}"

# --- é…ç½® 1: Stock MCP Server (Python) ---
def get_stock_server_params():
    return StdioServerParameters(
        command="uv",
        args=["run", "stock_mcp_server.py"], 
        env=os.environ.copy()
    )

# --- é…ç½® 2: Filesystem MCP Server (Node.js) ---
def get_filesystem_server_params():
    # ä½¿ç”¨ npx ç›´æ¥åŸ·è¡Œå®˜æ–¹çš„ filesystem server
    # åƒæ•¸æ˜¯æˆ‘å€‘æŒ‡å®šçš„ WORKSPACE_DIRï¼Œé€™å°±æ˜¯å®ƒçš„ã€Œæ ¹ç›®éŒ„ã€
    return StdioServerParameters(
        command="npx",
        args=["-y", "@modelcontextprotocol/server-filesystem", WORKSPACE_DIR],
        env=os.environ.copy()
    )

# ==========================================
# 3. å®šç¾©æ‰€æœ‰å·¥å…· (Local + MCP Wrappers)
# ==========================================

# --- [A] Local Tools ---
@tool
def lookup_pdf_knowledge(query: str) -> str:
    """æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚"""
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ“˜ [Local RAG] æŸ¥è©¢: {query}")
    try:
        llm_rag = get_llm()
        prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm_rag
            | StrOutputParser()
        )
        return chain.invoke(query)
    except Exception as e:
        return f"RAG æª¢ç´¢å¤±æ•—: {e}"

@tool
def search_web(query: str) -> str:
    """æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚"""
    print(f"   ğŸŒ [Web Search] æœå°‹: {query}")
    try:
        search_tool = TavilySearchResults(k=3)
        results = search_tool.invoke(query)
        # ç°¡å–®æ ¼å¼åŒ–
        return str(results)[:2000] # é™åˆ¶é•·åº¦é¿å…çˆ† Token
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

# --- [B] MCP Tools Wrapper (Stock) ---
async def mcp_get_stock_price(ticker: str) -> str:
    """[MCP] æŸ¥è©¢è‚¡ç¥¨åƒ¹æ ¼ (ä¾‹å¦‚ 2330.TW, NVDA)ã€‚"""
    print(f"   ğŸ“ˆ [MCP Stock] æŸ¥è©¢: {ticker}")
    return await run_mcp_tool(
        get_stock_server_params(), 
        "get_stock_price", 
        {"ticker": ticker}
    )

stock_tool = StructuredTool.from_function(
    coroutine=mcp_get_stock_price,
    name="get_stock_price",
    description="æŸ¥è©¢å³æ™‚è‚¡åƒ¹ã€‚",
)

# --- [C] MCP Tools Wrapper (Filesystem) ---
# æˆ‘å€‘å°‡ Filesystem MCP çš„åŠŸèƒ½æ‹†åˆ†æˆå¹¾å€‹æ˜ç¢ºçš„ LangChain å·¥å…·

async def mcp_write_file(filename: str, content: str) -> str:
    """[MCP] å°‡å…§å®¹å¯«å…¥æª”æ¡ˆã€‚åƒ…é™ workspace ç›®éŒ„ã€‚"""
    print(f"   ğŸ’¾ [MCP File] å¯«å…¥æª”æ¡ˆ: {filename}")
    return await run_mcp_tool(
        get_filesystem_server_params(),
        "write_file",
        {"path": filename, "content": content} # æ³¨æ„ï¼šfilesystem server çš„åƒæ•¸åæ˜¯ 'path'
    )

async def mcp_read_file(filename: str) -> str:
    """[MCP] è®€å– workspace ç›®éŒ„ä¸‹çš„æª”æ¡ˆå…§å®¹ã€‚"""
    print(f"   ğŸ“– [MCP File] è®€å–æª”æ¡ˆ: {filename}")
    return await run_mcp_tool(
        get_filesystem_server_params(),
        "read_file",
        {"path": filename}
    )

async def mcp_list_files() -> str:
    """[MCP] åˆ—å‡º workspace ç›®éŒ„ä¸‹çš„æ‰€æœ‰æª”æ¡ˆã€‚"""
    print(f"   ğŸ“‚ [MCP File] åˆ—å‡ºç›®éŒ„")
    return await run_mcp_tool(
        get_filesystem_server_params(),
        "list_directory",
        {"path": WORKSPACE_DIR} # list_directory éœ€è¦æŒ‡å®šè·¯å¾‘
    )

write_file_tool = StructuredTool.from_function(
    coroutine=mcp_write_file,
    name="save_file", # çµ¦ LLM çœ‹çš„åå­—
    description="å°‡æ–‡æœ¬å…§å®¹å„²å­˜åˆ°æª”æ¡ˆä¸­ã€‚é©åˆç”¨ä¾†å„²å­˜å ±å‘Šã€ç¨‹å¼ç¢¼æˆ–æ‘˜è¦ã€‚",
)

read_file_tool = StructuredTool.from_function(
    coroutine=mcp_read_file,
    name="read_file",
    description="è®€å–å·²å­˜åœ¨çš„æª”æ¡ˆå…§å®¹ã€‚",
)

list_files_tool = StructuredTool.from_function(
    coroutine=mcp_list_files,
    name="list_files",
    description="æŸ¥çœ‹ç›®å‰å·¥ä½œç›®éŒ„ä¸‹æœ‰å“ªäº›æª”æ¡ˆã€‚",
)

# æ•´åˆæ‰€æœ‰å·¥å…·
tools_list = [
    lookup_pdf_knowledge, 
    search_web, 
    stock_tool,
    write_file_tool,
    read_file_tool,
    list_files_tool
]

# ==========================================
# 4. å»ºæ§‹ LangGraph
# ==========================================
class AgentInput(BaseModel):
    messages: List[Union[HumanMessage, AIMessage, SystemMessage]]

def create_agent_graph():
    llm = get_llm()
    llm_with_tools = llm.bind_tools(tools_list)

    def agent_node(state: MessagesState):
        messages = state["messages"]
        
        # ç³»çµ±æç¤ºè©ï¼šæ˜ç¢ºå‘ŠçŸ¥å®ƒæœ‰æª”æ¡ˆæ“ä½œèƒ½åŠ›
        has_system_msg = any(isinstance(msg, SystemMessage) for msg in messages)
        if not has_system_msg:
            system_msg = SystemMessage(
                content="ä½ æ˜¯ä¸€å€‹å¼·å¤§çš„ AI åŠ©æ‰‹ï¼Œé…å‚™äº†å¤šç¨®å·¥å…·ã€‚\n"
                "ä½ å¯ä»¥æŸ¥è©¢è‚¡åƒ¹ã€æœå°‹ç¶²è·¯ã€æŸ¥è©¢å…§éƒ¨çŸ¥è­˜åº«ã€‚\n"
                "ğŸ”¥ é‡è¦ï¼šä½ ç¾åœ¨æ“æœ‰æª”æ¡ˆç³»çµ±æ¬Šé™ï¼\n"
                "- ç•¶ä½¿ç”¨è€…è¦æ±‚ã€å¯«å ±å‘Šã€ã€ã€å­˜æª”ã€æ™‚ï¼Œè«‹å‹™å¿…ä½¿ç”¨ save_file å·¥å…·ã€‚\n"
                "- ä½ å¯ä»¥å…ˆæœå°‹è³‡è¨Šï¼Œæ•´ç†å¾Œå†å¯«å…¥æª”æ¡ˆã€‚\n"
                "- æª”æ¡ˆé è¨­å­˜åœ¨ä¼ºæœå™¨çš„ workspace ç›®éŒ„ä¸­ã€‚"
            )
            messages = [system_msg] + messages
        
        response = llm_with_tools.invoke(messages)
        return {"messages": [response]}

    builder = StateGraph(MessagesState)
    builder.add_node("agent", agent_node)
    builder.add_node("tools", ToolNode(tools_list))

    builder.add_edge(START, "agent")
    builder.add_conditional_edges("agent", tools_condition)
    builder.add_edge("tools", "agent")

    memory = MemorySaver()
    graph = builder.compile(checkpointer=memory)
    graph = graph.with_types(input_type=AgentInput)
    # è¨­å®š thread_idï¼Œé€™æ˜¯ä½¿ç”¨ checkpointer æ™‚å¿…éœ€çš„
    graph = graph.with_config(configurable={"thread_id": "web-user-demo"})
    return graph

# ==========================================
# 5. FastAPI æ‡‰ç”¨
# ==========================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    # æª¢æŸ¥ç’°å¢ƒ
    if not os.path.exists("stock_mcp_server.py"):
        print("âŒ è­¦å‘Šï¼šæ‰¾ä¸åˆ° stock_mcp_server.py")
    
    # ç°¡å–®æª¢æŸ¥ npx æ˜¯å¦å¯ç”¨
    import shutil
    if not shutil.which("npx"):
        print("âŒ åš´é‡è­¦å‘Šï¼šæ‰¾ä¸åˆ° 'npx' æŒ‡ä»¤ï¼ŒFilesystem MCP ç„¡æ³•å•Ÿå‹•ï¼è«‹å®‰è£ Node.jsã€‚")
    
    yield
    print("ğŸ‘‹ Server é—œé–‰ä¸­...")

app = FastAPI(
    title="Super Agent (Stock + Filesystem)",
    version="2.0",
    description="Agent with Local Tools, Stock MCP, and Filesystem MCP",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

graph = create_agent_graph()

add_routes(app, graph, path="/agent", playground_type="default")

if __name__ == "__main__":
    print("\nğŸš€ Super Server å•Ÿå‹•ä¸­...")
    print(f"ğŸ“‚ å·¥ä½œç›®éŒ„: {WORKSPACE_DIR}")
    print("ğŸ‘‰ Playground: http://localhost:8000/agent/playground/")
    uvicorn.run(app, host="0.0.0.0", port=8000)
# 2025-12-15 æœ‰å¾ˆå¤§çš„å•é¡Œï¼Œå‡ºç¾äº†error
# backend_server_LLM_gemini.py
import os
import uvicorn
import asyncio
from contextlib import asynccontextmanager, AsyncExitStack
from typing import List, Union

# --- Third Party Imports ---
from dotenv import load_dotenv
import yfinance as yf # é›–ç„¶ä¸»è¦é‚è¼¯ç§»èµ°äº†ï¼Œä½†ä¿ç•™ä»¥é˜²è¬ä¸€

# --- FastAPI & LangServe ---
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from langserve import add_routes
from pydantic import BaseModel

# --- LangChain & LangGraph ---
from langchain_groq import ChatGroq
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool, StructuredTool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from langgraph.graph import StateGraph, START, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver

# --- MCP Imports ---
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

load_dotenv()

# ==========================================
# 0. å…¨åŸŸè®Šæ•¸ç®¡ç†
# ==========================================
# ç”¨ä¾†å­˜æ”¾æ‰€æœ‰å·¥å…· (åŒ…å«æœ¬åœ° + MCP)
global_tools_list = []
# ç”¨ä¾†ç®¡ç† MCP çš„é€£ç·šè³‡æºï¼Œç¢ºä¿é—œé–‰æ™‚èƒ½æ–·ç·š
exit_stack = AsyncExitStack()

def get_llm():
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
    return ChatGroq(
        model="llama-3.3-70b-versatile", # è«‹ç¢ºä¿ Groq æ”¯æ´æ­¤æ¨¡å‹åç¨±ï¼Œæˆ–æ”¹ç‚º "llama3-70b-8192" ç­‰
        temperature=0,
        max_retries=2,
    )

# ==========================================
# 1. æœ¬åœ° RAG å·¥å…· (ç¶­æŒä¸è®Š)
# ==========================================
print("ğŸš€ [Server] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº« (Local RAG)...")
pdf_path = "./data/Tree_of_Thoughts.pdf"
retriever = None

if os.path.exists(pdf_path):
    # æ³¨æ„ï¼šé€™è£¡å‡è¨­ä½ æœ‰ GOOGLE_API_KEY ç”¨æ–¼ Embedding
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… PDF è¼‰å…¥å®Œæˆã€‚")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

@tool
def lookup_pdf_knowledge(query: str) -> str:
    """æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚"""
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ”§ [Tool: RAG] Server æ­£åœ¨æª¢ç´¢ PDF: {query}")
    llm_rag = get_llm()
    prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_rag
        | StrOutputParser()
    )
    return chain.invoke(query)

# ==========================================
# 2. MCP é€£ç·šè¨­å®šèˆ‡å·¥å…·è¼‰å…¥
# ==========================================
def get_mcp_servers():
    """å®šç¾©è¦é€£æ¥çš„ MCP Servers"""
    return {
        # 1. æœ¬åœ° Python è‚¡å¸‚å¾®æœå‹™
        "stock_server": StdioServerParameters(
            command="uv", # ç¢ºä¿ä½ æœ‰å®‰è£ uv
            args=["run", "stock_mcp_server.py"], 
            env=os.environ.copy()
        ),
        # 2. Brave Search (Node.js) - å®˜æ–¹ MCP
        "brave_server": StdioServerParameters(
            command="npx",
            args=["-y", "@modelcontextprotocol/server-brave-search"],
            env={**os.environ.copy(), "BRAVE_API_KEY": os.getenv("BRAVE_API_KEY", "")}
        )
    }

async def load_mcp_tools_into_global():
    """é€£æ¥æ‰€æœ‰ MCP Servers ä¸¦å°‡å·¥å…·åŠ å…¥ global_tools_list"""
    global global_tools_list
    
    servers = get_mcp_servers()
    mcp_tools = []
    
    print("\nğŸ”Œ æ­£åœ¨å»ºç«‹ MCP é€£ç·š...")
    
    for server_name, server_params in servers.items():
        try:
            print(f"   ğŸ‘‰ é€£æ¥: {server_name}...")
            # ä½¿ç”¨ exit_stack ç®¡ç†é€£ç·šç”Ÿå‘½é€±æœŸ
            read, write = await exit_stack.enter_async_context(stdio_client(server_params))
            session = await exit_stack.enter_async_context(ClientSession(read, write))
            await session.initialize()
            
            # åˆ—å‡ºè©² Server çš„å·¥å…·
            tools_list = await session.list_tools()
            print(f"      âœ… æˆåŠŸï¼Œå·¥å…·åˆ—è¡¨: {[t.name for t in tools_list.tools]}")
            
            # å°‡ MCP Tool è½‰æ›ç‚º LangChain Tool
            for tool_info in tools_list.tools:
                # å®šç¾© wrapper function ä¸¦é–å®š session èˆ‡ tool_name
                def make_tool_func(current_session, tool_name):
                    async def mcp_wrapper(**kwargs):
                        # print(f"DEBUG: Calling MCP tool {tool_name} with {kwargs}")
                        result = await current_session.call_tool(tool_name, arguments=kwargs)
                        # è§£æ MCP å›å‚³çµæœ (TextContent)
                        if result.content and hasattr(result.content[0], 'text'):
                            return result.content[0].text
                        return str(result)
                    return mcp_wrapper

                mcp_tool = StructuredTool.from_function(
                    func=None,
                    coroutine=make_tool_func(session, tool_info.name),
                    name=tool_info.name,
                    description=tool_info.description or f"MCP Tool: {tool_info.name}",
                )
                mcp_tools.append(mcp_tool)
                
        except Exception as e:
            print(f"âŒ é€£æ¥ {server_name} å¤±æ•—: {e}")

    # æ›´æ–°å…¨åŸŸå·¥å…·åˆ—è¡¨ï¼šæœ¬åœ° RAG + æ‰€æœ‰ MCP å·¥å…·
    global_tools_list = [lookup_pdf_knowledge] + mcp_tools
    print(f"ğŸ‰ å·¥å…·è¼‰å…¥å®Œç•¢ï¼Œç¸½å…± {len(global_tools_list)} å€‹å·¥å…·å¯ç”¨ã€‚")

# ==========================================
# 3. LangGraph å®šç¾©
# ==========================================
class AgentInput(BaseModel):
    messages: List[Union[HumanMessage, AIMessage, SystemMessage]]

def create_agent_graph():
    # æ³¨æ„ï¼šé€™è£¡ä¸ç›´æ¥ bindï¼Œè€Œæ˜¯åœ¨ node å…§éƒ¨ bindï¼Œ
    # é€™æ¨£å¯ä»¥ç¢ºä¿ç”¨åˆ°æœ€æ–°çš„ global_tools_list
    
    def agent_node(state: MessagesState):
        messages = state["messages"]
        
        # 1. æ³¨å…¥ System Message (å¦‚æœé‚„æ²’æœ‰)
        if not any(isinstance(msg, SystemMessage) for msg in messages):
            system_msg = SystemMessage(
                content="ä½ æ˜¯ä¸€å€‹å¼·å¤§çš„ AI åŠ©æ‰‹ï¼Œæ“æœ‰å³æ™‚è¯ç¶² (Brave Search) å’Œè‚¡å¸‚æŸ¥è©¢ (Stock MCP) çš„èƒ½åŠ›ã€‚\n"
                "å°æ–¼å³æ™‚è³‡è¨Šï¼Œè«‹å„ªå…ˆä½¿ç”¨ brave_web_searchã€‚\n"
                "å°æ–¼è‚¡åƒ¹ï¼Œè«‹ä½¿ç”¨ get_stock_priceã€‚\n"
                "å°æ–¼ 'Tree of Thoughts' è«–æ–‡å•é¡Œï¼Œè«‹ä½¿ç”¨ lookup_pdf_knowledgeã€‚"
            )
            messages = [system_msg] + messages
        
        # 2. å‹•æ…‹ç¶å®šç•¶å‰çš„å·¥å…·åˆ—è¡¨
        llm = get_llm()
        llm_with_tools = llm.bind_tools(global_tools_list)
        
        # 3. åŸ·è¡Œ
        response = llm_with_tools.invoke(messages)
        return {"messages": [response]}

    # å»ºæ§‹ Graph
    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", agent_node)
    
    # ToolNode å¿…é ˆä½¿ç”¨ "ç•¶ä¸‹" çš„å·¥å…·åˆ—è¡¨
    # é€™è£¡ä½¿ç”¨ä¸€å€‹ lambda æˆ– wrapper ä¾†ç¢ºä¿å®ƒèƒ½æŠ“åˆ°æœ€æ–°çš„ global_tools_list
    # ä½† ToolNode åˆå§‹åŒ–æ™‚éœ€è¦ listï¼Œæ‰€ä»¥æˆ‘å€‘æœƒåœ¨ lifespan æ›´æ–°å¾Œé‡å»º graphï¼Œ
    # æˆ–æ˜¯é€™è£¡å…ˆå‚³ä¸€å€‹ç©ºçš„ï¼Œä½†åŸ·è¡Œæ™‚å¸Œæœ›èƒ½å‹•æ…‹ã€‚
    # ç‚ºäº†ç°¡å–®èµ·è¦‹ï¼šæˆ‘å€‘å‡è¨­ lifespan æœƒåœ¨ app å•Ÿå‹•å‰è·‘å®Œï¼Œé€™è£¡ä½¿ç”¨ global è®Šæ•¸å¼•ç”¨
    workflow.add_node("tools", ToolNode(global_tools_list))

    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges("agent", tools_condition)
    workflow.add_edge("tools", "agent")

    return workflow.compile(checkpointer=MemorySaver())

# ==========================================
# 4. FastAPI Setup & Lifespan
# ==========================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    # --- å•Ÿå‹•æ™‚ ---
    # 1. å»ºç«‹ MCP é€£ç·šä¸¦å¡«å…¥ global_tools_list
    await load_mcp_tools_into_global()
    
    # 2. é‡æ–°å»ºç«‹ Graph (å› ç‚ºå·¥å…·åˆ—è¡¨å·²æ›´æ–°)
    # æ³¨æ„ï¼šadd_routes å·²ç¶“åœ¨ä¸‹é¢åŸ·è¡Œäº†ï¼Œä½†æˆ‘å€‘å¯ä»¥æ›´æ–° app.state æˆ–é‡æ–°è³¦å€¼
    # é‡å° LangServeï¼Œæœ€ç°¡å–®çš„æ–¹æ³•æ˜¯è®“ graph åœ¨é€™è£¡è¢«å®Œå…¨åˆå§‹åŒ–
    # ä½†ç”±æ–¼ add_routes åœ¨ import time åŸ·è¡Œï¼Œé€™æœ‰é» trickyã€‚
    # ä¸éï¼Œå› ç‚º ToolNode å­˜çš„æ˜¯ referenceï¼Œæˆ–è€…æˆ‘å€‘åœ¨é€™è£¡é‡æ–°åŸ·è¡Œ add_routes (ä¸æ¨è–¦)ã€‚
    
    # æŠ€å·§ï¼šæˆ‘å€‘åœ¨é€™è£¡æ›´æ–°ä¸€å€‹å…¨åŸŸçš„ graph ç‰©ä»¶ (å¦‚æœæœ‰çš„è©±)ï¼Œ
    # ä½†ç‚ºäº†è®“ä¸Šé¢çš„ create_agent_graph ç”Ÿæ•ˆï¼Œæˆ‘å€‘éœ€è¦ç¢ºä¿ global_tools_list å·²ç¶“æœ‰æ±è¥¿ã€‚
    # å¯¦éš›ä¸Šï¼Œglobal_tools_list åœ¨é€™è£¡è¢«å¡«æ»¿ã€‚
    
    # æˆ‘å€‘åœ¨é€™è£¡é‡æ–° compile ä¸€æ¬¡ graphï¼Œä»¥ç¢ºä¿ ToolNode æ‹¿åˆ°æ­£ç¢ºçš„å·¥å…·
    # é€™æœƒå½±éŸ¿åˆ°å¾ŒçºŒçš„è«‹æ±‚
    app.state.graph = create_agent_graph()
    
    yield
    
    # --- é—œé–‰æ™‚ ---
    print("ğŸ‘‹ é—œé–‰ Agent Serverï¼Œæ­£åœ¨æ–·é–‹ MCP é€£ç·š...")
    await exit_stack.aclose()

app = FastAPI(
    title="Hybrid MCP Agent",
    version="2.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ç‚ºäº†è®“ LangServe åœ¨å•Ÿå‹•å‰å°±èƒ½è¨»å†Šè·¯å¾‘ï¼Œæˆ‘å€‘å…ˆå»ºç«‹ä¸€å€‹ã€Œæš«æ™‚ã€çš„ graph
# çœŸæ­£çš„å·¥å…·æœƒåœ¨ lifespan å•Ÿå‹•å¾Œæ³¨å…¥
initial_graph = create_agent_graph()

# ä½¿ç”¨ RunnableLambda åŒ…è£ï¼Œä»¥ä¾¿åœ¨åŸ·è¡Œæ™‚å‹•æ…‹ç²å–æœ€æ–°çš„ graph
# é€™æ˜¯è§£æ±º "Lifespan è¼‰å…¥å·¥å…· vs Import time è¨»å†Šè·¯å¾‘" çš„é€²éšæŠ€å·§
from langchain_core.runnables import RunnableLambda

def get_graph_runnable(input_data):
    # å˜—è©¦å¾ app.state ç²å–åˆå§‹åŒ–å®Œæˆçš„ graphï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨åˆå§‹ graph
    if hasattr(app, "state") and hasattr(app.state, "graph"):
        return app.state.graph.invoke(input_data)
    return initial_graph.invoke(input_data)

# é€™è£¡æˆ‘å€‘è¨»å†Šä¸€å€‹å‹•æ…‹çš„ Runnable
add_routes(
    app,
    RunnableLambda(get_graph_runnable).with_types(input_type=AgentInput),
    path="/agent",
    playground_type="default",
)

if __name__ == "__main__":
    print("\nğŸš€ å•Ÿå‹• Hybrid MCP Agent...")
    print("ğŸ‘‰ Playground: http://localhost:8000/agent/playground/")
    uvicorn.run(app, host="0.0.0.0", port=8000)
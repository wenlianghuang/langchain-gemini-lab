import os
import yfinance as yf
from dotenv import load_dotenv

# --- LangChain Imports ---
from langchain_groq import ChatGroq
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.tools.tavily_search import TavilySearchResults

# --- LangGraph Imports ---
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()

# ==========================================
# 0. è¨­å®š LLM
# ==========================================
def get_llm():
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("âŒ æ‰¾ä¸åˆ° GROQ_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")
    return ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0,
        max_retries=2,
    )

# ==========================================
# 1. ç³»çµ±åˆå§‹åŒ–ï¼šå…¨åŸŸè³‡æº
# ==========================================
print("ğŸš€ [System] æ­£åœ¨åˆå§‹åŒ–å‘é‡è³‡æ–™åº«...")
pdf_path = "./data/Tree_of_Thoughts.pdf"

retriever = None
if os.path.exists(pdf_path):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("âœ… PDF è¼‰å…¥å®Œæˆã€‚")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# ==========================================
# 2. å®šç¾©å·¥å…·
# ==========================================
@tool
def get_stock_price(ticker: str) -> str:
    """
    æŸ¥è©¢è‚¡ç¥¨çš„å³æ™‚åƒ¹æ ¼ã€‚
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç¢¼ï¼Œä¾‹å¦‚ "2330.TW" (å°ç©é›»), "NVDA" (NVIDIA), "GOOG" (Google)
    
    Returns:
        è‚¡ç¥¨çš„ç•¶å‰åƒ¹æ ¼è³‡è¨Š
    """
    print(f"   ğŸ”§ [Tool: Stock] æŸ¥è©¢: {ticker}")
    try:
        stock = yf.Ticker(ticker)
        hist = stock.history(period="1d")
        if hist.empty: return f"æ‰¾ä¸åˆ° {ticker}"
        price = hist['Close'].iloc[-1]
        curr = stock.info.get('currency', '?')
        return f"{ticker} ç¾åƒ¹: {price:.2f} {curr}"
    except Exception as e:
        return f"è‚¡å¸‚æŸ¥è©¢éŒ¯èª¤: {e}"

@tool
def lookup_pdf_knowledge(query: str) -> str:
    """æŸ¥è©¢é—œæ–¼ 'Tree of Thoughts' (ToT) è«–æ–‡çš„å…§éƒ¨çŸ¥è­˜åº«ã€‚"""
    if not retriever: return "è³‡æ–™åº«æœªè¼‰å…¥ã€‚"
    print(f"   ğŸ”§ [Tool: RAG] æª¢ç´¢ PDF: {query}")
    llm_rag = get_llm()
    prompt = ChatPromptTemplate.from_template("åŸºæ–¼æ–‡ä»¶å›ç­”ï¼š\n{context}\nå•é¡Œï¼š{question}")
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_rag
        | StrOutputParser()
    )
    return chain.invoke(query)

@tool
def search_web(query: str) -> str:
    """æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚"""
    print(f"   ğŸ”§ [Tool: Web] ä¸Šç¶²æœå°‹: {query}")
    try:
        tool = TavilySearchResults(k=3)
        return tool.invoke(query)
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

tools_list = [get_stock_price, lookup_pdf_knowledge, search_web]

# ==========================================
# 3. å»ºæ§‹ LangGraph
# ==========================================
from langchain_core.messages import SystemMessage

llm = get_llm()
# ç¶å®šå·¥å…·åˆ° LLM
llm_with_tools = llm.bind_tools(tools_list)

def agent_node(state: MessagesState):
    messages = state["messages"]
    
    # ç¢ºä¿ç¬¬ä¸€æ¢è¨Šæ¯æ˜¯ç³»çµ±æç¤ºï¼Œå¼•å°æ¨¡å‹æ­£ç¢ºä½¿ç”¨å·¥å…·
    # æ³¨æ„ï¼šéœ€è¦æª¢æŸ¥æ˜¯å¦å·²æœ‰ç³»çµ±è¨Šæ¯ï¼Œé¿å…é‡è¤‡æ·»åŠ 
    has_system_msg = any(isinstance(msg, SystemMessage) for msg in messages)
    if not has_system_msg:
        system_msg = SystemMessage(
            content="ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·ä¾†å›ç­”å•é¡Œã€‚"
            "ç•¶éœ€è¦æŸ¥è©¢è‚¡ç¥¨åƒ¹æ ¼æ™‚ï¼Œä½¿ç”¨ get_stock_price å·¥å…·ï¼›"
            "ç•¶éœ€è¦æŸ¥è©¢PDFçŸ¥è­˜æ™‚ï¼Œä½¿ç”¨ lookup_pdf_knowledge å·¥å…·ï¼›"
            "ç•¶éœ€è¦æœå°‹ç¶²è·¯è³‡è¨Šæ™‚ï¼Œä½¿ç”¨ search_web å·¥å…·ã€‚"
            "è«‹ä½¿ç”¨æ¨™æº–çš„JSONæ ¼å¼é€²è¡Œå·¥å…·å‘¼å«ï¼Œéµå¾ªLangChainçš„å·¥å…·å‘¼å«è¦ç¯„ã€‚"
        )
        messages = [system_msg] + messages
    
    try:
        response = llm_with_tools.invoke(messages)
        
        # é©—è­‰å›æ‡‰æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„å·¥å…·å‘¼å«
        if hasattr(response, 'tool_calls') and response.tool_calls:
            # æª¢æŸ¥å·¥å…·å‘¼å«æ ¼å¼æ˜¯å¦æ­£ç¢º
            for tool_call in response.tool_calls:
                if not isinstance(tool_call, dict):
                    raise ValueError(f"å·¥å…·å‘¼å«æ ¼å¼ä¸æ­£ç¢º: {tool_call}")
                if 'name' not in tool_call or 'args' not in tool_call:
                    raise ValueError(f"å·¥å…·å‘¼å«ç¼ºå°‘å¿…è¦æ¬„ä½: {tool_call}")
        
        return {"messages": [response]}
        
    except Exception as e:
        error_str = str(e)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Groq å·¥å…·å‘¼å«æ ¼å¼éŒ¯èª¤
        if "tool_use_failed" in error_str or "Failed to call a function" in error_str:
            from langchain_core.messages import AIMessage
            error_msg = AIMessage(
                content="æŠ±æ­‰ï¼Œå·¥å…·å‘¼å«æ ¼å¼ç™¼ç”ŸéŒ¯èª¤ã€‚è®“æˆ‘å˜—è©¦ç”¨æ–‡å­—æ–¹å¼å›ç­”æ‚¨çš„å•é¡Œã€‚"
                f"ï¼ˆåŸå§‹éŒ¯èª¤ï¼š{error_str[:100]}...ï¼‰"
            )
            return {"messages": [error_msg]}
        else:
            # å…¶ä»–éŒ¯èª¤ï¼Œè¿”å›è©³ç´°éŒ¯èª¤è¨Šæ¯
            from langchain_core.messages import AIMessage
            error_msg = AIMessage(
                content=f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error_str}ã€‚è«‹é‡æ–°å˜—è©¦æˆ–æ›å€‹æ–¹å¼æå•ã€‚"
            )
            return {"messages": [error_msg]}

builder = StateGraph(MessagesState)
builder.add_node("agent", agent_node)
builder.add_node("tools", ToolNode(tools_list))

builder.add_edge(START, "agent")
builder.add_conditional_edges("agent", tools_condition)
builder.add_edge("tools", "agent")

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# ==========================================
# âœ¨ æ–°å¢åŠŸèƒ½ï¼šè¦–è¦ºåŒ– Graph
# ==========================================
def generate_visualization(graph_obj):
    """å°‡ Graph çµæ§‹åŒ¯å‡ºç‚º PNG åœ–ç‰‡"""
    print("\nğŸ“Š æ­£åœ¨ç”¢ç”Ÿ LangGraph æµç¨‹åœ–...")
    try:
        # å–å¾— Graph çš„ Mermaid PNG äºŒé€²ä½è³‡æ–™
        image_data = graph_obj.get_graph().draw_mermaid_png()
        
        # å¯«å…¥æª”æ¡ˆ
        output_file = "agent_graph.png"
        with open(output_file, "wb") as f:
            f.write(image_data)
        print(f"âœ… æµç¨‹åœ–å·²æˆåŠŸå„²å­˜ç‚º '{output_file}'ï¼Œè«‹åœ¨æª”æ¡ˆç¸½ç®¡ä¸­æ‰“é–‹æŸ¥çœ‹ï¼")
        
    except Exception as e:
        # å¦‚æœå› ç‚ºç¼ºå°‘ä¾è³´ (å¦‚ graphviz) å¤±æ•—ï¼Œå‰‡å°å‡ºæ–‡å­—ç‰ˆä»£ç¢¼
        print(f"âš ï¸ åœ–ç‰‡ç”¢ç”Ÿå¤±æ•— (å¯èƒ½æ˜¯ç¼ºå°‘ç¹ªåœ–ä¾è³´): {e}")
        print("ğŸ‘‰ æ‚¨å¯ä»¥è¤‡è£½ä¸‹æ–¹çš„ Mermaid ä»£ç¢¼ï¼Œè²¼åˆ° https://mermaid.live æŸ¥çœ‹ï¼š")
        print("-" * 30)
        print(graph_obj.get_graph().draw_mermaid())
        print("-" * 30)

# ==========================================
# 4. åŸ·è¡Œä¸»ç¨‹å¼
# ==========================================
def main():
    # 1. å…ˆç”¢ç”Ÿè¦–è¦ºåŒ–åœ–è¡¨
    generate_visualization(graph)

    print("\nğŸ¤– LangGraph Super Agent (Engine: Groq) ä¸Šç·šï¼")
    print("ğŸ’¡ è©¦è©¦ï¼š'å°ç©é›»è‚¡åƒ¹å¤šå°‘ï¼Ÿå¦å¤–ç¶²è·¯ä¸Šæœ‰ä»€éº¼é—œæ–¼å®ƒçš„æ–°èï¼Ÿ'\n")

    config = {"configurable": {"thread_id": "demo-viz-001"}}

    while True:
        try:
            user_input = input("User: ").strip()
            if user_input.lower() in ["quit", "exit"]:
                break
            if not user_input: continue

            print("   (Groq æ€è€ƒä¸­...)")
            
            last_printed_msg_id = None
            for event in graph.stream({"messages": [HumanMessage(content=user_input)]}, config, stream_mode="values"):
                current_messages = event["messages"]
                if not current_messages: continue
                last_msg = current_messages[-1]
                if last_msg.id == last_printed_msg_id: continue
                last_printed_msg_id = last_msg.id

                if last_msg.type == "ai" and last_msg.tool_calls:
                    tool_names = [tc["name"] for tc in last_msg.tool_calls]
                    print(f"   â¡ï¸ [Agent] æ±ºå®šå‘¼å«: {tool_names}")
                elif last_msg.type == "tool":
                    print(f"   â¡ï¸ [Tool] {last_msg.name} å®Œæˆã€‚")
                elif last_msg.type == "ai" and not last_msg.tool_calls:
                    print(f"\nAI: {last_msg.content}\n")
                    
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
import os
from dotenv import load_dotenv

# è¼‰å…¥ LangChain å…ƒä»¶ï¼ˆä½¿ç”¨æœ€æ–°çš„ LCEL APIï¼‰
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def main():
    pdf_path = "./data/Tree_of_Thoughts.pdf"
    if not os.path.exists(pdf_path):
        print("âŒ æ‰¾ä¸åˆ° PDF æª”æ¡ˆï¼Œè«‹ç¢ºèª data/Tree_of_Thoughts.pdf å­˜åœ¨ã€‚")
        return

    print("ðŸš€ åˆå§‹åŒ–å…·å‚™ã€Œè¨˜æ†¶åŠŸèƒ½ã€çš„ RAG ç³»çµ±ï¼ˆä½¿ç”¨æœ€æ–° LCEL APIï¼‰...")

    # --- 1. æº–å‚™è³‡æ–™ (è·Ÿä¸Šä¸€å ‚èª²ä¸€æ¨£) ---
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # --- 2. æº–å‚™ LLM ---
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest", temperature=0)

    # --- 3. å»ºç«‹ã€Œå•é¡Œé‡çµ„ã€éˆ (History Aware Retriever) - ä½¿ç”¨ LCEL ---
    # é€™å€‹ Prompt çš„ç›®çš„æ˜¯ï¼šå¦‚æžœä½¿ç”¨è€…å•äº†ä»£åè©žï¼Œåƒè€ƒæ­·å²ç´€éŒ„æŠŠå®ƒæ”¹å¯«æˆå®Œæ•´å•é¡Œ
    contextualize_q_system_prompt = """
    çµ¦å®šä¸€æ®µèŠå¤©æ­·å²è¨˜éŒ„å’Œä½¿ç”¨è€…æœ€æ–°çš„å•é¡Œï¼ˆè©²å•é¡Œå¯èƒ½å¼•ç”¨äº†æ­·å²è¨˜éŒ„ä¸­çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œ
    è«‹å°‡è©²å•é¡Œé‡æ–°è¡¨è¿°ç‚ºä¸€å€‹ç¨ç«‹çš„å•é¡Œï¼Œä½¿å…¶åœ¨æ²’æœ‰èŠå¤©æ­·å²è¨˜éŒ„çš„æƒ…æ³ä¸‹ä¹Ÿèƒ½è¢«ç†è§£ã€‚
    ç›´æŽ¥å›žå‚³æ”¹å¯«å¾Œçš„å•é¡Œå³å¯ï¼Œä¸è¦å›žç­”å•é¡Œï¼Œä¹Ÿä¸è¦è§£é‡‹ã€‚
    """
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # ä½¿ç”¨ LCEL å¯¦ç¾ï¼šå…ˆæ”¹å¯«å•é¡Œï¼Œå†ç”¨æ”¹å¯«å¾Œçš„å•é¡Œæª¢ç´¢
    def format_docs(docs):
        """å°‡æª¢ç´¢åˆ°çš„æ–‡æª”æ ¼å¼åŒ–ç‚ºå­—ä¸²"""
        return "\n\n".join(doc.page_content for doc in docs)
    
    # History-aware retriever: å¦‚æžœæœ‰æ­·å²è¨˜éŒ„ï¼Œå…ˆæ”¹å¯«å•é¡Œå†æª¢ç´¢ï¼›å¦å‰‡ç›´æŽ¥æª¢ç´¢
    def get_standalone_question(input_dict):
        """æ ¹æ“šæ­·å²è¨˜éŒ„æ”¹å¯«å•é¡Œï¼Œä½¿å…¶æˆç‚ºç¨ç«‹å•é¡Œ"""
        # å¦‚æžœæœ‰æ­·å²è¨˜éŒ„ï¼Œç”¨ LLM æ”¹å¯«å•é¡Œ
        if input_dict.get("chat_history"):
            standalone_question_chain = contextualize_q_prompt | llm | StrOutputParser()
            return standalone_question_chain.invoke(input_dict)
        # æ²’æœ‰æ­·å²è¨˜éŒ„ï¼Œç›´æŽ¥è¿”å›žåŽŸå§‹å•é¡Œ
        return input_dict["input"]
    
    # çµ„åˆï¼šæ”¹å¯«å•é¡Œ -> æª¢ç´¢æ–‡æª” -> æ ¼å¼åŒ–
    def retrieve_documents(input_dict):
        """æª¢ç´¢æ–‡æª”ä¸¦æ ¼å¼åŒ–"""
        question = get_standalone_question(input_dict)
        docs = retriever.invoke(question)
        return format_docs(docs)
    
    # --- 4. å»ºç«‹ã€Œå•ç­”ã€éˆ (Answer Chain) - ä½¿ç”¨ LCEL ---
    qa_system_prompt = """
    ä½ æ˜¯ä¸€å€‹å•ç­”åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ä¸Šä¸‹æ–‡ç‰‡æ®µä¾†å›žç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æžœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±èªªä¸çŸ¥é“ï¼Œä¸è¦è©¦åœ–ç·¨é€ ç­”æ¡ˆã€‚
    å›žç­”è«‹ä¿æŒç°¡æ½”ã€‚
    
    {context}
    """
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    # ä½¿ç”¨ LCEL çµ„åˆå®Œæ•´çš„ RAG éˆ
    rag_chain = (
        {
            "context": RunnableLambda(retrieve_documents),
            "input": lambda x: x["input"],
            "chat_history": lambda x: x.get("chat_history", []),
        }
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    # --- 5. é–‹å§‹å°è©± (ç®¡ç†è¨˜æ†¶) ---
    print("\nâœ… ç³»çµ±å°±ç·’ï¼æˆ‘æ˜¯æœ‰è¨˜æ†¶çš„ PDF åŠ©æ‰‹ã€‚(è¼¸å…¥ 'exit' é›¢é–‹)\n")
    
    # æˆ‘å€‘ç”¨ä¸€å€‹ List ä¾†æ‰‹å‹•ç®¡ç†å°è©±æ­·å²
    chat_history = []

    while True:
        user_input = input("ä½ ï¼š")
        if user_input.lower() in ["exit", "quit", "bye"]:
            break
        
        if not user_input.strip():
            continue

        print("ðŸ¤– (æ€è€ƒä¸­)...", end="", flush=True)
        
        # å‘¼å« Chainï¼Œä¸¦å‚³å…¥ç›®å‰çš„ chat_history
        response = rag_chain.invoke({
            "input": user_input,
            "chat_history": chat_history
        })
        
        print(f"\rAIï¼š{response}\n")
        
        # æ›´æ–°æ­·å²ç´€éŒ„
        # 1. åŠ å…¥ä½¿ç”¨è€…çš„è©±
        chat_history.append(HumanMessage(content=user_input))
        # 2. åŠ å…¥ AI çš„å›žç­”
        chat_history.append(AIMessage(content=response))

        # (é¸ç”¨) ä¿æŒæ­·å²ç´€éŒ„ä¸è¦å¤ªé•·ï¼Œä»¥å…å¡žçˆ† Context Windowï¼Œé›–ç„¶ Gemini 1.5 å¾ˆå¤§æ²’å·®
        if len(chat_history) > 10: 
            chat_history = chat_history[-10:]

if __name__ == "__main__":
    main()
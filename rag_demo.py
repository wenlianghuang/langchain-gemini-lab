import os
from dotenv import load_dotenv

# 1. è¼‰å…¥å¿…è¦çš„ LangChain å…ƒä»¶
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸ (è®€å– .env ä¸­çš„ GOOGLE_API_KEY)
load_dotenv()

# --- è³‡æ–™æº–å‚™éšæ®µ (æ¨¡æ“¬æ‚¨çš„å…¬å¸å…§éƒ¨æ–‡ä»¶) ---
# é€™è£¡æˆ‘å€‘ç›´æ¥ç”¨å­—ä¸²æ¨¡æ“¬ï¼Œé€šå¸¸é€™è£¡æœƒæ˜¯è®€å– PDF æˆ– TXT æª”
raw_text = """
ã€Tidalwave AI å“¡å·¥æ‰‹å†Š - 2025ç‰ˆã€‘
1. ä¸Šç­æ™‚é–“ï¼šæˆ‘å€‘æ¡å½ˆæ€§å·¥æ™‚ï¼Œæ ¸å¿ƒå·¥ä½œæ™‚é–“ç‚º 10:00 - 16:00ã€‚
2. è«‹å‡è¦å®šï¼šè©¦ç”¨æœŸä¸‰å€‹æœˆå…§å³äº«æœ‰ç‰¹ä¼‘ï¼Œåªè¦æå‰ä¸‰å¤©åœ¨ Slack æå‡ºå³å¯ã€‚
3. é ç«¯å·¥ä½œï¼šå·¥ç¨‹å¸«æ¯é€±äºŒã€å››å¯è‡ªç”±é¸æ“‡åœ¨å®¶å·¥ä½œ (WFH)ã€‚
4. ç¦åˆ©ï¼šè¾¦å…¬å®¤é›¶é£Ÿæ«ƒç„¡é™ä¾›æ‡‰ï¼Œæ¯é€±äº”ä¸‹åˆæœ‰ Happy Hourã€‚
5. å ±å¸³æµç¨‹ï¼šè³¼è²·é–‹ç™¼å·¥å…· (å¦‚ Copilot, Cursor) å¯å…¨é¡å ±å¸³ï¼Œéœ€ç¶“ CTO æ ¸å‡†ã€‚
"""

# --- RAG æµç¨‹é–‹å§‹ ---

def main():
    print("ğŸš€ åˆå§‹åŒ– RAG ç³»çµ±ä¸­...")

    # 1. åˆå§‹åŒ– Google çš„ AI æ¨¡å‹èˆ‡ Embedding å·¥å…·
    # ä½¿ç”¨ Google çš„ Embedding APIï¼Œæ¸›è¼• Mac Air çš„è² æ“”
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    llm = ChatGoogleGenerativeAI(model="gemini-flash-latest")

    # 2. æ–‡ä»¶è™•ç† (Splitting)
    # æŠŠé•·æ–‡ç« åˆ‡æˆå°å¡Šï¼Œé€™æ¨£ AI æœå°‹æ™‚æ¯”è¼ƒæº–ç¢º
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    docs = [Document(page_content=raw_text)]
    splits = text_splitter.split_documents(docs)

    # 3. å»ºç«‹å‘é‡è³‡æ–™åº« (Vector Store)
    # é€™æ­¥æœƒæŠŠæ–‡å­—è®Šæˆå‘é‡ä¸¦å­˜å­˜åœ¨è¨˜æ†¶é«”ä¸­ (æˆ–å­˜æˆæª”æ¡ˆ)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    # 4. å®šç¾© Prompt (æç¤ºè©)
    # é€™æ˜¯ RAG çš„é—œéµï¼šæˆ‘å€‘å‘Šè¨´ AI "åªæ ¹æ“šæä¾›çš„ context å›ç­”"
    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„äººè³‡åŠ©ç†ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ã€å…¬å¸è¦ç« ã€‘å…§å®¹å›ç­”å“¡å·¥çš„å•é¡Œã€‚
    å¦‚æœè¦ç« è£¡æ²’æœ‰æåˆ°ï¼Œè«‹ç›´æ¥èªªã€Œæ‰‹å†Šä¸­æœªæåŠã€ï¼Œä¸è¦çæ°ã€‚

    ã€å…¬å¸è¦ç« ã€‘ï¼š
    {context}

    å“¡å·¥å•é¡Œï¼š{question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 5. å»ºç«‹ RAG Chain (éˆ)
    # é€™æ˜¯ LangChain æœ€å„ªé›…çš„ "LCEL" èªæ³•
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # --- æ¸¬è©¦éšæ®µ ---
    print("\nâœ… ç³»çµ±æº–å‚™å®Œæˆï¼é–‹å§‹æ¸¬è©¦...\n")
    
    questions = [
        "è«‹å•æˆ‘å¦‚æœæƒ³è²· Cursor ç·¨è¼¯å™¨ï¼Œå¯ä»¥å ±å¸³å—ï¼Ÿ",
        "è©¦ç”¨æœŸæœ‰ç‰¹ä¼‘å—ï¼Ÿ",
        "è«‹å•å…¬å¸æœ‰æä¾›å…è²»åˆé¤å—ï¼Ÿ" # é€™æ˜¯é™·é˜±é¡Œï¼Œæ‰‹å†Šæ²’å¯«
    ]

    for q in questions:
        print(f"å•ï¼š{q}")
        print(f"ç­”ï¼š", end="", flush=True)
        # å¯¦éš›åŸ·è¡Œ
        result = rag_chain.invoke(q)
        print(result)
        print("-" * 30)

if __name__ == "__main__":
    main()
import os
import yfinance as yf
from dotenv import load_dotenv
from typing import List, TypedDict, Annotated, Iterator, Tuple, Optional, Any, Dict
import operator
import re
import torch
import shutil
import uuid
import gradio as gr
import mlx.core as mx
from mlx_lm import load, generate as mlx_generate

# è¨­å®š HuggingFace æ¨¡å‹ç·©å­˜ç›®éŒ„åˆ°å¤–æ¥ SSD
EXTERNAL_SSD_PATH = "/Volumes/T7_SSD"
HF_CACHE_DIR = os.path.join(EXTERNAL_SSD_PATH, "huggingface_cache")

# æª¢æŸ¥å¤–æ¥ SSD æ˜¯å¦å­˜åœ¨
if os.path.exists(EXTERNAL_SSD_PATH):
    # å‰µå»ºç·©å­˜ç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(HF_CACHE_DIR, exist_ok=True)
    # è¨­ç½® HuggingFace ç’°å¢ƒè®Šæ•¸ï¼ˆå¿…é ˆåœ¨å°å…¥ HuggingFace ç›¸é—œåº«ä¹‹å‰è¨­ç½®ï¼‰
    os.environ["HF_HOME"] = HF_CACHE_DIR
    os.environ["TRANSFORMERS_CACHE"] = os.path.join(HF_CACHE_DIR, "transformers")
    os.environ["HF_HUB_CACHE"] = os.path.join(HF_CACHE_DIR, "hub")
    print(f"ğŸ’¾ æ¨¡å‹ç·©å­˜ç›®éŒ„ï¼š{HF_CACHE_DIR}")
else:
    print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°å¤–æ¥ SSD {EXTERNAL_SSD_PATH}ï¼Œå°‡ä½¿ç”¨é è¨­ç·©å­˜ç›®éŒ„")

# LangChain
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults

# RAG ç›¸é—œå°å…¥
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough

# LangGraph
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()

# ==========================================
# 0. MLX æ¨¡å‹åŒ…è£å™¨ï¼ˆLangChain å…¼å®¹ï¼‰
# ==========================================
class MLXChatModel(BaseChatModel):
    """
    MLX æ¨¡å‹çš„ LangChain åŒ…è£å™¨
    å°‡ MLX æ¨¡å‹æ•´åˆåˆ° LangChain ç”Ÿæ…‹ç³»çµ±ä¸­
    """
    model: Any = None
    tokenizer: Any = None
    max_tokens: int = 512
    temperature: float = 0.7
    
    def __init__(self, model, tokenizer, max_tokens=512, temperature=0.7, **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.tokenizer = tokenizer
        self.max_tokens = max_tokens
        self.temperature = temperature
    
    @property
    def _llm_type(self) -> str:
        return "mlx"
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """ç”Ÿæˆå›ç­”"""
        # å°‡ LangChain æ¶ˆæ¯è½‰æ›ç‚ºæ¨¡å‹æ ¼å¼
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                formatted_messages.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                formatted_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                formatted_messages.append({"role": "assistant", "content": msg.content})
        
        # ä½¿ç”¨ tokenizer æ ¼å¼åŒ–å°è©±
        try:
            prompt = self.tokenizer.apply_chat_template(
                formatted_messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception:
            # å¦‚æœ apply_chat_template å¤±æ•—ï¼Œä½¿ç”¨æ‰‹å‹•æ ¼å¼
            prompt_parts = []
            for msg in formatted_messages:
                role = msg["role"]
                content = msg["content"]
                if role == "system":
                    prompt_parts.append(f"<|im_start|>system\n{content}<|im_end|>")
                elif role == "user":
                    prompt_parts.append(f"<|im_start|>user\n{content}<|im_end|>")
                elif role == "assistant":
                    prompt_parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
            prompt_parts.append("<|im_start|>assistant\n")
            prompt = "\n".join(prompt_parts)
        
        # ä½¿ç”¨ MLX çš„ generate å‡½æ•¸ä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆæ›´å¿«ï¼‰
        # æ³¨æ„ï¼šMLX çš„ generate ä¸æ”¯æ´ temperature åƒæ•¸ï¼Œä½†é€Ÿåº¦æ›´å¿«
        try:
            response_text = mlx_generate(
                self.model,
                self.tokenizer,
                prompt=prompt,
                max_tokens=self.max_tokens,
                verbose=False
            )
        except Exception as e:
            # å¦‚æœ generate å¤±æ•—ï¼Œå›é€€åˆ°é€å€‹ token ç”Ÿæˆ
            print(f"   âš ï¸ MLX generate å¤±æ•—ï¼Œä½¿ç”¨é€å€‹ token ç”Ÿæˆ: {e}")
            tokens = self.tokenizer.encode(prompt)
            tokens = mx.array(tokens)
            
            generated_tokens = []
            for _ in range(self.max_tokens):
                # å‰å‘å‚³æ’­
                logits = self.model(tokens[None, :])
                logits = logits[0, -1, :]
                
                # ä½¿ç”¨è²ªå©ªè§£ç¢¼ï¼ˆæœ€å¿«ï¼‰
                next_token = mx.argmax(logits)
                next_token = int(next_token.item())
                
                # æª¢æŸ¥çµæŸç¬¦
                if next_token == self.tokenizer.eos_token_id:
                    break
                
                generated_tokens.append(next_token)
                tokens = mx.concatenate([tokens, mx.array([next_token])])
            
            # è§£ç¢¼å›ç­”
            response_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # å‰µå»º ChatResult
        message = AIMessage(content=response_text)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])
    
    def bind_tools(self, tools: List[Any], **kwargs: Any):
        """
        ç¶å®šå·¥å…·ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        æ³¨æ„ï¼šMLX æ¨¡å‹å¯èƒ½ä¸ç›´æ¥æ”¯æ´å·¥å…·èª¿ç”¨ï¼Œé€™è£¡è¿”å›è‡ªèº«
        å¦‚æœéœ€è¦å·¥å…·èª¿ç”¨ï¼Œå¯èƒ½éœ€è¦é¡å¤–çš„å¾Œè™•ç†
        """
        # å°‡å·¥å…·ä¿¡æ¯æ·»åŠ åˆ°ç³»çµ±æç¤ºä¸­
        self._tools = tools
        return self

# å…¨åŸŸ MLX æ¨¡å‹è®Šæ•¸ï¼ˆå»¶é²è¼‰å…¥ï¼‰
_mlx_model = None
_mlx_tokenizer = None

def load_mlx_model():
    """è¼‰å…¥ MLX æ¨¡å‹ï¼ˆåªè¼‰å…¥ä¸€æ¬¡ï¼‰"""
    global _mlx_model, _mlx_tokenizer
    
    if _mlx_model is None or _mlx_tokenizer is None:
        model_id = "mlx-community/Qwen2.5-Coder-7B-Instruct-4bit"
        print(f"ğŸ“¦ æ­£åœ¨è¼‰å…¥ MLX æ¨¡å‹ {model_id}...")
        _mlx_model, _mlx_tokenizer = load(model_id)
        print("âœ… MLX æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    return _mlx_model, _mlx_tokenizer

# ==========================================
# 0.5. RAG ç³»çµ±åˆå§‹åŒ–ï¼ˆåœ¨å·¥å…·å®šç¾©ä¹‹å‰ï¼‰
# ==========================================
def get_device():
    """è‡ªå‹•æª¢æ¸¬å¯ç”¨çš„è¨­å‚™ï¼ˆå„ªå…ˆä½¿ç”¨ Apple Silicon GPUï¼‰"""
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def init_rag_system():
    """åˆå§‹åŒ– RAG ç³»çµ±ï¼ˆPDF å‘é‡è³‡æ–™åº«ï¼‰"""
    pdf_path = "./data/Tree_of_Thoughts.pdf"
    retriever = None
    
    if not os.path.exists(pdf_path):
        print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° {pdf_path}ï¼ŒRAG åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")
        return retriever
    
    print("ğŸš€ [RAG] æ­£åœ¨åˆå§‹åŒ– PDF å‘é‡è³‡æ–™åº«ï¼ˆä½¿ç”¨ Jina Embeddings v3ï¼‰...")
    
    try:
        # è¼‰å…¥ PDF
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        print(f"   âœ“ PDF è¼‰å…¥å®Œæˆï¼Œå…± {len(splits)} å€‹æ–‡å­—å¡Š")
        
        # åˆå§‹åŒ– Jina Embeddings
        device = get_device()
        device_name = "Apple Silicon GPU (MPS)" if device == "mps" else ("NVIDIA GPU (CUDA)" if device == "cuda" else "CPU")
        print(f"   ğŸ“¦ æ­£åœ¨è¼‰å…¥ Jina Embeddings æ¨¡å‹ï¼ˆä½¿ç”¨è¨­å‚™ï¼š{device_name}ï¼‰...")
        
        # è¨­å®šç·©å­˜ç›®éŒ„
        cache_folder = None
        if os.path.exists(EXTERNAL_SSD_PATH):
            cache_folder = os.path.join(HF_CACHE_DIR, "transformers")
            os.makedirs(cache_folder, exist_ok=True)
        
        # æº–å‚™ model_kwargs
        model_kwargs = {
            "device": device,
            "trust_remote_code": True
        }
        
        # å»ºç«‹ embeddings
        embeddings_kwargs = {
            "model_name": "jinaai/jina-embeddings-v3",
            "model_kwargs": model_kwargs,
            "encode_kwargs": {
                "normalize_embeddings": True,
                "batch_size": 4,
            },
            "show_progress": True
        }
        
        if cache_folder:
            embeddings_kwargs["cache_folder"] = cache_folder
        
        # å˜—è©¦è¼‰å…¥æ¨¡å‹
        try:
            embeddings = HuggingFaceEmbeddings(**embeddings_kwargs)
            print("   âœ… Jina Embeddings è¼‰å…¥å®Œæˆ")
        except (FileNotFoundError, OSError, Exception) as e:
            error_msg = str(e)
            if "No such file or directory" in error_msg or "cache" in error_msg.lower() or "transformers_modules" in error_msg:
                print("   âš ï¸ æª¢æ¸¬åˆ°æ¨¡å‹ç·©å­˜ä¸å®Œæ•´ï¼Œæ­£åœ¨æ¸…ç†ä¸¦é‡æ–°ä¸‹è¼‰...")
                cache_paths_to_clean = [
                    os.path.join(HF_CACHE_DIR, "modules", "transformers_modules", "jinaai"),
                    os.path.join(HF_CACHE_DIR, "modules", "transformers_modules", "jinaai", "jina_hyphen_embeddings_hyphen_v3"),
                ]
                
                for cache_path in cache_paths_to_clean:
                    if os.path.exists(cache_path):
                        try:
                            shutil.rmtree(cache_path)
                        except Exception:
                            pass
                
                print("   æ­£åœ¨é‡æ–°ä¸‹è¼‰æ¨¡å‹ï¼ˆé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰...")
                embeddings = HuggingFaceEmbeddings(**embeddings_kwargs)
                print("   âœ… Jina Embeddings è¼‰å…¥å®Œæˆï¼ˆå·²é‡æ–°ä¸‹è¼‰ï¼‰")
            else:
                print(f"   âŒ è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error_msg}")
                return None
        
        # å»ºç«‹å‘é‡è³‡æ–™åº«
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        print("   âœ… RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        print(f"   âŒ RAG ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        return None
    
    return retriever

# åˆå§‹åŒ– RAG ç³»çµ±
rag_retriever = init_rag_system()

# ==========================================
# 1. å®šç¾© Deep Agent ç‹€æ…‹ (æ ¸å¿ƒå‡ç´š)
# ==========================================
class DeepAgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    tasks: List[str]            # å¾…åŸ·è¡Œçš„å­ä»»å‹™æ¸…å–®
    completed_tasks: Annotated[List[str], operator.add]  # å·²å®Œæˆçš„ä»»å‹™ï¼ˆä½¿ç”¨ operator.add è¿½åŠ ï¼‰
    research_notes: Annotated[List[str], operator.add]   # å„²å­˜æ¯ä¸€è¼ªæœå°‹åˆ°çš„æ·±åº¦å…§å®¹ï¼ˆä½¿ç”¨ operator.add è¿½åŠ ï¼‰
    iteration: int              # è¿½è¹¤è¿­ä»£æ¬¡æ•¸ï¼Œé˜²æ­¢ç„¡é™å¾ªç’°
    query: str                  # åŸå§‹å•é¡Œ

# ==========================================
# 2. åˆå§‹åŒ–èˆ‡å·¥å…· (åŒ…å« RAG å·¥å…·)
# ==========================================
def get_llm():
    """
    ç²å– LLM å¯¦ä¾‹
    ä½¿ç”¨æœ¬åœ° MLX æ¨¡å‹æ›¿ä»£ Groq API
    """
    # è¼‰å…¥ MLX æ¨¡å‹
    model, tokenizer = load_mlx_model()
    
    # å‰µå»º MLX ChatModel åŒ…è£å™¨
    return MLXChatModel(
        model=model,
        tokenizer=tokenizer,
        max_tokens=512,
        temperature=0.7
    )

@tool
def get_company_deep_info(ticker: str) -> str:
    """æŸ¥è©¢è‚¡ç¥¨çš„è©³ç´°ç‡Ÿé‹ç‹€æ³ï¼ŒåŒ…æ‹¬ç¾åƒ¹ã€å¸‚å€¼ã€æœ¬ç›Šæ¯”ã€ç‡Ÿæ”¶å¢é•·ç­‰æ·±åº¦æ•¸æ“šã€‚"""
    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        summary = (
            f"è‚¡ç¥¨: {info.get('longName')} ({ticker})\n"
            f"ç¾åƒ¹: {info.get('currentPrice')} {info.get('currency')}\n"
            f"å¸‚å€¼: {info.get('marketCap')}\n"
            f"æœ¬ç›Šæ¯” (PE): {info.get('trailingPE')}\n"
            f"ç‡Ÿæ”¶å¢é•·: {info.get('revenueGrowth')}\n"
            f"æ¥­å‹™æ‘˜è¦: {info.get('longBusinessSummary')[:500]}..."
        )
        return summary
    except Exception as e:
        return f"æ•¸æ“šæŸ¥è©¢å¤±æ•—: {e}"

@tool
def search_web(query: str) -> str:
    """æœå°‹ç¶²éš›ç¶²è·¯ä»¥ç²å–æœ€æ–°æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚"""
    try:
        tool = TavilySearchResults(k=5) # å¢åŠ æœå°‹é‡ä»¥ç²å–æ·±åº¦è³‡è¨Š
        return str(tool.invoke(query))
    except Exception as e:
        return f"æœå°‹éŒ¯èª¤: {e}"

@tool
def query_pdf_knowledge(query: str) -> str:
    """
    æŸ¥è©¢ PDF çŸ¥è­˜åº«ï¼ˆTree of Thoughts è«–æ–‡ï¼‰ä¸­çš„ç›¸é—œè³‡è¨Šã€‚
    ç•¶å•é¡Œæ¶‰åŠè«–æ–‡å…§å®¹ã€ç ”ç©¶æ¦‚å¿µã€æ–¹æ³•è«–æˆ–å­¸è¡“ç†è«–æ™‚ä½¿ç”¨æ­¤å·¥å…·ã€‚
    """
    if not rag_retriever:
        return "PDF çŸ¥è­˜åº«æœªè¼‰å…¥ï¼Œç„¡æ³•æŸ¥è©¢ã€‚"
    
    try:
        print(f"   ğŸ” [RAG] æ­£åœ¨æŸ¥è©¢ PDF çŸ¥è­˜åº«: {query}")
        
        # æª¢ç´¢ç›¸é—œæ–‡æª”
        docs = rag_retriever.invoke(query)
        
        if not docs:
            return "åœ¨ PDF çŸ¥è­˜åº«ä¸­æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Šã€‚"
        
        # æ ¼å¼åŒ–æª¢ç´¢çµæœ
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # ä½¿ç”¨ LLM åŸºæ–¼æª¢ç´¢åˆ°çš„å…§å®¹å›ç­”å•é¡Œ
        llm_rag = get_llm()
        prompt = ChatPromptTemplate.from_template(
            "è«‹æ ¹æ“šä»¥ä¸‹å¾ PDF çŸ¥è­˜åº«ä¸­æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µï¼Œå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚\n\n"
            "ä¸Šä¸‹æ–‡ï¼š\n{context}\n\n"
            "å•é¡Œï¼š{question}\n\n"
            "è«‹åŸºæ–¼ä¸Šä¸‹æ–‡å›ç­”ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚å›ç­”è«‹ä¿æŒç°¡æ½”ä¸”æº–ç¢ºã€‚"
        )
        chain = (
            {"context": lambda x: context, "question": RunnablePassthrough()}
            | prompt
            | llm_rag
            | StrOutputParser()
        )
        result = chain.invoke(query)
        return result
    except Exception as e:
        return f"PDF çŸ¥è­˜åº«æŸ¥è©¢å¤±æ•—: {e}"

# å·¥å…·åˆ—è¡¨ï¼ˆåŒ…å« RAG å·¥å…·ï¼‰
tools_list = [get_company_deep_info, search_web, query_pdf_knowledge]
llm = get_llm()
llm_with_tools = llm.bind_tools(tools_list)

# ==========================================
# 3. Deep Agent ç¯€é»é‚è¼¯
# ==========================================

def planner_node(state: DeepAgentState):
    """
    è¦åŠƒç¯€é»ï¼šå°‡è¤‡é›œå•é¡Œæ‹†è§£ç‚ºå…·é«”çš„ç ”ç©¶è¨ˆç•«
    
    ã€é‡è¦æ”¹é€²ã€‘æ ¹æ“šå•é¡Œé¡å‹å‹•æ…‹ç”Ÿæˆä»»å‹™ï¼Œé¿å…ç„¡é—œå·¥å…·èª¿ç”¨
    - å­¸è¡“ç†è«–å•é¡Œ â†’ å°ˆæ³¨ PDF çŸ¥è­˜åº«å’Œç¶²è·¯æœå°‹
    - è‚¡ç¥¨ç›¸é—œå•é¡Œ â†’ åŒ…å«è‚¡ç¥¨æŸ¥è©¢ã€æ–°èã€PDF çŸ¥è­˜åº«
    - é€šç”¨å•é¡Œ â†’ æ ¹æ“šå•é¡Œå…§å®¹æ™ºèƒ½é¸æ“‡å·¥å…·
    """
    try:
        query = state["query"]
        query_lower = query.lower()
        
        # ã€é—œéµæ”¹é€²é» 1ã€‘å•é¡Œé¡å‹æª¢æ¸¬ï¼šåˆ†æå•é¡Œæ˜¯å¦èˆ‡è‚¡ç¥¨æˆ–å­¸è¡“ç›¸é—œ
        # æª¢æ¸¬è‚¡ç¥¨ç›¸é—œé—œéµå­—
        stock_keywords = [
            'è‚¡ç¥¨', 'ticker', 'å…¬å¸', 'ç‡Ÿé‹', 'è²¡å ±', 'æŠ•è³‡', 'è‚¡åƒ¹', 'å¸‚å€¼',
            'msft', 'googl', 'aapl', 'tsla', 'nvda', 'amzn', 'meta', 'nflx'  # å¸¸è¦‹è‚¡ç¥¨ä»£ç¢¼
        ]
        is_stock_related = any(keyword in query_lower for keyword in stock_keywords)
        
        # æª¢æ¸¬å­¸è¡“ç†è«–ç›¸é—œé—œéµå­—
        academic_keywords = [
            'è«–æ–‡', 'ç†è«–', 'æ–¹æ³•', 'ç ”ç©¶', 'å­¸è¡“', 'tree of thoughts', 
            'chain of thought', 'cot', 'tot', 'methodology', 'framework',
            'æ¦‚å¿µ', 'æ¯”è¼ƒ', 'å·®ç•°', 'åˆ†æ', 'approach'
        ]
        is_academic_related = any(keyword in query_lower for keyword in academic_keywords)
        
        # ã€é—œéµæ”¹é€²é» 2ã€‘æ ¹æ“šå•é¡Œé¡å‹å‹•æ…‹ç”Ÿæˆæç¤ºè©
        if is_academic_related and not is_stock_related:
            # ç´”å­¸è¡“ç†è«–å•é¡Œï¼šå°ˆæ³¨æ–¼ PDF çŸ¥è­˜åº«å’Œå­¸è¡“æœå°‹
            prompt_template = (
                "ä½ æ˜¯ä¸€å€‹è³‡æ·±ç ”ç©¶è¦åŠƒå“¡ã€‚è«‹é‡å°ç”¨æˆ¶çš„å•é¡Œï¼š'{query}'\n"
                "æ‹†è§£å‡º 3-5 å€‹å…·é«”çš„ç ”ç©¶æ­¥é©Ÿã€‚\n\n"
                "ã€é‡è¦ã€‘é€™æ˜¯ä¸€å€‹å­¸è¡“ç†è«–å•é¡Œï¼Œè«‹å°ˆæ³¨æ–¼ï¼š\n"
                "1. æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç†è«–ã€æ–¹æ³•å’Œæ¦‚å¿µ\n"
                "2. æœå°‹ç¶²è·¯ä¸Šç›¸é—œçš„å­¸è¡“è³‡æ–™ã€è«–æ–‡å’Œæœ€æ–°ç ”ç©¶\n"
                "3. æ¯”è¼ƒå’Œåˆ†æä¸åŒæ¦‚å¿µæˆ–æ–¹æ³•çš„å·®ç•°\n"
                "4. ç¸½çµç†è«–è¦é»ã€å„ªç¼ºé»å’Œæ‡‰ç”¨å ´æ™¯\n\n"
                "ã€è«‹å‹¿ä½¿ç”¨ã€‘è‚¡ç¥¨æŸ¥è©¢å·¥å…·ï¼Œå› ç‚ºå•é¡Œèˆ‡è‚¡ç¥¨ç„¡é—œã€‚\n\n"
                "è«‹åªè¼¸å‡ºæ¸…å–®ï¼Œæ¯è¡Œä¸€å€‹ä»»å‹™ï¼Œæ ¼å¼ç‚ºï¼šæ•¸å­—. ä»»å‹™æè¿°"
            )
        elif is_stock_related:
            # è‚¡ç¥¨ç›¸é—œå•é¡Œï¼šåŒ…å«è‚¡ç¥¨æŸ¥è©¢ã€æ–°èã€PDF çŸ¥è­˜åº«ï¼ˆå¦‚æœæ¶‰åŠç†è«–ï¼‰
            prompt_template = (
                "ä½ æ˜¯ä¸€å€‹è³‡æ·±ç ”ç©¶è¦åŠƒå“¡ã€‚è«‹é‡å°ç”¨æˆ¶çš„å•é¡Œï¼š'{query}'\n"
                "æ‹†è§£å‡º 3-5 å€‹å…·é«”çš„ç ”ç©¶æ­¥é©Ÿï¼Œä¾‹å¦‚ï¼š\n"
                "1. æŸ¥è©¢åŸºç¤è²¡å ±æ•¸æ“šå’Œç‡Ÿé‹ç‹€æ³\n"
                "2. æœå°‹è¿‘æœŸé‡å¤§æ–°èå’Œå¸‚å ´å‹•æ…‹\n"
                "3. æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç†è«–æˆ–æ–¹æ³•ï¼ˆå¦‚é©ç”¨ï¼‰\n"
                "4. åˆ†æç”¢æ¥­ç«¶çˆ­åŠ›å’Œæœªä¾†å‰æ™¯\n"
                "è«‹åªè¼¸å‡ºæ¸…å–®ï¼Œæ¯è¡Œä¸€å€‹ä»»å‹™ï¼Œæ ¼å¼ç‚ºï¼šæ•¸å­—. ä»»å‹™æè¿°"
            )
        else:
            # é€šç”¨å•é¡Œï¼šæ ¹æ“šå•é¡Œå…§å®¹æ™ºèƒ½é¸æ“‡å·¥å…·
            prompt_template = (
                "ä½ æ˜¯ä¸€å€‹è³‡æ·±ç ”ç©¶è¦åŠƒå“¡ã€‚è«‹é‡å°ç”¨æˆ¶çš„å•é¡Œï¼š'{query}'\n"
                "æ‹†è§£å‡º 3-5 å€‹å…·é«”çš„ç ”ç©¶æ­¥é©Ÿã€‚\n\n"
                "å¯ç”¨çš„ç ”ç©¶æ–¹å¼åŒ…æ‹¬ï¼š\n"
                "- æŸ¥è©¢ PDF çŸ¥è­˜åº«ï¼ˆå¦‚æœå•é¡Œæ¶‰åŠå­¸è¡“ç†è«–ã€è«–æ–‡å…§å®¹æˆ–ç ”ç©¶æ–¹æ³•ï¼‰\n"
                "- æœå°‹ç¶²è·¯ï¼ˆç²å–æœ€æ–°è³‡è¨Šã€æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ï¼‰\n"
                "- æŸ¥è©¢è‚¡ç¥¨è³‡è¨Šï¼ˆåƒ…ç•¶å•é¡Œæ˜ç¢ºæ¶‰åŠè‚¡ç¥¨ä»£ç¢¼ã€å…¬å¸åç¨±æˆ–è²¡å‹™æ•¸æ“šæ™‚ï¼‰\n\n"
                "ã€é‡è¦ã€‘è«‹æ ¹æ“šå•é¡Œçš„å¯¦éš›éœ€æ±‚ï¼Œé¸æ“‡åˆé©çš„ç ”ç©¶æ–¹å¼ã€‚\n"
                "å¦‚æœå•é¡Œèˆ‡è‚¡ç¥¨ç„¡é—œï¼Œè«‹ä¸è¦åŒ…å«è‚¡ç¥¨æŸ¥è©¢ä»»å‹™ã€‚\n\n"
                "è«‹åªè¼¸å‡ºæ¸…å–®ï¼Œæ¯è¡Œä¸€å€‹ä»»å‹™ï¼Œæ ¼å¼ç‚ºï¼šæ•¸å­—. ä»»å‹™æè¿°"
            )
        
        prompt = ChatPromptTemplate.from_template(prompt_template)
        chain = prompt | llm | StrOutputParser()
        result = chain.invoke({"query": query})
        
        # æ›´å¥å£¯çš„ä»»å‹™è§£æï¼šæå–æ•¸å­—é–‹é ­æˆ–åˆ—è¡¨é …
        tasks = []
        for line in result.split("\n"):
            line = line.strip()
            if not line:
                continue
            # ç§»é™¤ç·¨è™Ÿï¼ˆå¦‚ "1. " æˆ– "- "ï¼‰
            cleaned = re.sub(r'^[\d\-â€¢]\s*\.?\s*', '', line)
            if cleaned:
                tasks.append(cleaned)
        
        # ã€é—œéµæ”¹é€²é» 3ã€‘æ ¹æ“šå•é¡Œé¡å‹ç”Ÿæˆå‚™ç”¨ä»»å‹™ï¼ˆé¿å…ç¡¬ç·¨ç¢¼è‚¡ç¥¨ä»»å‹™ï¼‰
        if not tasks:
            if is_academic_related and not is_stock_related:
                tasks = [
                    "æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç†è«–å’Œæ–¹æ³•",
                    "æœå°‹ç¶²è·¯ä¸Šç›¸é—œçš„å­¸è¡“è³‡æ–™å’Œè«–æ–‡",
                    "æ¯”è¼ƒå’Œåˆ†æä¸åŒæ¦‚å¿µæˆ–æ–¹æ³•çš„å·®ç•°",
                    "ç¸½çµç†è«–è¦é»ã€å„ªç¼ºé»å’Œæ‡‰ç”¨å ´æ™¯"
                ]
            elif is_stock_related:
                tasks = [
                    "æŸ¥è©¢åŸºç¤è²¡å‹™æ•¸æ“šå’Œç‡Ÿé‹ç‹€æ³",
                    "æœå°‹è¿‘æœŸé‡å¤§æ–°èå’Œå¸‚å ´å‹•æ…‹",
                    "æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç†è«–ï¼ˆå¦‚é©ç”¨ï¼‰",
                    "åˆ†æç”¢æ¥­ç«¶çˆ­åŠ›å’Œæœªä¾†å‰æ™¯"
                ]
            else:
                # é€šç”¨å•é¡Œçš„é è¨­ä»»å‹™
                tasks = [
                    "æœå°‹ç¶²è·¯ä¸Šç›¸é—œè³‡è¨Š",
                    "æŸ¥è©¢ PDF çŸ¥è­˜åº«ï¼ˆå¦‚é©ç”¨ï¼‰",
                    "æ•´ç†å’Œåˆ†ææ”¶é›†åˆ°çš„è³‡è¨Š"
                ]
        
        print(f"   ğŸ“ [Planner] å•é¡Œé¡å‹æª¢æ¸¬ï¼šå­¸è¡“={is_academic_related}, è‚¡ç¥¨={is_stock_related}")
        print(f"   ğŸ“ [Planner] ç”Ÿæˆè¨ˆç•«: {tasks}")
        return {
            "tasks": tasks, 
            "completed_tasks": [], 
            "research_notes": [],
            "iteration": 0
        }
    except Exception as e:
        print(f"   âš ï¸ [Planner] è¦åŠƒå¤±æ•—: {e}ï¼Œä½¿ç”¨é è¨­è¨ˆç•«")
        # ã€é—œéµæ”¹é€²é» 4ã€‘ç•°å¸¸è™•ç†æ™‚ä¹Ÿæ ¹æ“šå•é¡Œé¡å‹é¸æ“‡é è¨­ä»»å‹™
        query = state.get("query", "")
        query_lower = query.lower()
        is_stock_related = any(keyword in query_lower for keyword in [
            'è‚¡ç¥¨', 'ticker', 'å…¬å¸', 'ç‡Ÿé‹', 'è²¡å ±'
        ])
        
        if is_stock_related:
            default_tasks = [
                "æŸ¥è©¢åŸºç¤è²¡å‹™æ•¸æ“šå’Œç‡Ÿé‹ç‹€æ³",
                "æœå°‹è¿‘æœŸé‡å¤§æ–°èå’Œå¸‚å ´å‹•æ…‹",
                "æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç†è«–ï¼ˆå¦‚é©ç”¨ï¼‰",
                "åˆ†æç”¢æ¥­ç«¶çˆ­åŠ›å’Œæœªä¾†å‰æ™¯"
            ]
        else:
            # éè‚¡ç¥¨å•é¡Œçš„é è¨­ä»»å‹™
            default_tasks = [
                "æŸ¥è©¢ PDF çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç†è«–å’Œæ–¹æ³•",
                "æœå°‹ç¶²è·¯ä¸Šç›¸é—œçš„å­¸è¡“è³‡æ–™",
                "æ•´ç†å’Œåˆ†ææ”¶é›†åˆ°çš„è³‡è¨Š"
            ]
        
        return {
            "tasks": default_tasks,
            "completed_tasks": [],
            "research_notes": [],
            "iteration": 0
        }

def research_agent_node(state: DeepAgentState):
    """
    åŸ·è¡Œç¯€é»ï¼šæ ¹æ“šç›®å‰çš„ä»»å‹™æ¸…å–®ï¼Œä½¿ç”¨å·¥å…·é€²è¡Œæ·±åº¦ç ”ç©¶
    
    ã€é‡è¦æ”¹é€²ã€‘æ ¹æ“šä»»å‹™å…§å®¹æ™ºèƒ½æŒ‡å°å·¥å…·é¸æ“‡ï¼Œé¿å…èª¿ç”¨ç„¡é—œå·¥å…·
    """
    # æª¢æŸ¥è¿­ä»£æ¬¡æ•¸ï¼Œé˜²æ­¢ç„¡é™å¾ªç’°
    max_iterations = 5
    current_iteration = state.get("iteration", 0)
    if current_iteration >= max_iterations:
        return {"messages": [AIMessage(content="å·²é”æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œåœæ­¢ç ”ç©¶ã€‚")]}
    
    current_task_idx = len(state.get("completed_tasks", []))
    tasks = state.get("tasks", [])
    
    if current_task_idx >= len(tasks):
        return {"messages": [AIMessage(content="æ‰€æœ‰ç ”ç©¶ä»»å‹™å·²å®Œæˆã€‚")]}
    
    current_task = tasks[current_task_idx]
    print(f"   ğŸ•µï¸ [Researcher] æ­£åœ¨åŸ·è¡Œä»»å‹™ {current_task_idx + 1}/{len(tasks)}: {current_task}")
    
    try:
        # ã€é—œéµæ”¹é€²é» 5ã€‘æ ¹æ“šä»»å‹™å…§å®¹åˆ¤æ–·æ‡‰è©²ä½¿ç”¨å“ªäº›å·¥å…·ï¼Œæä¾›æ˜ç¢ºæŒ‡å°
        task_lower = current_task.lower()
        tool_guidance = ""
        
        # æª¢æ¸¬ä»»å‹™é¡å‹ä¸¦æä¾›å°æ‡‰çš„å·¥å…·ä½¿ç”¨å»ºè­°
        if any(keyword in task_lower for keyword in ["pdf", "çŸ¥è­˜åº«", "ç†è«–", "è«–æ–‡", "å­¸è¡“", "æ–¹æ³•"]):
            tool_guidance = (
                "\nã€å·¥å…·é¸æ“‡æŒ‡å°ã€‘æ­¤ä»»å‹™æ‡‰å„ªå…ˆä½¿ç”¨ PDF çŸ¥è­˜åº«æŸ¥è©¢å·¥å…·ï¼ˆquery_pdf_knowledgeï¼‰ã€‚"
                "\nå¦‚æœä»»å‹™æ¶‰åŠå­¸è¡“ç†è«–ã€è«–æ–‡å…§å®¹æˆ–ç ”ç©¶æ–¹æ³•ï¼Œè«‹ä½¿ç”¨ query_pdf_knowledgeã€‚"
                "\nè«‹å‹¿ä½¿ç”¨è‚¡ç¥¨æŸ¥è©¢å·¥å…·ï¼ˆget_company_deep_infoï¼‰ï¼Œé™¤éä»»å‹™æ˜ç¢ºè¦æ±‚ã€‚"
            )
        elif any(keyword in task_lower for keyword in ["è‚¡ç¥¨", "è²¡å ±", "ç‡Ÿé‹", "å…¬å¸", "æŠ•è³‡", "è‚¡åƒ¹", "å¸‚å€¼"]):
            tool_guidance = (
                "\nã€å·¥å…·é¸æ“‡æŒ‡å°ã€‘æ­¤ä»»å‹™æ‡‰ä½¿ç”¨è‚¡ç¥¨è³‡è¨ŠæŸ¥è©¢å·¥å…·ï¼ˆget_company_deep_infoï¼‰ã€‚"
                "\nè«‹å¾ä»»å‹™æè¿°ä¸­æå–è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚ MSFT, GOOGLï¼‰ï¼Œç„¶å¾Œä½¿ç”¨ get_company_deep_info æŸ¥è©¢ã€‚"
            )
        elif any(keyword in task_lower for keyword in ["æœå°‹", "ç¶²è·¯", "æ–°è", "å‹•æ…‹", "è³‡è¨Š", "è³‡æ–™"]):
            tool_guidance = (
                "\nã€å·¥å…·é¸æ“‡æŒ‡å°ã€‘æ­¤ä»»å‹™æ‡‰ä½¿ç”¨ç¶²è·¯æœå°‹å·¥å…·ï¼ˆsearch_webï¼‰ã€‚"
                "\nè«‹ä½¿ç”¨ search_web ç²å–æœ€æ–°çš„ç¶²è·¯è³‡è¨Šã€æ–°èæˆ–ä¸€èˆ¬çŸ¥è­˜ã€‚"
            )
        else:
            # é€šç”¨æŒ‡å°ï¼šæ ¹æ“šä»»å‹™å…§å®¹é¸æ“‡åˆé©çš„å·¥å…·
            tool_guidance = (
                "\nã€å·¥å…·é¸æ“‡æŒ‡å°ã€‘è«‹æ ¹æ“šä»»å‹™å…§å®¹é¸æ“‡æœ€åˆé©çš„å·¥å…·ï¼š"
                "\n- å¦‚æœä»»å‹™æ¶‰åŠå­¸è¡“ç†è«–ã€è«–æ–‡æˆ– PDF å…§å®¹ â†’ ä½¿ç”¨ query_pdf_knowledge"
                "\n- å¦‚æœä»»å‹™æ¶‰åŠè‚¡ç¥¨ã€å…¬å¸è²¡å‹™ â†’ ä½¿ç”¨ get_company_deep_info"
                "\n- å¦‚æœä»»å‹™éœ€è¦æœ€æ–°è³‡è¨Šã€æ–°è â†’ ä½¿ç”¨ search_web"
                "\nè«‹åªä½¿ç”¨èˆ‡ä»»å‹™ç›¸é—œçš„å·¥å…·ï¼Œä¸è¦ä½¿ç”¨ä¸ç›¸é—œçš„å·¥å…·ã€‚"
            )
        
        # ã€é—œéµæ”¹é€²é» 6ã€‘æ§‹å»ºæ›´æ™ºèƒ½çš„ç³»çµ±æç¤ºï¼Œæ˜ç¢ºå·¥å…·ä½¿ç”¨è¦å‰‡
        system_msg = SystemMessage(content=(
            f"ä½ æ˜¯ä¸€ä½æ·±åº¦ç ”ç©¶å“¡ã€‚ç•¶å‰ç›®æ¨™ä»»å‹™æ˜¯ï¼š{current_task}\n"
            f"{tool_guidance}\n"
            f"\nå¯ç”¨çš„å·¥å…·è©³ç´°èªªæ˜ï¼š\n"
            f"- query_pdf_knowledge(query: str): æŸ¥è©¢ PDF çŸ¥è­˜åº«ï¼Œç”¨æ–¼å­¸è¡“ç†è«–ã€è«–æ–‡å…§å®¹ã€ç ”ç©¶æ–¹æ³•ç­‰\n"
            f"- search_web(query: str): ç¶²è·¯æœå°‹ï¼Œç”¨æ–¼ç²å–æœ€æ–°è³‡è¨Šã€æ–°èã€ä¸€èˆ¬çŸ¥è­˜ç­‰\n"
            f"- get_company_deep_info(ticker: str): è‚¡ç¥¨è³‡è¨ŠæŸ¥è©¢ï¼Œåƒ…ç”¨æ–¼æŸ¥è©¢è‚¡ç¥¨ä»£ç¢¼å°æ‡‰çš„å…¬å¸è²¡å‹™æ•¸æ“š\n"
            f"\nã€é‡è¦åŸå‰‡ã€‘"
            f"\n1. è«‹æ ¹æ“šä»»å‹™å…§å®¹é¸æ“‡æœ€åˆé©çš„å·¥å…·"
            f"\n2. å¦‚æœä»»å‹™èˆ‡è‚¡ç¥¨ç„¡é—œï¼Œè«‹å‹¿ä½¿ç”¨ get_company_deep_info"
            f"\n3. å¦‚æœä»»å‹™æ¶‰åŠå­¸è¡“ç†è«–ï¼Œè«‹å„ªå…ˆä½¿ç”¨ query_pdf_knowledge"
            f"\n4. ä½ å¯ä»¥é€²è¡Œå¤šè¼ªå·¥å…·èª¿ç”¨ä¾†æ·±å…¥æŒ–æ˜è³‡è¨Š"
            f"\n5. ç•¶ä½ èªç‚ºè³‡è¨Šå·²ç¶“è¶³å¤ æ™‚ï¼Œè«‹ç¸½çµä½ çš„ç™¼ç¾ä¸¦å›è¦†"
        ))
        
        # æ§‹å»ºä¸Šä¸‹æ–‡ï¼šåŒ…å«åŸå§‹å•é¡Œã€å·²å®Œæˆä»»å‹™å’Œç ”ç©¶ç­†è¨˜
        context_messages = [system_msg]
        
        # å¦‚æœæœ‰ç ”ç©¶ç­†è¨˜ï¼ŒåŠ å…¥ä¸Šä¸‹æ–‡
        if state.get("research_notes"):
            notes_summary = "\n".join(state["research_notes"][-3:])  # åªå–æœ€è¿‘3æ¢ç­†è¨˜
            context_messages.append(SystemMessage(
                content=f"å…ˆå‰çš„ç ”ç©¶ç™¼ç¾ï¼š\n{notes_summary}"
            ))
        
        # åŠ å…¥åŸå§‹å•é¡Œï¼Œå¹«åŠ© LLM ç†è§£æ•´é«”ç›®æ¨™
        original_query = state.get("query", "")
        if original_query:
            context_messages.append(SystemMessage(
                content=f"ç”¨æˆ¶çš„åŸå§‹å•é¡Œï¼š{original_query}"
            ))
        
        # åŠ å…¥æ­·å²æ¶ˆæ¯
        context_messages.extend(state["messages"][-10:])  # åªä¿ç•™æœ€è¿‘10æ¢æ¶ˆæ¯é¿å…ä¸Šä¸‹æ–‡éé•·
        
        response = llm_with_tools.invoke(context_messages)
        return {
            "messages": [response],
            "iteration": current_iteration + 1
        }
    except Exception as e:
        print(f"   âš ï¸ [Researcher] ç ”ç©¶å¤±æ•—: {e}")
        error_msg = AIMessage(content=f"ç ”ç©¶éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return {
            "messages": [error_msg],
            "iteration": current_iteration + 1
        }

def note_taking_node(state: DeepAgentState):
    """ç´€éŒ„ç¯€é»ï¼šå°‡ç ”ç©¶çµæœè½‰åŒ–ç‚ºç­†è¨˜ï¼Œå­˜å…¥ research_notes ç·©å­˜"""
    try:
        last_msg = state["messages"][-1]
        completed_count = len(state.get("completed_tasks", []))
        tasks = state.get("tasks", [])
        
        if completed_count >= len(tasks):
            return {}
        
        current_task = tasks[completed_count]
        
        # ä½¿ç”¨ LLM æ‘˜è¦ç ”ç©¶çµæœï¼Œæå–é—œéµè³‡è¨Š
        try:
            summary_prompt = ChatPromptTemplate.from_template(
                "è«‹å°‡ä»¥ä¸‹ç ”ç©¶çµæœæ‘˜è¦ç‚º3-5å€‹é—œéµè¦é»ï¼š\n\n{content}\n\n"
                "è«‹ä»¥ç°¡æ½”çš„æ¢åˆ—å¼å‘ˆç¾ã€‚"
            )
            chain = summary_prompt | llm | StrOutputParser()
            summary = chain.invoke({"content": last_msg.content})
        except:
            # å¦‚æœæ‘˜è¦å¤±æ•—ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å…§å®¹
            summary = last_msg.content[:500] + "..." if len(last_msg.content) > 500 else last_msg.content
        
        note = f"ã€ä»»å‹™ {completed_count + 1}: {current_task}ã€‘\n{summary}\n"
        print(f"   ğŸ“Œ [NoteTaker] å·²ç´€éŒ„ä»»å‹™ {completed_count + 1} çš„ç ”ç©¶ç­†è¨˜ã€‚")
        
        # æ³¨æ„ï¼šç”±æ–¼ä½¿ç”¨äº† operator.addï¼Œé€™è£¡è¿”å›çš„åˆ—è¡¨æœƒè¢«è¿½åŠ åˆ°ç¾æœ‰åˆ—è¡¨
        return {
            "research_notes": [note], 
            "completed_tasks": [current_task]
        }
    except Exception as e:
        print(f"   âš ï¸ [NoteTaker] è¨˜éŒ„å¤±æ•—: {e}")
        return {}

def final_report_node(state: DeepAgentState):
    """
    ç¸½çµç¯€é»ï¼šå°‡æ‰€æœ‰ç ”ç©¶ç­†è¨˜å½™æ•´æˆæœ€çµ‚å ±å‘Š (é€™å°±æ˜¯ Deep Agent çš„æœ€çµ‚ç”¢å‡º)
    
    ã€é‡è¦æ”¹é€²ã€‘æ ¹æ“šå•é¡Œé¡å‹å‹•æ…‹èª¿æ•´å ±å‘Šçµæ§‹ï¼Œé¿å…è¦æ±‚ä¸ç›¸é—œçš„å…§å®¹
    """
    try:
        research_notes = state.get("research_notes", [])
        if not research_notes:
            return {"messages": [AIMessage(content="æœªæ”¶é›†åˆ°è¶³å¤ çš„ç ”ç©¶è³‡æ–™ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Šã€‚")]}
        
        all_notes = "\n\n".join(research_notes)
        completed_tasks = state.get("completed_tasks", [])
        query = state.get("query", "")
        query_lower = query.lower()
        
        # ã€é—œéµæ”¹é€²é» 7ã€‘æ ¹æ“šå•é¡Œé¡å‹å‹•æ…‹ç”Ÿæˆå ±å‘Šæ¨¡æ¿
        # æª¢æ¸¬å•é¡Œé¡å‹
        is_stock_related = any(keyword in query_lower for keyword in [
            'è‚¡ç¥¨', 'ticker', 'å…¬å¸', 'ç‡Ÿé‹', 'è²¡å ±', 'æŠ•è³‡', 'è‚¡åƒ¹'
        ])
        is_academic_related = any(keyword in query_lower for keyword in [
            'è«–æ–‡', 'ç†è«–', 'æ–¹æ³•', 'ç ”ç©¶', 'å­¸è¡“', 'tree of thoughts', 'chain of thought'
        ])
        
        # æ ¹æ“šå•é¡Œé¡å‹é¸æ“‡å ±å‘Šçµæ§‹
        if is_academic_related and not is_stock_related:
            # å­¸è¡“ç†è«–å•é¡Œçš„å ±å‘Šçµæ§‹
            report_structure = (
                "è«‹æ’°å¯«ä¸€ä»½å°ˆæ¥­çš„å­¸è¡“åˆ†æå ±å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š\n"
                "1. åŸ·è¡Œæ‘˜è¦ï¼ˆExecutive Summaryï¼‰- æ¦‚è¿°ä¸»è¦ç™¼ç¾å’Œçµè«–\n"
                "2. ç†è«–åŸºç¤èˆ‡æ¦‚å¿µèªªæ˜ - è©³ç´°è§£é‡‹ç›¸é—œç†è«–å’Œæ–¹æ³•\n"
                "3. æ¯”è¼ƒåˆ†æ - æ·±å…¥æ¯”è¼ƒä¸åŒæ¦‚å¿µæˆ–æ–¹æ³•çš„å·®ç•°\n"
                "4. å­¸è¡“åƒè€ƒèˆ‡æ–‡ç» - å¼•ç”¨ PDF çŸ¥è­˜åº«å’Œç¶²è·¯æœå°‹åˆ°çš„ç›¸é—œè³‡æ–™\n"
                "5. å„ªç¼ºé»åˆ†æ - è©•ä¼°ä¸åŒæ–¹æ³•çš„å„ªç¼ºé»\n"
                "6. æ‡‰ç”¨å ´æ™¯èˆ‡å¯¦å‹™è€ƒé‡ - èªªæ˜å¯¦éš›æ‡‰ç”¨æƒ…æ³\n"
                "7. çµè«–èˆ‡å»ºè­° - ç¸½çµè¦é»ä¸¦æä¾›å»ºè­°\n\n"
                "ã€é‡è¦ã€‘å¦‚æœç ”ç©¶ç­†è¨˜ä¸­æ²’æœ‰è²¡å‹™æ•¸æ“šæˆ–è‚¡ç¥¨è³‡è¨Šï¼Œè«‹ä¸è¦å¼·è¡ŒåŠ å…¥é€™äº›å…§å®¹ã€‚"
            )
        elif is_stock_related:
            # è‚¡ç¥¨ç›¸é—œå•é¡Œçš„å ±å‘Šçµæ§‹
            report_structure = (
                "è«‹æ’°å¯«ä¸€ä»½å°ˆæ¥­çš„æŠ•è³‡åˆ†æå ±å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š\n"
                "1. åŸ·è¡Œæ‘˜è¦ï¼ˆExecutive Summaryï¼‰\n"
                "2. æ•¸æ“šåˆ†æèˆ‡è²¡å‹™ç‹€æ³\n"
                "3. è¿‘æœŸå‹•æ…‹èˆ‡å¸‚å ´è¡¨ç¾\n"
                "4. ç†è«–åŸºç¤èˆ‡å­¸è¡“åƒè€ƒï¼ˆå¦‚é©ç”¨ï¼‰\n"
                "5. ç”¢æ¥­ç«¶çˆ­åŠ›åˆ†æ\n"
                "6. æŠ•è³‡é¢¨éšªè©•ä¼°\n"
                "7. çµè«–èˆ‡å»ºè­°\n"
            )
        else:
            # é€šç”¨å•é¡Œçš„å ±å‘Šçµæ§‹
            report_structure = (
                "è«‹æ’°å¯«ä¸€ä»½å°ˆæ¥­çš„åˆ†æå ±å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š\n"
                "1. åŸ·è¡Œæ‘˜è¦ï¼ˆExecutive Summaryï¼‰- æ¦‚è¿°ä¸»è¦ç™¼ç¾\n"
                "2. æ ¸å¿ƒå…§å®¹åˆ†æ - æ ¹æ“šç ”ç©¶ç­†è¨˜è©³ç´°åˆ†æå•é¡Œ\n"
                "3. è³‡æ–™ä¾†æºèˆ‡åƒè€ƒ - èªªæ˜ä½¿ç”¨çš„è³‡æ–™ä¾†æºï¼ˆPDF çŸ¥è­˜åº«ã€ç¶²è·¯æœå°‹ç­‰ï¼‰\n"
                "4. æ·±å…¥æ¢è¨ - é€²ä¸€æ­¥åˆ†æç›¸é—œè­°é¡Œ\n"
                "5. çµè«–èˆ‡å»ºè­° - ç¸½çµè¦é»ä¸¦æä¾›å»ºè­°\n\n"
                "ã€é‡è¦ã€‘è«‹æ ¹æ“šå¯¦éš›æ”¶é›†åˆ°çš„è³‡æ–™æ’°å¯«å ±å‘Šï¼Œä¸è¦æ·»åŠ æœªæ”¶é›†åˆ°çš„è³‡è¨Šã€‚"
            )
        
        prompt = ChatPromptTemplate.from_template(
            "ä½ æ˜¯ä¸€ä½å°ˆæ¥­åˆ†æå¸«ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ”¶é›†åˆ°çš„ç ”ç©¶ç­†è¨˜ï¼Œç‚ºç”¨æˆ¶å•é¡Œ '{query}' æ’°å¯«ä¸€ä»½çµæ§‹å®Œæ•´çš„æ·±åº¦å ±å‘Šã€‚\n\n"
            "å·²å®Œæˆçš„ç ”ç©¶ä»»å‹™ï¼š\n{completed_tasks}\n\n"
            "ç ”ç©¶ç­†è¨˜å…§å®¹ï¼š\n{notes}\n\n"
            "{report_structure}\n\n"
            "è«‹ç¢ºä¿å ±å‘Šå…§å®¹è©³å¯¦ã€é‚è¼¯æ¸…æ™°ï¼Œä¸¦åŸºæ–¼å¯¦éš›æ”¶é›†åˆ°çš„æ•¸æ“šå’Œè³‡æ–™ã€‚"
            "å¦‚æœæŸäº›éƒ¨åˆ†æ²’æœ‰ç›¸é—œè³‡æ–™ï¼Œè«‹æ˜ç¢ºèªªæ˜ï¼Œä¸è¦ç·¨é€ è³‡è¨Šã€‚"
        )
        chain = prompt | llm | StrOutputParser()
        report = chain.invoke({
            "query": query, 
            "notes": all_notes,
            "completed_tasks": "\n".join([f"- {task}" for task in completed_tasks]),
            "report_structure": report_structure
        })
        print(f"   ğŸ“Š [FinalReport] å ±å‘Šç”Ÿæˆå®Œæˆï¼ˆå•é¡Œé¡å‹ï¼šå­¸è¡“={is_academic_related}, è‚¡ç¥¨={is_stock_related}ï¼‰")
        return {"messages": [AIMessage(content=report)]}
    except Exception as e:
        print(f"   âš ï¸ [FinalReport] å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
        return {"messages": [AIMessage(content=f"å ±å‘Šç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")]}

# ==========================================
# 4. æ¢ä»¶è·¯ç”±
# ==========================================

def route_after_agent(state: DeepAgentState):
    """æ±ºå®šæ˜¯è¦å‘¼å«å·¥å…·ï¼Œé‚„æ˜¯é€²å…¥ç­†è¨˜éšæ®µ"""
    last_msg = state["messages"][-1]
    # æª¢æŸ¥æ˜¯å¦æœ‰å·¥å…·èª¿ç”¨
    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:
        return "tools"
    # æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸
    if state.get("iteration", 0) >= 20:
        return "note_taking"
    return "note_taking"

def route_after_note(state: DeepAgentState):
    """æ±ºå®šæ˜¯å¦é‚„æœ‰ä¸‹ä¸€å€‹ä»»å‹™è¦è·‘"""
    if len(state["completed_tasks"]) < len(state["tasks"]):
        return "research_agent"
    return "final_report"

# ==========================================
# 5. æ§‹å»º Deep Agent åœ–è¡¨
# ==========================================
builder = StateGraph(DeepAgentState)

builder.add_node("planner", planner_node)
builder.add_node("research_agent", research_agent_node)
builder.add_node("tools", ToolNode(tools_list))
builder.add_node("note_taking", note_taking_node)
builder.add_node("final_report", final_report_node)

builder.add_edge(START, "planner")
builder.add_edge("planner", "research_agent")

builder.add_conditional_edges(
    "research_agent",
    route_after_agent,
    {"tools": "tools", "note_taking": "note_taking"}
)
builder.add_edge("tools", "research_agent")

builder.add_conditional_edges(
    "note_taking",
    route_after_note,
    {"research_agent": "research_agent", "final_report": "final_report"}
)
builder.add_edge("final_report", END)

graph = builder.compile(checkpointer=MemorySaver())

# ==========================================
# 6. Gradio ç•Œé¢æ•´åˆ
# ==========================================

def run_research_agent(query: str, thread_id: str = None) -> Iterator[Tuple[str, str, str, str]]:
    """
    åŸ·è¡Œç ”ç©¶ä»£ç†ä¸¦å¯¦æ™‚è¿”å›ç‹€æ…‹ï¼ˆç”¨æ–¼ Gradio æµå¼æ›´æ–°ï¼‰
    
    ã€Gradio æ•´åˆã€‘è¿”å›ç”Ÿæˆå™¨ï¼Œè®“ Gradio å¯ä»¥å¯¦æ™‚æ›´æ–° UI
    ã€æµå¼è¼¸å‡ºã€‘æœ€çµ‚å ±å‘Šæœƒé€æ­¥ç”Ÿæˆï¼ŒæŒ‰å¥å­é€å¥é¡¯ç¤ºï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ¶é«”é©—
    è¿”å›æ ¼å¼: (ç•¶å‰ç¯€é»ç‹€æ…‹, ä»»å‹™åˆ—è¡¨, ç ”ç©¶ç­†è¨˜, æœ€çµ‚å ±å‘Š)
    
    Args:
        query: ç”¨æˆ¶è¼¸å…¥çš„ç ”ç©¶å•é¡Œ
        thread_id: å¯é¸çš„æœƒè©± IDï¼Œç”¨æ–¼å€åˆ†ä¸åŒçš„æŸ¥è©¢æœƒè©±
    
    Yields:
        Tuple[str, str, str, str]: (ç‹€æ…‹, ä»»å‹™åˆ—è¡¨, ç ”ç©¶ç­†è¨˜, å ±å‘Š)
    """
    if not query or not query.strip():
        yield "âŒ è«‹è¼¸å…¥å•é¡Œ", "", "", ""
        return
    
    # ç”Ÿæˆå”¯ä¸€çš„ thread_idï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if not thread_id:
        thread_id = f"deep-research-{uuid.uuid4().hex[:8]}"
    
    config = {"configurable": {"thread_id": thread_id}}
    
    # åˆå§‹åŒ–å®Œæ•´ç‹€æ…‹
    initial_state = {
        "query": query,
        "messages": [HumanMessage(content=query)],
        "tasks": [],
        "completed_tasks": [],
        "research_notes": [],
        "iteration": 0
    }
    
    # åˆå§‹åŒ–é¡¯ç¤ºè®Šæ•¸
    current_node = "ğŸ”„ åˆå§‹åŒ–ä¸­..."
    tasks_display = ""
    notes_display = ""
    report_display = ""
    full_report = ""  # å„²å­˜å®Œæ•´å ±å‘Šï¼Œç”¨æ–¼é€æ­¥é¡¯ç¤º
    
    try:
        # é–‹å§‹åŸ·è¡Œåœ–è¡¨
        events = graph.stream(
            initial_state,
            config,
            stream_mode="updates"
        )
        
        # éæ­·äº‹ä»¶æµï¼Œå¯¦æ™‚æ›´æ–° UI
        for event in events:
            for node, data in event.items():
                # æ›´æ–°ç•¶å‰ç¯€é»ç‹€æ…‹
                node_emoji = {
                    "planner": "ğŸ“",
                    "research_agent": "ğŸ•µï¸",
                    "tools": "ğŸ”§",
                    "note_taking": "ğŸ“Œ",
                    "final_report": "ğŸ“Š"
                }.get(node, "ğŸ”„")
                
                current_node = f"{node_emoji} æ­£åœ¨åŸ·è¡Œ: {node}"
                
                # æ›´æ–°ä»»å‹™åˆ—è¡¨é¡¯ç¤º
                if "tasks" in data:
                    tasks = data.get("tasks", [])
                    if tasks:
                        tasks_display = "\n".join([f"{i+1}. {task}" for i, task in enumerate(tasks)])
                
                # æ›´æ–°å®Œæˆä»»å‹™è¨ˆæ•¸
                if "completed_tasks" in data:
                    completed = data.get("completed_tasks", [])
                    tasks = data.get("tasks", [])
                    if completed and tasks:
                        completed_count = len(completed)
                        total_count = len(tasks)
                        progress = f"\n\nâœ… é€²åº¦: {completed_count}/{total_count} å€‹ä»»å‹™å·²å®Œæˆ"
                        tasks_display = "\n".join([f"{i+1}. {task}" for i, task in enumerate(tasks)]) + progress
                
                # æ›´æ–°ç ”ç©¶ç­†è¨˜é¡¯ç¤ºï¼ˆåªé¡¯ç¤ºæœ€è¿‘5æ¢ï¼Œé¿å…éé•·ï¼‰
                if "research_notes" in data:
                    notes = data.get("research_notes", [])
                    if notes:
                        # åªå–æœ€è¿‘5æ¢ç­†è¨˜
                        recent_notes = notes[-5:] if len(notes) > 5 else notes
                        notes_display = "\n\n" + "="*50 + "\n\n".join(recent_notes)
                
                # ã€é—œéµæ”¹é€²ã€‘æª¢æŸ¥æ˜¯å¦æ˜¯æœ€çµ‚å ±å‘Šï¼Œå¦‚æœæ˜¯å‰‡é€æ­¥ç”Ÿæˆï¼ˆæµå¼è¼¸å‡ºï¼‰
                if node == "final_report" and "messages" in data:
                    full_report = data["messages"][-1].content
                    current_node = "ğŸ“Š æ­£åœ¨ç”Ÿæˆå ±å‘Š..."
                    
                    # æŒ‰å¥å­åˆ†å‰²ä¸¦é€æ­¥é¡¯ç¤ºï¼ˆæ”¯æŒä¸­è‹±æ–‡æ¨™é»ï¼‰
                    import re
                    import time
                    
                    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²å¥å­ï¼ˆæ”¯æŒä¸­æ–‡æ¨™é»ï¼šã€‚ï¼ï¼Ÿå’Œè‹±æ–‡æ¨™é»ï¼š. ! ?ï¼‰
                    # ä¿ç•™æ¨™é»ç¬¦è™Ÿåœ¨å¥å­ä¸­
                    sentence_pattern = r'([ã€‚ï¼ï¼Ÿ\n\n]+|\.\s+|!\s+|\?\s+)'
                    parts = re.split(sentence_pattern, full_report)
                    
                    # é‡æ–°çµ„åˆå¥å­ï¼ˆä¿ç•™æ¨™é»ï¼‰
                    sentence_parts = []
                    i = 0
                    while i < len(parts):
                        if i + 1 < len(parts) and re.match(sentence_pattern, parts[i + 1]):
                            # å¥å­ + æ¨™é»
                            sentence_parts.append(parts[i] + parts[i + 1])
                            i += 2
                        else:
                            # å–®ç¨çš„å¥å­æˆ–æ¨™é»
                            if parts[i].strip():
                                sentence_parts.append(parts[i])
                            i += 1
                    
                    # å¦‚æœåˆ†å‰²å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„å­—ç¬¦å¡Šæ–¹å¼
                    if not sentence_parts or len(sentence_parts) == 1:
                        # æŒ‰å­—ç¬¦å¡Šé€æ­¥é¡¯ç¤ºï¼ˆæ¯20å€‹å­—ç¬¦ï¼‰
                        chunk_size = 20
                        accumulated_text = ""
                        for i in range(0, len(full_report), chunk_size):
                            accumulated_text = full_report[:i + chunk_size]
                            report_display = accumulated_text
                            yield current_node, tasks_display, notes_display, report_display
                            time.sleep(0.03)  # æ¯å¡Šä¹‹é–“çš„å»¶é²ï¼ˆ30æ¯«ç§’ï¼‰
                    else:
                        # é€æ­¥é¡¯ç¤ºæ¯å€‹å¥å­
                        accumulated_text = ""
                        for sentence in sentence_parts:
                            accumulated_text += sentence
                            report_display = accumulated_text
                            yield current_node, tasks_display, notes_display, report_display
                            time.sleep(0.1)  # æ¯å¥ä¹‹é–“çš„å»¶é²ï¼ˆ50æ¯«ç§’ï¼Œå¯èª¿æ•´ï¼‰
                    
                    # ç¢ºä¿å®Œæ•´å ±å‘Šé¡¯ç¤º
                    report_display = full_report
                    current_node = "âœ… å ±å‘Šç”Ÿæˆå®Œæˆï¼"
                    yield current_node, tasks_display, notes_display, report_display
                    continue  # è·³éå¾Œé¢çš„ yieldï¼Œé¿å…é‡è¤‡
                
                # å¯¦æ™‚è¿”å›ç‹€æ…‹ï¼ˆè®“ Gradio æ›´æ–° UIï¼‰
                yield current_node, tasks_display, notes_display, report_display
        
        # æœ€çµ‚ç‹€æ…‹
        yield "âœ… ç ”ç©¶å®Œæˆï¼", tasks_display, notes_display, report_display
        
    except Exception as e:
        error_msg = f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        print(f"éŒ¯èª¤è©³æƒ…: {e}")
        import traceback
        traceback.print_exc()
        yield error_msg, tasks_display, notes_display, report_display

def create_gradio_interface():
    """
    å‰µå»º Gradio ç•Œé¢
    
    ã€Gradio 6.x å…¼å®¹ã€‘ä½¿ç”¨æœ€æ–°çš„ Gradio API å‰µå»ºç¾è§€çš„ Web ç•Œé¢
    ã€é‡è¦ã€‘åœ¨ Gradio 6.0+ ä¸­ï¼Œtheme å’Œ css åƒæ•¸å·²ç§»è‡³ launch() æ–¹æ³•
    """
    # ä½¿ç”¨ Gradio 6.x çš„ä¸»é¡Œç³»çµ±ï¼ˆtheme å’Œ css å°‡åœ¨ launch() ä¸­è¨­ç½®ï¼‰
    with gr.Blocks(
        title="Deep Research Agent with RAG"
    ) as demo:
        # æ¨™é¡Œå€åŸŸ
        gr.Markdown(
            """
            <div class="header">
            <h1>ğŸš€ Deep Research Agent with RAG</h1>
            <p><strong>åŠŸèƒ½ç‰¹è‰²ï¼š</strong></p>
            <p>ğŸ“Š è‚¡ç¥¨è³‡è¨ŠæŸ¥è©¢ | ğŸŒ ç¶²è·¯æœå°‹ | ğŸ“š PDF çŸ¥è­˜åº«æŸ¥è©¢ï¼ˆTree of Thoughts è«–æ–‡ï¼‰</p>
            <p><strong>æ™ºèƒ½è¦åŠƒï¼š</strong> ç³»çµ±æœƒæ ¹æ“šå•é¡Œé¡å‹è‡ªå‹•é¸æ“‡åˆé©çš„ç ”ç©¶å·¥å…·</p>
            </div>
            """,
            elem_classes=["header"]
        )
        
        with gr.Row():
            with gr.Column(scale=2):
                # è¼¸å…¥å€åŸŸ
                query_input = gr.Textbox(
                    label="ğŸ“ è«‹è¼¸å…¥æ‚¨çš„ç ”ç©¶å•é¡Œ",
                    placeholder="ä¾‹å¦‚ï¼šèªªæ˜Tree of Thoughtsï¼Œä¸¦æ·±åº¦æ¯”è¼ƒä»–è·ŸChain of Thoughtçš„å·®è·åœ¨å“ªè£¡ï¼Ÿ",
                    lines=3,
                    value="æ¯”è¼ƒå¾®è»Ÿ(MSFT)å’Œè°·æ­Œ(GOOGL)åœ¨AIé ˜åŸŸçš„ä½ˆå±€ï¼Œä¸¦çµåˆ Tree of Thoughts è«–æ–‡ä¸­çš„æ–¹æ³•è«–é€²è¡Œåˆ†æ"
                )
                
                # æŒ‰éˆ•å€åŸŸ
                with gr.Row():
                    submit_btn = gr.Button("ğŸ” é–‹å§‹ç ”ç©¶", variant="primary", scale=1)
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤", variant="secondary", scale=1)
                
                # ç‹€æ…‹é¡¯ç¤º
                status_display = gr.Textbox(
                    label="ğŸ“Š ç•¶å‰ç‹€æ…‹",
                    value="ç­‰å¾…é–‹å§‹...",
                    interactive=False,
                    lines=2
                )
            
            with gr.Column(scale=1):
                # ä»»å‹™åˆ—è¡¨
                tasks_display = gr.Textbox(
                    label="ğŸ“‹ ç ”ç©¶ä»»å‹™åˆ—è¡¨",
                    lines=12,
                    interactive=False
                )
        
        with gr.Row():
            # ç ”ç©¶ç­†è¨˜ï¼ˆå¯¦æ™‚æ›´æ–°ï¼‰
            notes_display = gr.Textbox(
                label="ğŸ“Œ ç ”ç©¶ç­†è¨˜ï¼ˆå¯¦æ™‚æ›´æ–°ï¼‰",
                lines=15,
                interactive=False
            )
        
        with gr.Row():
            # æœ€çµ‚å ±å‘Š
            report_display = gr.Textbox(
                label="ğŸ“„ æœ€çµ‚æ·±åº¦å ±å‘Š",
                lines=20,
                interactive=False
            )
        
        # äº‹ä»¶è™•ç†å‡½æ•¸
        def process_query(query):
            """è™•ç†æŸ¥è©¢ä¸¦è¿”å›æµå¼æ›´æ–°"""
            if not query or not query.strip():
                return "âŒ è«‹è¼¸å…¥å•é¡Œ", "", "", ""
            
            # ä½¿ç”¨ç”Ÿæˆå™¨å‡½æ•¸å¯¦æ™‚æ›´æ–°ï¼ˆGradio 6.x æ”¯æŒæµå¼è¼¸å‡ºï¼‰
            for status, tasks, notes, report in run_research_agent(query):
                yield status, tasks, notes, report
        
        def clear_all():
            """æ¸…é™¤æ‰€æœ‰è¼¸å…¥å’Œè¼¸å‡º"""
            return "", "", "", "", "ç­‰å¾…é–‹å§‹..."
        
        # ç¶å®šäº‹ä»¶
        submit_btn.click(
            fn=process_query,
            inputs=query_input,
            outputs=[status_display, tasks_display, notes_display, report_display]
        )
        
        clear_btn.click(
            fn=clear_all,
            outputs=[query_input, tasks_display, notes_display, report_display, status_display]
        )
        
        # ç¤ºä¾‹å•é¡Œï¼ˆå¿«é€Ÿæ¸¬è©¦ï¼‰
        gr.Examples(
            examples=[
                "èªªæ˜Tree of Thoughtsï¼Œä¸¦æ·±åº¦æ¯”è¼ƒä»–è·ŸChain of Thoughtçš„å·®è·åœ¨å“ªè£¡ï¼Ÿ",
                "æ¯”è¼ƒå¾®è»Ÿ(MSFT)å’Œè°·æ­Œ(GOOGL)åœ¨AIé ˜åŸŸçš„ä½ˆå±€",
                "åˆ†æ Tree of Thoughts æ–¹æ³•çš„å„ªç¼ºé»å’Œæ‡‰ç”¨å ´æ™¯",
                "æŸ¥è©¢è˜‹æœ(AAPL)çš„è²¡å‹™ç‹€æ³å’Œè¿‘æœŸå‹•æ…‹"
            ],
            inputs=query_input
        )
        
        # é è…³èªªæ˜
        gr.Markdown(
            """
            ---
            **ä½¿ç”¨èªªæ˜ï¼š**
            1. åœ¨è¼¸å…¥æ¡†ä¸­è¼¸å…¥æ‚¨çš„ç ”ç©¶å•é¡Œ
            2. é»æ“Šã€Œé–‹å§‹ç ”ç©¶ã€æŒ‰éˆ•
            3. ç³»çµ±æœƒè‡ªå‹•è¦åŠƒç ”ç©¶æ­¥é©Ÿä¸¦åŸ·è¡Œ
            4. æ‚¨å¯ä»¥å¯¦æ™‚æŸ¥çœ‹ä»»å‹™é€²åº¦ã€ç ”ç©¶ç­†è¨˜å’Œæœ€çµ‚å ±å‘Š
            5. é»æ“Šã€Œæ¸…é™¤ã€æŒ‰éˆ•å¯ä»¥é‡ç½®æ‰€æœ‰å…§å®¹
            """
        )
    
    return demo

# ==========================================
# 7. ä¸»å‡½æ•¸ï¼ˆå•Ÿå‹• Gradio ç•Œé¢ï¼‰
# ==========================================

def main():
    """ä¸»å‡½æ•¸ï¼šå•Ÿå‹• Gradio ç•Œé¢"""
    print("\nğŸš€ Deep Research Agent with RAG (Groq Edition) å•Ÿå‹•ï¼")
    print("ğŸ’¡ æœ¬ç³»çµ±æ•´åˆäº†ï¼šè‚¡ç¥¨æŸ¥è©¢ã€ç¶²è·¯æœå°‹ã€PDF çŸ¥è­˜åº«æŸ¥è©¢åŠŸèƒ½\n")
    print("ğŸŒ æ­£åœ¨å•Ÿå‹• Gradio ç•Œé¢...\n")
    
    demo = create_gradio_interface()
    
    # ã€Gradio 6.0+ ä¿®å¾©ã€‘theme å’Œ css åƒæ•¸å¿…é ˆåœ¨ launch() æ–¹æ³•ä¸­è¨­ç½®
    # ã€æ³¨æ„ã€‘show_api åƒæ•¸åœ¨ Gradio 6.x ä¸­å·²è¢«ç§»é™¤
    demo.launch(
        server_name="0.0.0.0",  # å…è¨±å¤–éƒ¨è¨ªå•
        server_port=7860,        # ç«¯å£è™Ÿ
        share=False,            # è¨­ç‚º True å¯ç”Ÿæˆå…¬é–‹é€£çµï¼ˆéœ€è¦ Gradio å¸³è™Ÿï¼‰
        show_error=True,       # é¡¯ç¤ºéŒ¯èª¤è©³æƒ…
        theme=gr.themes.Soft(),  # ä¸»é¡Œè¨­ç½®ï¼ˆGradio 6.0+ å¿…é ˆåœ¨ launch() ä¸­ï¼‰
        css="""
        .gradio-container {
            font-family: 'Microsoft JhengHei', 'PingFang TC', Arial, sans-serif;
        }
        .header {
            text-align: center;
            padding: 20px;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        """  # CSS æ¨£å¼ï¼ˆGradio 6.0+ å¿…é ˆåœ¨ launch() ä¸­ï¼‰
    )

if __name__ == "__main__":
    main()

